{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stack_replacement import getStackQuestionsv2\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = pd.read_csv(\"Labeling - pandas-final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_depre_apis = pandas_df[\"DEPRECATED_API\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding replacement apis for pandas.SparseSeries\n",
      "finding replacement apis for pandas.SparseDataFrame\n",
      "finding replacement apis for pandas.SparseArray.values\n",
      "finding replacement apis for pandas.to_datetime\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [00:10<00:21,  1.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding replacement apis for pandas.Series.ftype\n",
      "finding replacement apis for pandas.Series.ftypes\n",
      "finding replacement apis for pandas.DataFrame.ftypes\n",
      "finding replacement apis for pandas.Index.contains\n",
      "finding replacement apis for pandas.DataFrame.get_dtype_counts\n",
      "finding replacement apis for pandas.MultiIndex.labels\n",
      "finding replacement apis for pandas.MultiIndex.set_labels\n",
      "finding replacement apis for pandas.Series.ptp\n",
      "finding replacement apis for pandas.Series.compress\n",
      "finding replacement apis for pandas.api.types.is_period\n",
      "finding replacement apis for pandas.api.types.is_datetimetz\n",
      "finding replacement apis for pandas.Series.nonzero\n",
      "finding replacement apis for pandas.Series.clip_lower\n",
      "finding replacement apis for pandas.Series.clip_upper\n",
      "finding replacement apis for pandas.DataFrame.clip_lower\n",
      "finding replacement apis for pandas.DataFrame.clip_upper\n",
      "finding replacement apis for pandas.Series.from_array\n",
      "finding replacement apis for pandas.SparseSeries.from_array\n",
      "finding replacement apis for pandas.Series.valid\n",
      "finding replacement apis for pandas.DataFrame.from_items\n",
      "finding replacement apis for pandas.Timestamp.weekday_name\n",
      "finding replacement apis for pandas.DatetimeIndex.weekday_name\n",
      "finding replacement apis for pandas.Series.dt.weekday_name\n",
      "finding replacement apis for pandas.tseries.plotting.tsplot\n",
      "finding replacement apis for pandas.tseries.register\n",
      "finding replacement apis for pandas.DataFrame.from_csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n",
      " 33%|███▎      | 1/3 [00:01<00:02,  1.05s/it]\n",
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\Anaconda3\\envs\\mlenv\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\User\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tqdm\\_monitor.py\", line 62, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"C:\\Users\\User\\Anaconda3\\envs\\mlenv\\lib\\_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      "100%|██████████| 3/3 [00:03<00:00,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding replacement apis for pandas.Series.from_csv\n",
      "finding replacement apis for pandas.options.html.borde\n",
      "finding replacement apis for pandas.TimeGrouper\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding replacement apis for pandas.cdate_range\n",
      "finding replacement apis for pandas.DataFrame.rename_axis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding replacement apis for pandas.Series.rename_axis\n",
      "finding replacement apis for pandas.reindex_axis\n",
      "finding replacement apis for pandas.Series.select\n",
      "finding replacement apis for pandas.DataFrame.select\n",
      "finding replacement apis for pandas.Series.argmax\n",
      "finding replacement apis for pandas.Series.argmin\n",
      "finding replacement apis for pandas.tslib.NaTType\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding replacement apis for pandas.tools.plotting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 3/4 [00:03<00:01,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding replacement apis for pandas.scatter_matrix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding replacement apis for pandas.SparseArray.to_dense\n",
      "finding replacement apis for pandas.Series.sortlevel\n",
      "finding replacement apis for pandas.DataFrame.sortlevel\n",
      "finding replacement apis for pandas.Term\n",
      "finding replacement apis for pandas.api.types:is_any_int_dtype\n",
      "finding replacement apis for pandas.tseries.util.pivot_annual\n",
      "finding replacement apis for pandas.tseries.util.isleapyear\n",
      "finding replacement apis for pandas.io.json.json_normalize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 8/16 [00:08<00:08,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding replacement apis for pandas.datetime\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 5/12 [00:05<00:07,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding replacement apis for pandas.np\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding replacement apis for pandas.SparseArray\n",
      "finding replacement apis for pandas.Categorical.take_nd\n",
      "finding replacement apis for pandas.RangeIndex._start\n",
      "finding replacement apis for pandas.RangeIndex._stop\n",
      "finding replacement apis for pandas.RangeIndex._step\n",
      "finding replacement apis for pandas.tseries.frequencies.get_offset\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'pandas.SparseSeries': '',\n",
       " 'pandas.SparseDataFrame': '',\n",
       " 'pandas.SparseArray.values': '',\n",
       " 'pandas.to_datetime': '',\n",
       " 'pandas.Series.ftype': '',\n",
       " 'pandas.Series.ftypes': '',\n",
       " 'pandas.DataFrame.ftypes': '',\n",
       " 'pandas.Index.contains': '',\n",
       " 'pandas.DataFrame.get_dtype_counts': '',\n",
       " 'pandas.MultiIndex.labels': '',\n",
       " 'pandas.MultiIndex.set_labels': '',\n",
       " 'pandas.Series.ptp': '',\n",
       " 'pandas.Series.compress': '',\n",
       " 'pandas.api.types.is_period': '',\n",
       " 'pandas.api.types.is_datetimetz': '',\n",
       " 'pandas.Series.nonzero': '',\n",
       " 'pandas.Series.clip_lower': '',\n",
       " 'pandas.Series.clip_upper': '',\n",
       " 'pandas.DataFrame.clip_lower': '',\n",
       " 'pandas.DataFrame.clip_upper': '',\n",
       " 'pandas.Series.from_array': '',\n",
       " 'pandas.SparseSeries.from_array': '',\n",
       " 'pandas.Series.valid': '',\n",
       " 'pandas.DataFrame.from_items': '',\n",
       " 'pandas.Timestamp.weekday_name': '',\n",
       " 'pandas.DatetimeIndex.weekday_name': '',\n",
       " 'pandas.Series.dt.weekday_name': '',\n",
       " 'pandas.tseries.plotting.tsplot': '',\n",
       " 'pandas.tseries.register': '',\n",
       " 'pandas.DataFrame.from_csv': 'pandas.DataFrame.from_csv?',\n",
       " 'pandas.Series.from_csv': '',\n",
       " 'pandas.options.html.borde': '',\n",
       " 'pandas.TimeGrouper': None,\n",
       " 'pandas.cdate_range': '',\n",
       " 'pandas.DataFrame.rename_axis': '',\n",
       " 'pandas.Series.rename_axis': '',\n",
       " 'pandas.reindex_axis': '',\n",
       " 'pandas.Series.select': '',\n",
       " 'pandas.DataFrame.select': '',\n",
       " 'pandas.Series.argmax': '',\n",
       " 'pandas.Series.argmin': '',\n",
       " 'pandas.tslib.NaTType': 'csv',\n",
       " 'pandas.tools.plotting': '',\n",
       " 'pandas.scatter_matrix': '%matplotlib inline',\n",
       " 'pandas.SparseArray.to_dense': '',\n",
       " 'pandas.Series.sortlevel': '',\n",
       " 'pandas.DataFrame.sortlevel': '',\n",
       " 'pandas.Term': '',\n",
       " 'pandas.api.types:is_any_int_dtype': '',\n",
       " 'pandas.tseries.util.pivot_annual': '',\n",
       " 'pandas.tseries.util.isleapyear': '',\n",
       " 'pandas.io.json.json_normalize': '',\n",
       " 'pandas.datetime': '',\n",
       " 'pandas.np': '|',\n",
       " 'pandas.SparseArray': '',\n",
       " 'pandas.Categorical.take_nd': '',\n",
       " 'pandas.RangeIndex._start': '',\n",
       " 'pandas.RangeIndex._stop': '',\n",
       " 'pandas.RangeIndex._step': '',\n",
       " 'pandas.tseries.frequencies.get_offset': ''}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_replacement_dict = {}\n",
    "for api in tqdm(pandas_depre_apis):\n",
    "    print(\"finding replacement apis for {}\".format(api))\n",
    "    pandas_replacement_dict[api] = getStackQuestionsv2(api, True)\n",
    "pandas_replacement_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "pandas_replacement_dict\n",
    "print(len(pandas_replacement_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas.DataFrame.from_csv\n",
      "pandas.TimeGrouper\n",
      "pandas.tslib.NaTType\n",
      "pandas.scatter_matrix\n",
      "pandas.np\n"
     ]
    }
   ],
   "source": [
    "for missing_api, replacement_api in pandas_replacement_dict.items():\n",
    "    if replacement_api != \"\":\n",
    "        print(missing_api)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seaborns Deprecated APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [{'is_answered': True, 'view_count': 359, 'accepted_answer_id': 61375635, 'answer_count': 1, 'score': 0, 'last_activity_date': 1587641116, 'creation_date': 1587581718, 'question_id': 61372948, 'link': 'https://stackoverflow.com/questions/61372948/show-the-yticks-in-the-marginal-plots-of-seaborn-jointgrid', 'title': 'Show the yticks in the marginal plots of seaborn.JointGrid', 'body': '<p>I need to show the yticks in the marginal distribution of a Seaborn.jointgrid plot, but it doesn\\'t seem simple.</p>\\n\\n<p>Here is an example of a code taken from the Seaborn documentation:</p>\\n\\n<pre><code>import seaborn as sns; sns.set(style=\"ticks\", color_codes=True)\\nimport bumpy as np\\ntips = sns.load_dataset(\"tips\")\\n\\ng = sns.JointGrid(x=\"total_bill\", y=\"tip\", data=tips)\\ng = g.plot_joint(sns.scatterplot, color=\"m\")\\n_ = g.ax_marg_x.hist(tips[\"total_bill\"], color=\"b\", alpha=.6,\\n                      bins=np.arange(0, 60, 5))\\n_ = g.ax_marg_y.hist(tips[\"tip\"], color=\"r\", alpha=.6,\\n                      orientation=\"horizontal\",\\n                      bins=np.arange(0, 12, 1))\\n</code></pre>\\n\\n<p>I would like to add the yaxis and its values to the marginal plots. I can plot the yaxis using:</p>\\n\\n<pre><code>sns.despine(ax=g.ax_marg_x)\\nsns.despine(ax=g.ax_marg_y)\\n</code></pre>\\n\\n<p>But it doesn\\'t contain any values or ticks. I have tried the solution proposed <a href=\"https://stackoverflow.com/questions/56798371/how-to-show-the-vertical-scale-of-marginal-histogram-in-seaborn-jointplot\">here</a> but it just doesn\\'t do anything.</p>\\n\\n<p>Here is the plot given by the code</p>\\n\\n<p><a href=\"https://i.stack.imgur.com/ER4XQ.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/ER4XQ.png\" alt=\"image\"></a></p>\\n'}], 'has_more': False, 'quota_max': 300, 'quota_remaining': 236}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 235}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 234}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 233}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 231}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 230}\n",
      "{'items': [{'is_answered': True, 'view_count': 584, 'accepted_answer_id': 61079377, 'answer_count': 1, 'score': 3, 'last_activity_date': 1586260099, 'creation_date': 1586249701, 'question_id': 61076255, 'link': 'https://stackoverflow.com/questions/61076255/set-axis-offset-dynamically-on-seaborn-despine', 'title': 'Set axis offset dynamically on seaborn.despine()', 'body': '<p>I wanted to calculate the axis offset for seaborn.despine() dynamically. It should stick to the 0 value of my matplotlib axes. But somehow it seems I have the wrong concept of calculating it. </p>\\n\\n<p>As I need to set seaborn\\'s offset in points I tried to calculate the image size, and set it into proportion with the x-axis with, but as it can be seen in the small code sample below this doen\\'t work as desired. </p>\\n\\n<pre><code>import numpy as np\\nimport pandas as pd\\nimport seaborn as sns\\nimport matplotlib.pyplot as plt\\n\\n# Define and use a simple function to label the plot in axes coordinates\\ndef label(x, color, label):\\n    ax = plt.gca()\\n    ax.text(0, .2, label, fontweight=\"bold\", color=color,\\n            ha=\"left\", va=\"center\", transform=ax.transAxes)\\n\\n# Create sample data\\nrs = np.random.RandomState(1979)\\nx = rs.randn(500)\\ng = np.tile(list(\"ABCDEFGHIJ\"), 50)\\ndf = pd.DataFrame(dict(x=x, g=g))\\n\\n# Plot\\nsns.set(style=\"white\", rc={\"axes.facecolor\": (0, 0, 0, 0)})\\n\\npal = sns.color_palette(\"hls\")\\n# Initialize the FacetGrid object\\ng = sns.FacetGrid(df, row=\"g\", hue=\"g\", aspect=20, height=0.7, palette=pal)\\n\\n# Draw the densities in a few steps\\ng.map(sns.kdeplot, \"x\", clip_on=False, shade=True, alpha=0.7, lw=2, bw=\\'scott\\')\\ng.map(sns.kdeplot, \"x\", clip_on=False, color=\"k\", lw=1, bw=\\'scott\\')\\n\\ng.map(label, \"x\")\\n\\n# Set the subplots to overlap\\ng.fig.subplots_adjust(hspace=-.25)\\n\\n\\n# calculate offset for left axis\\nax = g.axes[0]\\nax_size = abs(ax[0].get_xlim()[0]) + ax[0].get_xlim()[1]\\nfig = g.fig\\nsize = fig.get_size_inches()*fig.dpi\\nfig_size = size[0]\\noff = fig_size * ax[0].get_xlim()[0] / ax_size\\n\\n# Remove axes details that don\\'t play well with overlap\\ng.set_titles(\"\")\\ng.set(yticks=[])\\ng.despine(bottom=False, left=False, offset={\\'left\\':off}); # set offset\\n\\n</code></pre>\\n\\n<p>the plot looks like this:\\n<a href=\"https://i.stack.imgur.com/dS8Cm.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/dS8Cm.png\" alt=\"Plot\"></a></p>\\n\\n<p>as it can be seen, the axis is way off the zero-value. \\nAnyone an idea how to get this right?</p>\\n\\n<p>Thanks in advance,</p>\\n\\n<p>Exi</p>\\n'}, {'is_answered': False, 'view_count': 484, 'answer_count': 1, 'score': 0, 'last_activity_date': 1505381789, 'creation_date': 1505309639, 'question_id': 46199028, 'link': 'https://stackoverflow.com/questions/46199028/how-to-use-a-dictonary-for-offset-in-seaborn-despine', 'title': 'How to use a dictonary for offset in seaborn.despine?', 'body': '<p>I am trying to regulate the offset of the spines individually. For example I would like to have a bigger distance on the left spine than on the bottom one.\\nIn the documentation it says the offset-keyword also accepts a dictionary; however, I get an error message and I have no clue what it means. Surprisingly, I couldn\\'t find any examples for the dictonary-offset-comination.</p>\\n<blockquote>\\n<p><a href=\"http://seaborn.pydata.org/generated/seaborn.despine.html\" rel=\"nofollow noreferrer\">http://seaborn.pydata.org/generated/seaborn.despine.html</a></p>\\n<p><em><strong>offset : int or dict, optional</strong></em>\\nAbsolute distance, in points, spines should be moved away from the axes (negative values move spines inward). A single value applies to\\nall spines; a dict can be used to set offset values per side.</p>\\n</blockquote>\\n<p>That\\'s my not-working example code:</p>\\n<pre><code>sns.despine(ax=ax, trim=True, offset={\\'left\\':10,\\'right\\':10,\\'top\\':20,\\'bottom\\':13})\\n</code></pre>\\n<p>That\\'s is the error message:</p>\\n<pre><code>TypeError: unsupported operand type(s) for *: \\'dict\\' and \\'int\\'\\n</code></pre>\\n<p><strong>Summed up:</strong>\\nI would like to know how to set the offset for each spine individually; preferably with seaborn.despine.\\n(Additional explanations about what the error tells me would be appreciated as well.)</p>\\n<p>Thanks :-)</p>\\n<p><strong>Edit:</strong><br />\\nI tried the example code from ImportanceOfBeingErnest (only added \\'ticks\\' as style), but still get the same result. Any ideas why that happens?</p>\\n<pre><code>import matplotlib.pyplot as plt\\nimport seaborn as sns\\n\\nsns.set_style(\\'ticks\\') \\nfig,ax=plt.subplots()\\nax.plot([1,2,3])\\n\\nsns.despine(ax=ax, trim=True, offset={\\'left\\':10,\\'right\\':10,\\'top\\':20,\\'bottom\\':13})\\n\\nplt.show()\\n</code></pre>\\n<p>The plot gets displayed, but the spines don\\'t change at all.</p>\\n<pre><code>TypeError: unsupported operand type(s) for *: \\'dict\\' and \\'int\\'\\n</code></pre>\\n<p><a href=\"https://i.stack.imgur.com/hc5cT.png\" rel=\"nofollow noreferrer\">resulting plot</a></p>\\n'}], 'has_more': False, 'quota_max': 300, 'quota_remaining': 229}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:01<00:01,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 227}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 226}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 225}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 225}\n",
      "{'items': [{'is_answered': True, 'view_count': 60803, 'accepted_answer_id': 44542112, 'answer_count': 7, 'score': 14, 'last_activity_date': 1642866354, 'creation_date': 1453242011, 'question_id': 34888058, 'link': 'https://stackoverflow.com/questions/34888058/changing-width-of-bars-in-bar-chart-created-using-seaborn-factorplot', 'title': 'Changing width of bars in bar chart created using seaborn.factorplot', 'body': '<p>I\\'m trying to create bar chart using seaborn.factorplot. My code looks like this:</p>\\n\\n<pre><code> import seaborn\\n import matplotlib.pyplot as plt \\n\\ndf=pd.read_csv(\\'data.csv\\')\\n\\n fg = seaborn.factorplot(x=\\'vesselID\\', y=\\'dur_min\\', hue=\\'route\\', size=6,aspect=2    ,kind=\\'bar\\', data=df)\\n</code></pre>\\n\\n<p>my <strong>data.csv</strong> looks like this</p>\\n\\n<pre><code> ,route,vesselID,dur_min\\n 0,ANA-SJ,13,39.357894736842105\\n 1,ANA-SJ,20,24.747663551401867\\n 2,ANA-SJ,38,33.72142857142857\\n 3,ANA-SJ,69,37.064516129032256\\n 4,ED-KING,30,22.10062893081761\\n 5,ED-KING,36,21.821428571428573\\n 6,ED-KING,68,23.396551724137932\\n 7,F-V-S,1,13.623239436619718\\n 8,F-V-S,28,14.31294964028777\\n 9,F-V-S,33,16.161616161616163\\n 10,MUK-CL,18,13.953191489361702\\n 11,MUK-CL,19,14.306513409961687\\n 12,PD-TAL,65,12.477272727272727\\n 13,PT-COU,52,27.48148148148148\\n 14,PT-COU,66,28.24778761061947\\n 15,SEA-BI,25,30.94267515923567\\n 16,SEA-BI,32,31.0\\n 17,SEA-BI,37,31.513513513513512\\n 18,SEA-BR,2,55.8\\n 19,SEA-BR,13,57.0\\n 20,SEA-BR,15,54.05434782608695\\n 21,SEA-BR,17,50.43859649122807\\n</code></pre>\\n\\n<p><a href=\"https://i.stack.imgur.com/iPjtF.png\" rel=\"noreferrer\"><img src=\"https://i.stack.imgur.com/iPjtF.png\" alt=\"please click here to see the output\"></a></p>\\n\\n<p>Now my question is  how to change the width of the bar and I\\'m not able to achieve this by changing size and aspect. </p>\\n'}, {'is_answered': True, 'view_count': 1790, 'accepted_answer_id': 42416657, 'answer_count': 1, 'score': 2, 'last_activity_date': 1487855300, 'creation_date': 1487855063, 'question_id': 42416574, 'link': 'https://stackoverflow.com/questions/42416574/how-to-get-the-mean-for-each-group-in-pandas-dataframe-like-seaborn-factorplot', 'title': 'How to get the mean for each group in pandas.dataframe like seaborn.factorplot', 'body': '<p>I have a dataset formatted as a pandas dataframe. Please see this example in seaborn <a href=\"http://seaborn.pydata.org/generated/seaborn.factorplot.html#seaborn.factorplot\" rel=\"nofollow noreferrer\">http://seaborn.pydata.org/generated/seaborn.factorplot.html#seaborn.factorplot</a></p>\\n\\n<pre><code>&gt;&gt;&gt; import seaborn as sns\\n&gt;&gt;&gt; sns.set(style=\"ticks\")\\n&gt;&gt;&gt; exercise = sns.load_dataset(\"exercise\")\\n&gt;&gt;&gt; g = sns.factorplot(x=\"time\", y=\"pulse\", hue=\"kind\", data=exercise)\\n</code></pre>\\n\\n<p>With sns.factorplot, I can see the mean of the data by group (for this instance, the chart shows the mean of pulse at 1/15/30 mins group by the \"kind\").</p>\\n\\n<p>I want to directly get the \"values\" in the chart.\\nFor example</p>\\n\\n<pre><code>time      kind     mean    standard deviation\\n1 min     running  xx      xx\\n15 min    running  xx      xx\\n</code></pre>\\n\\n<p>I can use 2-depth loop to get the value I want, but I think there should be something easyier in pandas since it is a common requirement.</p>\\n\\n<p>Different from matplotlib, which will return all the values in the plot, seaborn returns a Facetgrid object. It seems that Facetgrid do not have the data I want.</p>\\n'}, {'is_answered': True, 'view_count': 1088, 'accepted_answer_id': 44927650, 'answer_count': 2, 'score': 2, 'last_activity_date': 1499261302, 'creation_date': 1499226467, 'question_id': 44916695, 'link': 'https://stackoverflow.com/questions/44916695/changing-legend-location-seaborn-factorplot', 'title': 'Changing legend location seaborn factorplot', 'body': '<pre><code>g = sns.factorplot(x=\"time\", y=\"pulse\", hue=\"kind\", col=\"diet\", data=exercise)\\n</code></pre>\\n\\n<p>In seaborn, is there a way to plot the legend in the first sub-plot instead of second? This example is from here: <a href=\"https://seaborn.pydata.org/generated/seaborn.factorplot.html?highlight=factor#seaborn.factorplot\" rel=\"nofollow noreferrer\">https://seaborn.pydata.org/generated/seaborn.factorplot.html?highlight=factor#seaborn.factorplot</a></p>\\n'}], 'has_more': False, 'quota_max': 300, 'quota_remaining': 223}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:03<00:00,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [{'is_answered': True, 'view_count': 3440, 'accepted_answer_id': 61501980, 'answer_count': 4, 'score': 7, 'last_activity_date': 1638956705, 'creation_date': 1588134716, 'question_id': 61494278, 'link': 'https://stackoverflow.com/questions/61494278/plotly-how-to-make-a-figure-with-multiple-lines-and-shaded-area-for-standard-de', 'title': 'Plotly: How to make a figure with multiple lines and shaded area for standard deviations?', 'body': '<p>How can I use Plotly to produce a line plot with a shaded standard deviation? I am trying to achieve something similar to seaborn.tsplot. Any help is appreciated.\\n<a href=\"https://i.stack.imgur.com/oo0Zw.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/oo0Zw.png\" alt=\"enter image description here\"></a></p>\\n'}, {'is_answered': False, 'view_count': 101, 'closed_date': 1502706291, 'answer_count': 0, 'score': 0, 'last_activity_date': 1502704804, 'creation_date': 1502702614, 'question_id': 45671311, 'link': 'https://stackoverflow.com/questions/45671311/python-matplotlib-confidence-interval-boxes', 'closed_reason': 'Duplicate', 'title': 'python matplotlib confidence interval boxes', 'body': '<p>can someone please guide me to produce a plot with python like the one in this picture: <a href=\"https://i.stack.imgur.com/H6NnX.png\" rel=\"nofollow noreferrer\">confidence boxes</a></p>\\n\\n<p>I am measuring the runtime of a process depending on one parameter which has 18 different values. I made 100 runs of measuring the runtime for each value. \\nNow I need the 99%(or 95%) confidence interval as boxes around the mean for each value of the parameter.</p>\\n\\n<p>I thought about using seaborn.tsplot() with interpolate=False and then tweaking the matplot output to produce <a href=\"https://i.stack.imgur.com/H6NnX.png\" rel=\"nofollow noreferrer\">confidence boxes like that</a></p>\\n\\n<p>Here is a minimal python example of what the pandas dataFrame looks like</p>\\n\\n<pre><code>import pandas as pd\\nimport numpy as np\\nimport seaborn as sns\\nimport scipy.stats as st\\n\\nRUNS=100\\n\\nX = list(range(20,210,10))\\n\\ndata = pd.DataFrame()\\nfor x in X:\\n    fakeData = np.array([x*10]*RUNS)\\n    randomVariation = np.random.random(RUNS)*x\\n    y = np.add(fakeData, randomVariation)\\n    data[x] = y\\n\\n#calculate mean and confidence intervals\\nfor x in X:\\n    y = np.array(data[x])\\n    mean = np.mean(y)\\n    ci = st.t.interval(0.99, len(y)-1, loc=mean, scale=st.sem(y))\\n    print((mean,)+ ci)\\n</code></pre>\\n\\n<p>EDIT: forgot to mention: I already can calculate the mean and confidence intervals. I just need some guidance to do the graphical part.</p>\\n'}], 'has_more': False, 'quota_max': 300, 'quota_remaining': 219}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [00:01<00:01,  1.04s/it]\n"
     ]
    }
   ],
   "source": [
    "seaborn_df = pd.read_csv(\"Labeling - seaborn-final.csv\")\n",
    "seaborn_depre_api = seaborn_df[\"DEPRECATED_API\"].to_list()\n",
    "seaborn_replacement_dict = {}\n",
    "for missing_api in seaborn_depre_api:\n",
    "    seaborn_replacement_dict[missing_api] = getStackQuestionsv2(missing_api, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'seaborn.JointGrid': 'tick_params',\n",
       " 'seaborn.coefplot': '',\n",
       " 'seaborn.interactplot': '',\n",
       " 'seaborn.apionly': '',\n",
       " 'seaborn.corrplot': '',\n",
       " 'seaborn.symmatplot': '',\n",
       " 'seaborn.despine': '',\n",
       " 'seaborn.set_color_palette': '',\n",
       " 'seaborn.palette_context': '',\n",
       " 'seaborn.JointGrid.annotate': '',\n",
       " 'seaborn.lvplot': '',\n",
       " 'seaborn.factorplot': 'set_width',\n",
       " 'seaborn.tsplot': ''}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seaborn_replacement_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEPRECATED_API</th>\n",
       "      <th>KNOWN_REPLACEMENT</th>\n",
       "      <th>AVAILABLE_IN_REL_NOTES</th>\n",
       "      <th>REPLACEMENT_FOUND</th>\n",
       "      <th>CORRECT replacement ?</th>\n",
       "      <th>PROPOSED_REPLACEMENT</th>\n",
       "      <th>EXTRACT_FROM_DOCUMENTATION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>seaborn.JointGrid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>seaborn.coefplot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>['interactplot']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>seaborn.interactplot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>seaborn.apionly</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['seaborn.regression'], ['reset_orig'], ['cent...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>seaborn.corrplot</td>\n",
       "      <td>heatmap</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['heatmap']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>seaborn.symmatplot</td>\n",
       "      <td>heatmap</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['heatmap']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>seaborn.despine</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[['PairGrid'], ['offset_spines'], ['set_palett...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>seaborn.set_color_palette</td>\n",
       "      <td>set_palette</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['set_palette']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>seaborn.palette_context</td>\n",
       "      <td>set_palette</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['set_palette']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>seaborn.JointGrid.annotate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>['heatmap']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>seaborn.lvplot</td>\n",
       "      <td>boxenplot</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['boxenplot']</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>seaborn.factorplot</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[['tutorial &lt;facet_grid&gt;'], ['pointplot'], ['h...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>seaborn.tsplot</td>\n",
       "      <td>lineplot</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[['lineplot'], ['statsmodels'], ['interactplo...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                DEPRECATED_API KNOWN_REPLACEMENT  AVAILABLE_IN_REL_NOTES  \\\n",
       "0            seaborn.JointGrid               NaN                       0   \n",
       "1             seaborn.coefplot               NaN                       0   \n",
       "2         seaborn.interactplot               NaN                       0   \n",
       "3              seaborn.apionly               NaN                       0   \n",
       "4             seaborn.corrplot           heatmap                       1   \n",
       "5           seaborn.symmatplot           heatmap                       1   \n",
       "6              seaborn.despine               NaN                       0   \n",
       "7    seaborn.set_color_palette       set_palette                       1   \n",
       "8      seaborn.palette_context       set_palette                       1   \n",
       "9   seaborn.JointGrid.annotate               NaN                       0   \n",
       "10              seaborn.lvplot         boxenplot                       1   \n",
       "11          seaborn.factorplot               NaN                       0   \n",
       "12              seaborn.tsplot          lineplot                       1   \n",
       "\n",
       "    REPLACEMENT_FOUND  CORRECT replacement ?  \\\n",
       "0                   1                      0   \n",
       "1                   0                      0   \n",
       "2                   0                      0   \n",
       "3                   1                      0   \n",
       "4                   1                      1   \n",
       "5                   1                      1   \n",
       "6                   1                      0   \n",
       "7                   1                      1   \n",
       "8                   1                      1   \n",
       "9                   1                      0   \n",
       "10                  1                      1   \n",
       "11                  0                      0   \n",
       "12                  1                      1   \n",
       "\n",
       "                                 PROPOSED_REPLACEMENT  \\\n",
       "0                                                 NaN   \n",
       "1                                    ['interactplot']   \n",
       "2                                                 NaN   \n",
       "3   ['seaborn.regression'], ['reset_orig'], ['cent...   \n",
       "4                                         ['heatmap']   \n",
       "5                                         ['heatmap']   \n",
       "6   [['PairGrid'], ['offset_spines'], ['set_palett...   \n",
       "7                                     ['set_palette']   \n",
       "8                                     ['set_palette']   \n",
       "9                                         ['heatmap']   \n",
       "10                                      ['boxenplot']   \n",
       "11  [['tutorial <facet_grid>'], ['pointplot'], ['h...   \n",
       "12   [['lineplot'], ['statsmodels'], ['interactplo...   \n",
       "\n",
       "    EXTRACT_FROM_DOCUMENTATION  \n",
       "0                          NaN  \n",
       "1                          NaN  \n",
       "2                          NaN  \n",
       "3                          NaN  \n",
       "4                          NaN  \n",
       "5                          NaN  \n",
       "6                          NaN  \n",
       "7                          NaN  \n",
       "8                          NaN  \n",
       "9                          NaN  \n",
       "10                         NaN  \n",
       "11                         NaN  \n",
       "12                         NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seaborn_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Numpy Missing APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 163}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 162}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 161}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 160}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 159}\n",
      "{'items': [{'is_answered': False, 'view_count': 1826, 'answer_count': 0, 'score': 0, 'last_activity_date': 1586994977, 'creation_date': 1586982237, 'question_id': 61237965, 'link': 'https://stackoverflow.com/questions/61237965/no-module-named-numpy-testing-decorators', 'title': 'No module named &#39;numpy.testing.decorators&#39;', 'body': \"<p>Hello please I need your help :</p>\\n\\n<p>I have python3.5 and i installed pandas</p>\\n\\n<p>sudo apt-get install python3-pandas</p>\\n\\n<p>then scikit learn : </p>\\n\\n<p>sudo pip install -U scikit-learn</p>\\n\\n<p>When I start to use :</p>\\n\\n<p>from sklearn.preprocessing import OneHotEncoder</p>\\n\\n<p>or anything related to sklearn this error appears:</p>\\n\\n<p>No module named 'numpy.testing.decorators'</p>\\n\\n<p>Numpy version 1.18.2\\npandas version pandas==1.0.3</p>\\n\"}], 'has_more': False, 'quota_max': 300, 'quota_remaining': 158}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [{'is_answered': True, 'view_count': 74638, 'answer_count': 8, 'score': 44, 'last_activity_date': 1622712207, 'creation_date': 1577237107, 'question_id': 59474533, 'link': 'https://stackoverflow.com/questions/59474533/modulenotfounderror-no-module-named-numpy-testing-nosetester', 'title': 'ModuleNotFoundError: No module named &#39;numpy.testing.nosetester&#39;', 'body': \"<p>I was using the Decision Tree and this error was raised. The same situation appeared when I used Back Propagation. How can I solve it?</p>\\n<pre><code>import pandas as pd\\nimport numpy as np\\na = np.test()\\nf = open('E:/lgdata.csv')\\ndata = pd.read_csv(f,index_col = 'id')\\n\\nx = data.iloc[:,10:12].as_matrix().astype(int)\\ny = data.iloc[:,9].as_matrix().astype(int)\\n\\nfrom sklearn.tree import DecisionTreeClassifier as DTC\\ndtc = DTC(criterion='entropy')\\ndtc.fit(x,y)\\nx=pd.DataFrame(x) \\n\\nfrom sklearn.tree import export_graphviz\\nwith open('tree.dot','w') as f1:\\n    f1 = export_graphviz(dtc, feature_names = x.columns, out_file = f1)\\n</code></pre>\\n<blockquote>\\n<p>Traceback (most recent call last):<br />\\n\\u2007\\u2007File &quot;&lt;ipython-input-40-4359c06ae1f0&gt;&quot;, line 1, in &lt;module&gt;<br />\\n\\u2007\\u2007\\u2007\\u2007runfile('C:/ProgramData/Anaconda3/lib/site-packages/scipy/_lib/_numpy_compat.py', wdir='C:/ProgramData/Anaconda3/lib/site-packages/scipy/_lib')<br />\\n\\u2007\\u2007File &quot;C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\spyder\\\\utils\\\\site\\\\sitecustomize.py&quot;, line 710, in runfile<br />\\n\\u2007\\u2007\\u2007\\u2007execfile(filename, namespace)<br />\\n\\u2007\\u2007File &quot;C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\spyder\\\\utils\\\\site\\\\sitecustomize.py&quot;, line 101, in execfile<br />\\n\\u2007\\u2007\\u2007\\u2007exec(compile(f.read(), filename, 'exec'), namespace)<br />\\n\\u2007\\u2007File &quot;C:/ProgramData/Anaconda3/lib/site-packages/scipy/_lib/_numpy_compat.py&quot;, line 9, in &lt;module&gt;<br />\\n\\u2007\\u2007\\u2007\\u2007from numpy.testing.nosetester import import_nose</p>\\n<p>ModuleNotFoundError: No module named 'numpy.testing.nosetester'</p>\\n</blockquote>\\n\"}, {'is_answered': True, 'view_count': 276, 'accepted_answer_id': 42844015, 'answer_count': 1, 'score': 1, 'last_activity_date': 1489695930, 'creation_date': 1489695319, 'question_id': 42843970, 'link': 'https://stackoverflow.com/questions/42843970/mac-system-generate-error-message-when-i-call-python-script-on-terminal', 'title': 'Mac system generate error message when I call python script on terminal', 'body': '<p>My code is simply the follow and work few days ago:</p>\\n\\n<pre><code>import pandas as pd\\n\\ndf = pd.read_csv(\\'WORLDBANK-ZAF_MYS_PROP_4044_SEC_MF.csv\\')\\nprint(df.head())\\n</code></pre>\\n\\n<p>But now whenever I try to run it by calling <code>python my_io.py</code> on my Mac terminal it generates the following messages:</p>\\n\\n<p>Bases-MacBook-Pro:data_analysis me$ python my_io.py\\nTraceback (most recent call last):\\n  File \"my_io.py\", line 1, in \\n    import pandas as pd\\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/pandas/<strong>init</strong>.py\", line 13, in \\n    <strong>import</strong>(dependency)\\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/<strong>init</strong>.py\", line 142, in \\n    from . import add_newdocs\\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/add_newdocs.py\", line 13, in \\n    from numpy.lib import add_newdoc\\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/lib/<strong>init</strong>.py\", line 8, in \\n    from .type_check import *\\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/lib/type_check.py\", line 11, in \\n    import numpy.core.numeric as _nx\\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/core/<strong>init</strong>.py\", line 72, in \\n    from numpy.testing.nosetester import _numpy_tester\\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/testing/<strong>init</strong>.py\", line 12, in \\n    from . import decorators as dec\\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/testing/decorators.py\", line 20, in \\n    from .utils import SkipTest, assert_warns\\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/numpy/testing/utils.py\", line 15, in \\n    from tempfile import mkdtemp, mkstemp\\n  File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/tempfile.py\", line 32, in \\n    import io as _io\\n  File \"/Users/gongzhuli/Desktop/data_analysis/io.py\", line 3, in </p>\\n\\n<p>AttributeError: \\'module\\' object has no attribute \\'read_csv\\'</p>\\n\\n<p>Can someone please help me, I have no idea what is going on here..</p>\\n'}], 'has_more': False, 'quota_max': 300, 'quota_remaining': 157}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 154}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 153}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 152}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 152}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 150}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 149}\n",
      "{'items': [{'is_answered': True, 'view_count': 319, 'accepted_answer_id': 37179441, 'answer_count': 1, 'score': 1, 'last_activity_date': 1463036274, 'creation_date': 1463034601, 'question_id': 37178905, 'link': 'https://stackoverflow.com/questions/37178905/convert-numpy-scalar-to-python-simple-native-with-one-twist', 'title': 'Convert numpy scalar to python simple native (with one twist)', 'body': '<p>I have a function that performs some operations and sets some protobuf values.</p>\\n\\n<p>Now protobuf requires that the values that are set are python natives and not numpy values.</p>\\n\\n<p>Now half the time this function is called with native values and half the time with numpy values.</p>\\n\\n<p>I need a fullproof way to convert numpy values to native values while not causing an issue if the type is already a python native value.</p>\\n\\n<p>What I tried:</p>\\n\\n<p>using numpy.asscalar\\nthis fails when getting native values\\nI have tried converting it to a string then back again but that feels very slow and a horrible way to perform this operation.</p>\\n'}], 'has_more': False, 'quota_max': 300, 'quota_remaining': 148}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 146}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 145}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 145}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 143}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 142}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 141}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 140}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 139}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 138}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 137}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 136}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 135}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 134}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 133}\n",
      "{'items': [{'is_answered': True, 'view_count': 319, 'accepted_answer_id': 37179441, 'answer_count': 1, 'score': 1, 'last_activity_date': 1463036274, 'creation_date': 1463034601, 'question_id': 37178905, 'link': 'https://stackoverflow.com/questions/37178905/convert-numpy-scalar-to-python-simple-native-with-one-twist', 'title': 'Convert numpy scalar to python simple native (with one twist)', 'body': '<p>I have a function that performs some operations and sets some protobuf values.</p>\\n\\n<p>Now protobuf requires that the values that are set are python natives and not numpy values.</p>\\n\\n<p>Now half the time this function is called with native values and half the time with numpy values.</p>\\n\\n<p>I need a fullproof way to convert numpy values to native values while not causing an issue if the type is already a python native value.</p>\\n\\n<p>What I tried:</p>\\n\\n<p>using numpy.asscalar\\nthis fails when getting native values\\nI have tried converting it to a string then back again but that feels very slow and a horrible way to perform this operation.</p>\\n'}], 'has_more': False, 'quota_max': 300, 'quota_remaining': 132}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [{'is_answered': True, 'view_count': 203, 'accepted_answer_id': 63400772, 'answer_count': 1, 'score': 2, 'last_activity_date': 1616354241, 'creation_date': 1597328562, 'question_id': 63397246, 'link': 'https://stackoverflow.com/questions/63397246/time-value-of-money-numpy-functions-working-with-varying-floating-rates', 'title': 'Time Value of Money NumPy Functions, Working with Varying Floating Rates', 'body': '<p>I would like to ask about the NumPy functions such as NumPy.fv(). I am aware of how to execute this function but ONLY for interest rates that are fixed. I would like to ask what if the rates are floating/varying interest rate?</p>\\n<p>For example,\\nABC deposited $1,000,000 into a bank, the bank pays a floating rate annually as shown:\\n[1.2%, 1%, 1.8%, 1.2%, 0.9%]. What is the total amount ABC will receive after 5 years?</p>\\n<p>What I understand is through the use of for-loops and I know how to work this out via Excel but I have been scratching my head around this if the TVM functions may be implemented inside this for-loop to work out the final compounded amount after 5 years?</p>\\n'}], 'has_more': False, 'quota_max': 300, 'quota_remaining': 130}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 128}\n",
      "{'items': [{'is_answered': True, 'view_count': 236, 'answer_count': 1, 'score': -1, 'last_activity_date': 1629870228, 'creation_date': 1629869778, 'question_id': 68917339, 'link': 'https://stackoverflow.com/questions/68917339/unable-to-run-the-numpy-nper-function-in-jupyterlab', 'title': 'Unable to run the Numpy.nper function in JupyterLab', 'body': '<p>A hint that something was incorrect was when Jupyter does not show a signture for the numpy.nper function (&quot;Signature: numpy.nper(*args, **kwds)&quot;)\\nI go ahead and fill the arguments with the variables but recieve the following error. I have made sure there are no spelling mistakes in the variable names or syntax errors so I don\\'t know where the mistake is. Can someone please help?</p>\\n<p>&quot;RuntimeError                              Traceback (most recent call last)\\n in \\n----&gt; 1 period = numpy.nper(investment_rate, -annual_cash, 0, desired_cash)</p>\\n<p>~\\\\OneDrive\\\\Ani\\\\Programs\\\\Python\\\\lib\\\\site-packages\\\\numpy_<em>init</em>_.py in _expired(*args, **kwds)\\n275\\n276                 def _expired(*args, **kwds):\\n--&gt; 277                     raise RuntimeError(msg)\\n278\\n279                 return _expired</p>\\n<p>RuntimeError: In accordance with NEP 32, the function nper was removed from NumPy version 1.20.  A replacement for this function is available in the numpy_financial library: <a href=\"https://pypi.org/project/numpy-financial\" rel=\"nofollow noreferrer\">https://pypi.org/project/numpy-financial</a></p>\\n'}], 'has_more': False, 'quota_max': 300, 'quota_remaining': 127}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 125}\n",
      "{'items': [{'is_answered': True, 'view_count': 177, 'accepted_answer_id': 54040925, 'answer_count': 1, 'score': 0, 'last_activity_date': 1546612744, 'creation_date': 1546611272, 'question_id': 54040654, 'link': 'https://stackoverflow.com/questions/54040654/python-dataframe-sum-on-specific-columns', 'title': 'Python dataframe sum on specific columns', 'body': \"<p>My task is to calculate the cumulative principal. Using numpy.ppmt it only gives a specific month's paid principal, so I want to add columns which contains monthly principal for each record and then take the sum to get the cumulative principal. </p>\\n\\n<p>For example, I have a dataframe looks like the following:</p>\\n\\n<pre><code>frame = pd.DataFrame({'rate':[0.1, 0.1], 'per':[2, 4], 'nper':[360, 360], 'pv':[100000, 200000]})\\n\\nmax_per = frame['per'].max()\\ncolumns = ['principal%s'%i for i in range(1, max_per + 1)]\\ndf = pd.DataFrame(index=frame.index, columns=columns, dtype='float').fillna(0)\\n\\nfor index, column in enumerate(columns):\\n    df[column] = -np.ppmt(rate=frame['rate'] / 100 / 12, per=index + 1, nper=frame['nper'], \\n                          pv=frame['pv'], when=when)\\nframe.join(df)\\n</code></pre>\\n\\n<p>The dataframe will look like the following:</p>\\n\\n<pre><code>   nper  per      pv  rate          epp  principal1  principal2  principal3  \\\\\\n0   360    2  100000   0.1   547.309838  273.643517  273.666321  273.689126   \\n1   360    4  200000   0.1  2189.421796  547.287034  547.332642  547.378253   \\n\\n   principal4  \\n0  273.711934  \\n1  547.423868\\n</code></pre>\\n\\n<p>The problem is that for record one, principal3 and principal4 should be 0. One workaround is to calculate the sum of principal1-principal4 based on column 'per', for example if frame.per == 2, then I only sum principal1 and principal 2, and if frame.per == 4, then I only sum principal1 through principal4. Any help to do that. </p>\\n\\n<p>I can calculate the cumulative principal by calling apply, but I do not want to do that because it is slow.</p>\\n\\n<p>Thanks.</p>\\n\"}], 'has_more': False, 'quota_max': 300, 'quota_remaining': 124}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 123}\n",
      "{'items': [{'is_answered': True, 'view_count': 199, 'accepted_answer_id': 66170654, 'answer_count': 1, 'score': 2, 'last_activity_date': 1613127832, 'creation_date': 1553639587, 'question_id': 55367124, 'link': 'https://stackoverflow.com/questions/55367124/numpy-rate-function-on-large-dataset-returning-nulls', 'title': 'Numpy Rate function on large dataset returning nulls', 'body': '<p>I am using the <strong>numpy</strong> rate function in order to mimic the <strong>Excel Rate</strong> function on loans.</p>\\n\\n<p>The function returns the correct result when working with a subset of my dataframe (1 million records). </p>\\n\\n<p>However, when working with the entire dataframe (over 10 million records), it returns <strong>null</strong> results for all.</p>\\n\\n<p>Could this be a memory issue? If that is the case, how can it be solved?\\nI have already tried to chunk the data and use a while/for loop to calculate, but this didn\\'t solve the problem.</p>\\n\\n<p>This worked (not when I looped through the 10 million records though):</p>\\n\\n<pre><code>test = df2.iloc[:1000000,:]\\ntest = test.loc[:,[\\'LoanTerm\\',Instalment,\\'LoanAmount\\']]\\ntest[\\'True_Effective_Rate\\'] = ((1+np.rate(test[\\'LoanTerm\\'],-test[\\'Instalment\\'],test[\\'LoanAmount\\'],0))**12-1)*100\\n</code></pre>\\n\\n<p>I am trying to get this to work:</p>\\n\\n<pre><code>df2[\\'True_Effective_Rate\\'] = ((1+np.rate(df2[\\'LoanTerm\\'],-df2[\\'Instalment\\'],df2[\\'LoanAmount\\'],0))**12-1)*100\\n</code></pre>\\n\\n<p>I see a similar question has been asked in the past where all the values returned are nulls when one of the parameter inputs are incorrect.</p>\\n\\n<p><a href=\"https://stackoverflow.com/questions/27977057/using-numpy-rate-on-numpy-array-returns-nans-unexpectedly\">Using numpy.rate, on numpy array returns nan&#39;s unexpectedly</a></p>\\n\\n<p>My dataframe doesn\\'t have 0 values though. How can I prevent this from happening?</p>\\n'}, {'is_answered': False, 'view_count': 482, 'answer_count': 1, 'score': 1, 'last_activity_date': 1540230372, 'creation_date': 1540229678, 'question_id': 52934794, 'link': 'https://stackoverflow.com/questions/52934794/using-numpy-rate-to-calculate-annualised-interest-rate', 'title': 'Using Numpy.rate to calculate annualised interest rate', 'body': \"<p>I am exploring Numpy.rate function to determine the annualised interest rate in order to attain a future value within a defined time period. </p>\\n\\n<p>For example:</p>\\n\\n<ul>\\n<li>I have an initial saving of $10,000 to create an investment portfolio</li>\\n<li>Every month I will contribute $500 to this portfolio </li>\\n<li>At the end of 15 years, I hope to see my portfolio grow to $120,000 (with compounding effect)</li>\\n</ul>\\n\\n<p>Below is my code snippet in Python:</p>\\n\\n<pre><code>    import numpy as np\\n\\n    initial_value = 10000.0\\n\\n    monthly_payment = 500.0\\n\\n    number_of_years = 15\\n\\n    future_value = 120000.0\\n\\n    np.rate(number_of_years, -monthly_payment*12, initial_value, future_value, when='end')\\n\\n: 0.06583239141591554\\n</code></pre>\\n\\n<p>Based on the output, does it mean the annualised return is 0.0658 * 100 = 6.58%?</p>\\n\"}, {'is_answered': False, 'view_count': 77, 'answer_count': 0, 'score': 1, 'last_activity_date': 1549962588, 'creation_date': 1549962588, 'question_id': 54646377, 'link': 'https://stackoverflow.com/questions/54646377/numpy-rate-with-pmt-as-array', 'title': 'numpy rate with pmt as array', 'body': '<p>I am having an issue with calculating the rate with the numpy.rate function. I want to calculate the rate of interest per period for a non-uniform pmt done over nper periods of time that have a certain future value. I am not able to get the input parameters for the function right. </p>\\n\\n<p>After stripping down the code here is the sample code that I tried. </p>\\n\\n<pre><code>import numpy as np\\n\\nfirst_payment = 5100\\nfinal_payment = 5400\\nperiods  = 20 \\nnper = np.arange(start=periods, stop=0, step=-1, dtype=float)\\npmt = np.linspace(first_payment, final_payment, 20, dtype=float)\\npv = 0\\nfv = -120000\\n\\nprint(np.rate(nper, pmt, pv, fv))\\n</code></pre>\\n\\n<p>I am expecting an array of returns, I am not sure if this the right approach for a problem like this. </p>\\n\\n<p>Another approach I tried is that I assumed a rate of return r and tried to solve the equation by multiplying each payment with the number of periods. But that does not look good. TIA.</p>\\n'}, {'is_answered': True, 'view_count': 86, 'accepted_answer_id': 68900678, 'answer_count': 1, 'score': 0, 'last_activity_date': 1629915532, 'creation_date': 1628784493, 'question_id': 68760844, 'link': 'https://stackoverflow.com/questions/68760844/how-to-solve-rate-of-return-in-python', 'title': 'How to solve rate of return in python', 'body': '<p>Does anyone know if there is a function in python that can solve for rate of return based on variable payments.   for instance\\ncash flows = 5000 , 10000, 0, 0, 6000 with an ending fund value of 50,000\\nsolve for r such that</p>\\n<p>5000 * ( 1+r)^5 + 10000 * (1+r)^4 + 0* (1+r)^3 +0*(1+r)^2 +6000*(1+r)1 = 50000</p>\\n<p>r should solve for 34%</p>\\n<p>have looked into numpy.rate but it does not account for variable payment but rather a flat fixed payment for 5 years.</p>\\n'}], 'has_more': False, 'quota_max': 300, 'quota_remaining': 121}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 1/4 [00:01<00:03,  1.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [{'is_answered': True, 'view_count': 9719, 'accepted_answer_id': 46204127, 'answer_count': 1, 'score': 4, 'last_activity_date': 1606066514, 'creation_date': 1505324325, 'question_id': 46203735, 'link': 'https://stackoverflow.com/questions/46203735/calculating-variable-cash-flow-irr-in-python-pandas', 'title': 'Calculating Variable Cash-flow IRR in Python (pandas)', 'body': \"<p>I have a DataFrame of unpredictable cashflows and unpredictable period lengths, and I need to generate a backward-looking IRR.</p>\\n\\n<p>Doing it in Excel is pretty straightforward using the solver, wondering if there's a good way to pull it off in Python. (I think I could leverage openpyxl to get solver to work in excel from python, but that feels unnecessarily cumbersome).</p>\\n\\n<p>The problem is pretty straightforward:</p>\\n\\n<blockquote>\\n  <p>NPV of Cash Flow = ((cash_flow)/(1+IRR)^years_ago)</p>\\n</blockquote>\\n\\n<p>GOAL: Find IRR where SUM(NPV) = 0</p>\\n\\n<p>My dataframe looks something like this:</p>\\n\\n<pre><code>cash_flow    |years_ago\\n-----------------------\\n-3.60837e+06 |4.09167    \\n31462        |4.09167    \\n1.05956e+06  |3.63333    \\n-1.32718e+06 |3.28056    \\n-4.46554e+06 |3.03889    \\n</code></pre>\\n\\n<p>It seems as though other IRR calculators (such as numpy.irr) assume strict period cutoffs (every 3 months, 1 year, etc), which won't work. The other option seems to be the iterative route, where I continually guess, check, and iterate, but that feels like the wrong way to tackle this. Ideally, I'm looking for something that would do this:</p>\\n\\n<pre><code>irr = calc_irr((cash_flow1,years_ago1),(cash_flow2,years_ago2),etc)\\n</code></pre>\\n\\n<p>EDIT: Here is the code I'm running the problem from. I have a list of transactions, and I've chosen to create temporary tables by id.</p>\\n\\n<pre><code>for id in df_tran.id.unique():\\n   temp_df = df_tran[df_tran.id == id] \\n\\n   cash_flow = temp_df.cash_flows.values\\n   years = temp_df.years.values\\n\\n   print(id, cash_flow)\\n   print(years)\\n#irr_calc = irr(cfs=cash_flow, yrs=years,x0=0.100000)\\n#print(sid, irr_calc)\\n</code></pre>\\n\\n<p>where df_tran (which temp_df is based on) looks like:</p>\\n\\n<pre><code>    cash_flow       |years     |id\\n0   -3.60837e+06     4.09167    978237\\n1   31462            4.09167    978237\\n4   1.05956e+06      3.63333    978237\\n6   -1.32718e+06     3.28056    978237\\n8   -4.46554e+06     3.03889    978237\\n10  -3.16163e+06     2.81944    978237\\n12  -5.07288e+06     2.58889    978237\\n14  268833           2.46667    978237\\n17  -4.74703e+06     1.79167    978237\\n20  -964987          1.40556    978237\\n22  -142920          1.12222    978237\\n24  163894           0.947222   978237\\n26  -2.2064e+06      0.655556   978237\\n27  1.23804e+06      0.566667   978237\\n29  180655           0.430556   978237\\n30  -85297           0.336111   978237\\n34  -2.3529e+07      0.758333   1329483\\n36  21935            0.636111   1329483\\n38  -3.55067e+06     0.366667   1329483\\n41  -4e+06           4.14167    1365051\\n</code></pre>\\n\\n<p>temp_df looks identical to df_tran, except it only holds transactions for a single id.</p>\\n\"}, {'is_answered': False, 'view_count': 697, 'answer_count': 0, 'score': 2, 'last_activity_date': 1537887711, 'creation_date': 1537437432, 'question_id': 52422399, 'link': 'https://stackoverflow.com/questions/52422399/pandas-calculating-irr-of-loan-cashflows', 'title': 'Pandas: Calculating IRR of loan cashflows', 'body': '<p>I have a Pandas dataframe of cashflows which are of unpredictable length. They range monthly over a period of between a few months and 30+ months and there is a column each month which contains that months cashflow information. I need to compute the IRR for each loan which is defined as follows;</p>\\n\\n<p><a href=\"https://i.stack.imgur.com/BJRU0.png\" rel=\"nofollow noreferrer\">Definition of IRR, where r is \"IRR\" when NPV is set to 0.</a></p>\\n\\n<p>Below is a link to an example of the df where I have \"months_remaining\", the initial outflow and a range of cashflows over variable periods (between Feb-19 and Dec-29). \"S\" represents a loan that has settled, i.e. it has reached maturity and had paid back the full amount. \"0\" represents a loan that has defaulted therefore no further cashflows are expected in perpetuity;</p>\\n\\n<p><a href=\"https://i.stack.imgur.com/Naxhr.png\" rel=\"nofollow noreferrer\">Loans DF</a></p>\\n\\n<p>I have tried a range of approaches. (1) Using iterrows to create a numpy array for each row then tried using numpy.irr, however the issue of only calculating IRR for a variable time period still stands (I need to recognise when cashflows stop with an \"S\" and stop calculating from this point.) (2) Avoiding row by row approaches and trying to perform calculations in Pandas, again the mixture of some \"S\" loans and some normal cashflows in columns trips this approach up. </p>\\n\\n<p><a href=\"https://i.stack.imgur.com/d4kDj.png\" rel=\"nofollow noreferrer\">Cell which I am currently working in.</a></p>\\n\\n<p>Thanks for any help you can provide. </p>\\n\\n<p>[EDIT] An example of working code using a for loop</p>\\n\\n<pre><code> for row in irr_array:\\n  LoanID=row[1]\\n  Tier=row[2]\\n  row=row[2:2000]\\n  if \"S\" in row:\\n    new_array = [i for i in row if i is not \"S\"]\\n    irr_results[\"IRR\"]= (pd.Series(round(irr(new_array),3)))\\n    irr_results[\"LoanID\"]=(pd.Series(LoanID))\\n    irr_results[\"Tier\"]=(pd.Series(Tier))\\n    results=irr_df.append(irr_results)\\n  if 0 in row:\\n    irr_results[\"IRR\"]= 0\\n    irr_results[\"LoanID\"]=(pd.Series(LoanID))\\n    irr_results[\"Tier\"]=(pd.Series(Tier))\\n    results=irr_df.append(irr_results)      \\n</code></pre>\\n'}, {'is_answered': False, 'view_count': 329, 'answer_count': 2, 'score': 1, 'last_activity_date': 1535733666, 'creation_date': 1525568476, 'question_id': 50195360, 'link': 'https://stackoverflow.com/questions/50195360/numpy-irr-using-monthy-data', 'title': 'numpy.irr Using Monthy Data', 'body': '<p>I am using a series of monthly data for my IRR calculation.\\nIt looks to me that this is set up to return the IRR based on annual data. \\nAm I wrong?  How does numpy.irr adjust for time periods?\\nThanks.</p>\\n'}, {'is_answered': False, 'view_count': 221, 'answer_count': 0, 'score': 0, 'last_activity_date': 1457099646, 'creation_date': 1457098479, 'question_id': 35797433, 'link': 'https://stackoverflow.com/questions/35797433/make-the-percentages-equal-using-a-formula-function-in-python-for-the-values-pul', 'title': 'Make the percentages equal using a formula function in python for the values pulled from Excel', 'body': '<pre><code>import xlrd,numpy\\nexcel = \\'/Users/Bob/Desktop/\\'\\n\\nwb1 = xlrd.open_workbook(excel + \\'assignment3.xlsx\\')\\nsh1 = wb1.sheet_by_index(0)\\n\\ncolA,colB = [],[]\\nfor a in range(3,sh1.nrows):\\n    colA.append(int(sh1.cell(a,0).value))\\n    colB.append(int(sh1.cell(a,1).value))\\nprint(colA)\\nprint(colB)\\n\\ngiven_per = (sh1.cell_value(2,1))*100\\nprint(\"Given Percentage:\", given_per)\\n\\ncalc_per = numpy.irr(colB) * 100  # Using this formula to get the 5%\\nprint(\"Calculated Percentage\", round(calc_per, 0), \\'%\\', \\'\\\\n\\')\\n\\n#Given percentage (10%) is the required percentage.We are getting 5%.\\n#Change values 1275 and 400 because they have the word \"change\" beside \\n#them in a case when given_per &gt; calc_per such that the adjustment\\n#of those values make the calc_per &gt;= given_per. \\n# NOTE: Don\\'t make a change directly in the excel file.\\n\\nif calc_per &gt;= given_per:\\n    print(\"Percentages are equal.\")  # In our case they aren\\'t.\\nelse:\\n    print(\"Percentages are NOT equal.\")\\n    # Adjust 1275 and 400. Print the adjusted colB as new_colB\\n    # Print the adjusted values in B6 and B8\\n    # And display the re-calculated calc_per as new_calc_per\\n    # NOTE: Make sure that this code is generic. It should work for any excel\\n    # file of the same format. The word \"change\" could be in any row. Keep that in mind.\\n    # Don\\'t forget to skip/ignore the first two rows and the third column.\\n    for i in range(4,sh1.nrows):                   # *****\\n        if sh1.cell(i,3).value == \"change\":\\n            for b in int(sh1.cell(i,1).value):\\n                calc_per = numpy.irr(colB) * 100\\n                if calc_per &lt; given_per:\\n                    for i in range(4,sh1.nrows):\\n                        if sh1.cell(i,3).value == \"change\":\\n                            for b in sh1.cell(i,1).value:\\n                                b+1;\\n                else:\\n                    break                          # *****\\n    # Remember to use the formula to check the percentages.\\n    #print(\"With adjusted values, the percentages are now equal,\"\\n    #      \"or calc_per is greater than given_per.\")\\n</code></pre>\\n\\n<p>Please go through the comments to understand the code. The part I am struggling with are the lines from starting and ending with #*****.</p>\\n\\n<p>What we need to do in this assignment is to equal the \"Given Percentage\" value of 10% by using the formula function numpy.irr as shown in line 24 of the code. With the current values in column B we get only 5%, so we need to adjust the values in B6 and B8 such that when we use the numpy.irr again, we get a percent value that is 10% or above. Basically, we need the calc_per >= given_per. </p>\\n\\n<p>When I used excel solver to get that calc_per to be 10%, the new values in B6 and B8 were 1661.45 and 434.39 respectively. Obviously, they could be anything. This is what excel produced.</p>\\n\\n<p>Snippet of the excel file is below as an image:</p>\\n\\n<p><a href=\"http://i.stack.imgur.com/qnz8b.png\" rel=\"nofollow\">Excel File Snippet</a></p>\\n\\n<p>Thanks!</p>\\n'}, {'is_answered': False, 'view_count': 94, 'answer_count': 0, 'score': 0, 'last_activity_date': 1457463895, 'creation_date': 1457461032, 'question_id': 35874938, 'link': 'https://stackoverflow.com/questions/35874938/comparing-two-percentages-value-until-they-are-equal', 'title': 'Comparing two percentages value until they are equal', 'body': '<pre><code>import xlrd,numpy\\nexcel = \\'/Users/Bob/Desktop/\\'\\n\\nwb1 = xlrd.open_workbook(excel + \\'assignment3.xlsx\\')\\nsh1 = wb1.sheet_by_index(0)\\n\\ncolA,colB = [],[]\\nfor a in range(3,sh1.nrows):\\n    colA.append(int(sh1.cell(a,0).value))\\n    colB.append(int(sh1.cell(a,1).value))\\nprint(colA)\\nprint(colB)\\n\\ngiven_per = (sh1.cell_value(2,1))*100\\nprint(\"Given Percentage:\", given_per)\\n\\ncalc_per = numpy.irr(colB) * 100  # Using this formula to get the 5%\\nprint(\"Calculated Percentage\", round(calc_per, 0), \\'%\\', \\'\\\\n\\')\\n\\n#Given percentage (10%) is the required percentage.We are getting 5%.\\n#Change values 1275 and 400 because they have the word \"change\" beside \\n#them in a case when given_per &gt; calc_per such that the adjustment\\n#of those values make the calc_per &gt;= given_per. \\n# NOTE: Don\\'t make a change directly in the excel file.\\n\\nif calc_per &gt;= given_per:\\n    print(\"Percentages are equal.\")  # In our case they aren\\'t.\\nelse:\\n    print(\"Percentages are NOT equal.\")\\n    # Adjust 1275 and 400. Print the adjusted colB as new_colB\\n    # Print the adjusted values in B6 and B8\\n    # And display the re-calculated calc_per as new_calc_per\\n    # NOTE: Make sure that this code is generic. It should work for any excel\\n    # file of the same format. The word \"change\" could be in any row. Keep that in mind.\\n    # Don\\'t forget to skip/ignore the first two rows and the third column.\\n    for i in range(4,sh1.nrows):                   # *****\\n        if sh1.cell(i,3).value == \"change\":\\n            for b in int(sh1.cell(i,1).value):\\n                calc_per = numpy.irr(colB) * 100\\n                if calc_per &lt; given_per:\\n                    for i in range(4,sh1.nrows):\\n                        if sh1.cell(i,3).value == \"change\":\\n                            for b in sh1.cell(i,1).value:\\n                                b+1;\\n                else:\\n                    break                          # *****\\n    # Remember to use the formula to check the percentages.\\n    #print(\"With adjusted values, the percentages are now equal,\"\\n    #      \"or calc_per is greater than given_per.\")\\n</code></pre>\\n\\n<p>Please go through the comments to understand the code. The part I am struggling with are the lines from starting and ending with #*****.</p>\\n\\n<p>What we need to do in this assignment is to equal the \"Given Percentage\" value of 10% by using the formula function numpy.irr as shown in line 24 of the code. With the current values in column B we get only 5%, so we need to adjust the values in B6 and B8 such that when we use the numpy.irr again, we get a percent value that is 10% or above. Basically, we need the calc_per >= given_per.</p>\\n\\n<p>When I used excel solver to get that calc_per to be 10%, the new values in B6 and B8 were 1661.45 and 434.39 respectively. Obviously, they could be anything. This is what excel produced.</p>\\n\\n<p>Snippet of the excel file is below as an image:</p>\\n\\n<p><a href=\"https://i.stack.imgur.com/qHOX1.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/qHOX1.png\" alt=\"enter image description here\"></a></p>\\n\\n<p>Thanks!</p>\\n'}, {'is_answered': False, 'view_count': 1109, 'answer_count': 0, 'score': 0, 'last_activity_date': 1483352786, 'creation_date': 1483352786, 'question_id': 41425011, 'link': 'https://stackoverflow.com/questions/41425011/numpy-irr-returns-error-valueerrorinput-must-be-a-rank-1-array', 'title': 'numpy.irr returns error: ValueError(&quot;Input must be a rank-1 array.&quot;)', 'body': '<p>Im trying to calculate the IRR of a monthly savings plan using the numpy.IRR function. I keep getting an ValueError(\"Input must be a rank-1 array.\"). </p>\\n\\n<p>How can I turn my array \"Cashflow\" into an array that does not yield this error? I have tried \"ravel\" or \"numpy.reshape\" but I keep getting the error. Or is there another mistake? Thanks,\\nR.</p>\\n\\n<p>Here is the code:</p>\\n\\n<pre><code>import numpy as np\\ni=0\\nInitialPayment=2500\\nNumberOfPayments=225\\nPayment=100\\nMonthsToMaturity=240\\nFV=50000\\nCashflow=[(0-InitialPayment)]\\nwhile i&lt; NumberOfPayments:\\n    Cashflow.append (0-Payment)\\n    i +=1\\nwhile i&lt; (MonthsToMaturity):\\n    Cashflow.append (0)\\n    i  +=1\\nCashflow.append (FV)\\nCashflow = np.reshape(Cashflow,-1)\\nprint (\"Cashflow: \") +str(Cashflow)\\nIRR = np.irr([Cashflow])\\nprint (\"IRR\") +str(IRR)\\n</code></pre>\\n'}, {'is_answered': False, 'view_count': 192, 'answer_count': 0, 'score': 0, 'last_activity_date': 1561747576, 'creation_date': 1561747576, 'question_id': 56811864, 'link': 'https://stackoverflow.com/questions/56811864/filling-a-pandas-series-with-missing-periodindex-values', 'title': 'Filling a pandas series with missing periodIndex values', 'body': '<p>Given a pandas series with a PeriodIndex, which has some missing values, how \\ndoes one fill in missing periods, with value 0.</p>\\n\\n<p><strong>Example:</strong></p>\\n\\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\\n\\nindex = pd.PeriodIndex([\\'2019-01\\', \\'2019-02\\', \\'2019-04\\'], freq=\\'M\\')\\nseries = pd.Series([1, 1, 1], index=index)\\n\\n# Do some magic with the series\\n</code></pre>\\n\\n<p>Series should now be:</p>\\n\\n<pre><code>\\'2019-01\\' 1\\n\\'2019-02\\' 1\\n\\'2019-03\\' 0\\n\\'2019-04\\' 1\\n</code></pre>\\n\\n<p><strong>What I\\'ve tried:</strong></p>\\n\\n<p>I\\'ve tried using <code>resample(\\'M\\').fillna()</code>, but found the new options on <code>fillna()</code> don\\'t include filling with 0, and unlike previous questions I\\'ve found on stackoverflow, I\\'m using a later version of pandas, where <code>fill_method</code> is no longer available.</p>\\n\\n<p>I\\'ve also tried reindexing with the index from the series itself, but this still appears to leave holes.</p>\\n\\n<p><strong>Context:</strong>\\nIn case this is a case of \"you\\'re just doing it the wrong way!\" I thought I\\'d some context. I\\'m using a series with a periodindex to model a predicted cashflow for a loan, and calling numpy.irr on the series to calculate it\\'s rate of return. Unfortunately, numpy irr is unaware of indices, so I need to convert the loan into a full array, where empty months are given by 0 values, but there are always 12 values per year.</p>\\n\\n<p>I\\'m open to hearing other methods of calculating this - with libraries, or DateTimeIndex or any other methods.</p>\\n'}, {'is_answered': True, 'view_count': 43, 'accepted_answer_id': 62637243, 'answer_count': 1, 'score': 0, 'last_activity_date': 1593430593, 'creation_date': 1593427924, 'question_id': 62636533, 'link': 'https://stackoverflow.com/questions/62636533/is-there-a-way-to-implement-the-irr-algorithm-in-pulp', 'title': 'Is there a way to implement the IRR algorithm in PuLP?', 'body': \"<p>Numpy has a function which allows me to calculate the IRR of an array of floats. My problem is that I am trying to use it within a PuLP problem, and the array that I want to pass to the function is compossed of the variables of the problem. Here i</p>\\n<pre><code>    problem = pulp.LpProblem(&quot;TIR_MINIMIZE&quot;, pulp.LpMaximize)\\n    price_ppa = pulp.LpVariable(&quot;price_ppa&quot;)\\n    price_production = []\\n\\n    for i in range(10):\\n        price_production.append(price_ppa * annual_production[i])\\n        # anual_production is an array of values calculated outside the function\\n\\n    irr = numpy.irr(price_production)\\n\\n    # CONSTRAINTS #####################################################################################\\n    problem += irr&gt;= 0.075\\n\\n    objective_function = -irr\\n    problem += objective_function\\n\\n    #####################################################################################################\\n    problem.solve()\\n</code></pre>\\n<p>And this code doesn't work because numpy.irr is expecting an array of floats, while I pass it an array of LpAffineExpressions. My question is, is there a way to implement this in a somewhat easy way? I have tried to implement the algorithm manually, but I can't do it inside the PuLP constraint definition.</p>\\n\"}], 'has_more': False, 'quota_max': 300, 'quota_remaining': 120}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▎        | 1/8 [00:01<00:07,  1.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [{'is_answered': True, 'view_count': 177, 'accepted_answer_id': 55707056, 'answer_count': 1, 'score': 2, 'last_activity_date': 1555436599, 'creation_date': 1555383187, 'question_id': 55699890, 'link': 'https://stackoverflow.com/questions/55699890/adding-a-column-to-a-data-frame-using-the-output-of-numpy-npv-function', 'title': 'Adding a column to a data frame using the output of numpy.npv function', 'body': \"<p>I am trying to use 2 columns in my pyspark data frame to calculate the Net Present Value using the numpy.npv() function.I am getting the following error</p>\\n\\n<blockquote>\\n  <p>return (values / (1+rate)**np.arange(0, len(values))).sum(axis=0)\\n  TypeError: len() of unsized object</p>\\n</blockquote>\\n\\n<p>I have also unsuccessfully tried to use the numpy.npv function as udf </p>\\n\\n<p>Requesting help to resolve this issue</p>\\n\\n<pre><code># Creating the DataFrame\\ndf = sc.parallelize([('a',1,100),('a',2,200),('a',3,300),('a',4,400), \\n('a',5,500),('a',6,600),('b',1,23),('b',2,32),('b',3,34),('b',4,55), \\n('b',5,43)]).toDF(['Name','yr','cash'])\\ndf.show()\\n\\n# Loading the requisite packages\\nfrom pyspark.sql import Window\\nfrom pyspark.sql.functions import col, collect_list\\nimport numpy as np\\nw = (Window.partitionBy('Name').orderBy(col('yr').desc()).rangeBetween(Window.unboundedPreceding, 0))\\n\\ndf = df.withColumn('cash_list', collect_list('cash').over(w))    \\ndf.show(truncate=False)\\ndf = df.withColumn('discount_rate', lit(0.3))\\n\\n#calculate npv\\ndf = df.withColumn('npv_value', np.npv(df.discount_rate, df.cash_list))\\n</code></pre>\\n\"}, {'is_answered': True, 'view_count': 151, 'accepted_answer_id': 67816763, 'answer_count': 1, 'score': 1, 'last_activity_date': 1622713269, 'creation_date': 1622686512, 'question_id': 67814572, 'link': 'https://stackoverflow.com/questions/67814572/how-to-find-discount-rate-value-if-total-present-value-and-future-time-series-of', 'title': 'How to find discount rate value if total present value and future time series of cash flows is known in Python', 'body': '<p>While going through an <a href=\"http://people.stern.nyu.edu/adamodar/podcasts/valUGspr21/session6slides.pdf\" rel=\"nofollow noreferrer\">article</a>, I encountered a situation where I encountered below polynomial equation.</p>\\n<p>For reference, below is the equation.</p>\\n<pre><code>15446 = 537.06/(1+r) + 612.25/(1+r)**2 + 697.86/(1+r)**3 + 795.67/(1+r)**4 + 907.07/(1+r)**5\\n</code></pre>\\n<p>This is discount cash flow time series values which we use in finance to get the idea of present value of future cash flows after applying the appropriate discount rate.</p>\\n<p>So from above equation, I need to calculate the variable <code>r</code> in python programming environment?. I do hope that there must be some library which can be used to solve such equations?.</p>\\n<p>I solve this, I thought to use the <a href=\"https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.npv.html\" rel=\"nofollow noreferrer\">numpy.npv</a> API.</p>\\n<pre><code>import numpy as np\\npresentValue = 15446\\nfutureValueList = [537.06, 612.25, 697.86,795.67, 907.07]\\n// I know it is not possible to get r from below. Just put\\n// it like this to describe my intention.\\npresentValue = np.npv(r, futureValueList)\\nprint(r)\\n</code></pre>\\n'}, {'is_answered': False, 'view_count': 18, 'answer_count': 0, 'score': 0, 'last_activity_date': 1631800423, 'creation_date': 1631800423, 'question_id': 69209716, 'link': 'https://stackoverflow.com/questions/69209716/how-can-we-use-numpy-npv-to-group-by-item-and-calculate-npvs-per-group', 'title': 'How can we use numpy.npv to group by item and calculate NPVs per group?', 'body': '<p>I just stumbled across numpy.npv today. Here is some documentation.</p>\\n<p><a href=\"https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.npv.html\" rel=\"nofollow noreferrer\">https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.npv.html</a></p>\\n<p>I have a dataframe that looks like this.</p>\\n<pre><code>import pandas as pd\\n# Intitialise data of lists\\ndata = [{\\'Month\\': \\'2020-01-01\\', \\'Expense\\':1000, \\'Revenue\\':5000, \\'Building\\':\\'Stadium\\'}, \\n       {\\'Month\\': \\'2020-02-01\\', \\'Expense\\':3000, \\'Revenue\\':4000, \\'Building\\':\\'Stadium\\'},\\n       {\\'Month\\': \\'2020-03-01\\', \\'Expense\\':7000, \\'Revenue\\':5000, \\'Building\\':\\'Stadium\\'}, \\n       {\\'Month\\': \\'2020-04-01\\', \\'Expense\\':3000, \\'Revenue\\':4000, \\'Building\\':\\'Stadium\\'},\\n       {\\'Month\\': \\'2020-01-01\\', \\'Expense\\':5000, \\'Revenue\\':6000, \\'Building\\':\\'Casino\\'}, \\n       {\\'Month\\': \\'2020-02-01\\', \\'Expense\\':5000, \\'Revenue\\':4000, \\'Building\\':\\'Casino\\'},\\n       {\\'Month\\': \\'2020-03-01\\', \\'Expense\\':5000, \\'Revenue\\':9000, \\'Building\\':\\'Casino\\'},\\n       {\\'Month\\': \\'2020-04-01\\', \\'Expense\\':6000, \\'Revenue\\':10000, \\'Building\\':\\'Casino\\'}]\\ndf = pd.DataFrame(data)\\ndf\\n</code></pre>\\n<p><a href=\"https://i.stack.imgur.com/5GNXo.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/5GNXo.png\" alt=\"enter image description here\" /></a></p>\\n<p>I\\'m wondering how to get NPV, based on Revenue, for Stadium and Casino. So, maybe it would be another dataframe with one NPV for Stadium and one NPV for Casino. Or, maybe it would be a list. Not sure. Thoughts? Suggestions?</p>\\n'}], 'has_more': False, 'quota_max': 300, 'quota_remaining': 117}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:02<00:01,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 114}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 113}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 113}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 111}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 111}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 110}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 108}\n",
      "{'items': [{'is_answered': True, 'view_count': 67, 'accepted_answer_id': 30516220, 'answer_count': 1, 'score': 2, 'last_activity_date': 1432844373, 'creation_date': 1432843755, 'question_id': 30516180, 'link': 'https://stackoverflow.com/questions/30516180/what-is-that-in-size-dimenson-of-numpy', 'title': 'What is that &quot;.&quot; in size dimenson of NumPy?', 'body': '<p>On <a href=\"http://docs.scipy.org/doc/numpy/reference/generated/numpy.random.random_integers.html#numpy.random.random_integers\" rel=\"nofollow\">http://docs.scipy.org/doc/numpy/reference/generated/numpy.random.random_integers.html#numpy.random.random_integers</a></p>\\n\\n<pre><code>np.random.random_integers(5, size=(3.,2.))\\n</code></pre>\\n\\n<p>I did not understand the \".\" that comes after \"3\"</p>\\n\\n<p>I tried </p>\\n\\n<pre><code>np.random.random_integers(5, size=(3,2))\\n</code></pre>\\n\\n<p>and it seems to be exact same as one with \".\"</p>\\n\\n<p>What am I missing ? </p>\\n'}], 'has_more': False, 'quota_max': 300, 'quota_remaining': 107}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 105}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 104}\n",
      "{'items': [{'is_answered': False, 'view_count': 192, 'answer_count': 0, 'score': 3, 'last_activity_date': 1590194792, 'creation_date': 1583550651, 'question_id': 60574047, 'link': 'https://stackoverflow.com/questions/60574047/apply-an-aggregate-function-to-a-python-datatable-column-after-group-by', 'title': 'Apply an aggregate function to a python datatable column after group by', 'body': '<p>Is it possible to \"apply\" a user function to a python datatable after groupby?</p>\\n\\n<p>For example:</p>\\n\\n<pre><code>import datatable as dt\\nfrom datatable import f, by, sum\\ndf = dt.Frame(SYM=[\\'A\\',\\'A\\',\\'A\\',\\'B\\',\\'B\\'], xval=[1.1,1.2,2.3,2.4,2.5])\\nprint(df[:, sum(f.xval), by(f.SYM)])\\n</code></pre>\\n\\n<p>This works. But I would like to replace the \"sum\" function with a user function defined using:</p>\\n\\n<pre><code>def func(x):\\n    # do some operations here; e.g. ranking\\n    y = x\\n    return(y)    \\n</code></pre>\\n\\n<p>Is this possible? Can you please provide an example (may be using numpy.rank inside func above)?</p>\\n'}, {'is_answered': True, 'view_count': 32, 'accepted_answer_id': 71094925, 'answer_count': 1, 'score': 0, 'last_activity_date': 1644692677, 'creation_date': 1644691590, 'question_id': 71094850, 'link': 'https://stackoverflow.com/questions/71094850/create-a-custom-percentile-rank-for-a-pandas-series', 'title': 'Create a custom percentile rank for a pandas series', 'body': \"<p>I need to calculate the percentile using a specific algorithm that is not available using either pandas.rank() or numpy.rank().</p>\\n<p>The ranking algorithm is calculated as follows for a series:</p>\\n<blockquote>\\n<p>rank[i] = (# of values in series less than i + # of values equal to\\ni*0.5)/total # of values</p>\\n</blockquote>\\n<p>so if I had the following series</p>\\n<pre><code>s=pd.Series(data=[5,3,8,1,9,4,14,12,6,1,1,4,15])\\n</code></pre>\\n<ul>\\n<li><p>For the first element, 5 there are 6 values less than 5 and no other values = to 5.  The rank would be (6+0x0.5)/13 or 6/13.</p>\\n</li>\\n<li><p>For the fourth element (1) it would be (0+ 2x0.5)/13 or 1/13.</p>\\n</li>\\n</ul>\\n<p>How could I calculate this without using a loop?  I assume a combination of <code>s.apply</code> and/or <code>s.where()</code> but can't figure it out and have tried searching. I am looking to apply to the entire series at once, with the result being a series with the percentile ranks.</p>\\n\"}], 'has_more': False, 'quota_max': 300, 'quota_remaining': 103}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 102}\n",
      "{'items': [{'is_answered': False, 'view_count': 152, 'answer_count': 0, 'score': 0, 'last_activity_date': 1558624948, 'creation_date': 1558624948, 'question_id': 56278452, 'link': 'https://stackoverflow.com/questions/56278452/how-to-convert-bytes-to-ndarray-in-python-3-using-numpy-ndarray-tostring-and-n', 'title': 'How to convert bytes to ndarray in python 3 using numpy.ndarray.toString() and numpy.fromString()?', 'body': '<p>In order to save in lmdb , I use numpy.ndarray.toString() to convert an ndarray to a bytes object,and when I want to read the bytes to an ndarray with numpy.fromString(),I get an ndarray but the shape changed.So how to convert between them?</p>\\n\\n<p>I have tried to use pickle with dumps() and loads(),but there was some exception in </p>\\n\\n<pre><code>_pickle.UnpicklingError: invalid load key, \\'?\\'.\\n</code></pre>\\n\\n<p>save code here :</p>\\n\\n<pre class=\"lang-py prettyprint-override\"><code>class face_lmdb:\\n    def add_embed_to_lmdb(self,id,vector):\\n        self.db_file=os.path.abspath(face_comm.get_conf(\\'lmdb\\',\\'lmdb_path\\'))\\n        id = str(id)\\n        evn = lmdb.open(self.db_file)\\n        wfp = evn.begin(write=True)\\n        print(vector.shape)# here is (512,)\\n        wfp.put(key=id.encode(), value=vector.toString())\\n        wfp.commit()\\n        evn.close()\\n</code></pre>\\n\\n<p>load code here</p>\\n\\n<pre class=\"lang-py prettyprint-override\"><code>    def create_index_from_lmdb(self):\\n        lmdb_file = self.lmdb_file\\n        if os.path.isdir(lmdb_file):\\n            evn = lmdb.open(lmdb_file)\\n            wfp = evn.begin()\\n            annoy = AnnoyIndex(self.f)\\n            for key, value in wfp.cursor():\\n                key = int(key)\\n                print(type(value))#here is bytes\\n                value = np.fromstring(value)\\n                print(value.shape)# here is (256,)\\n                annoy.add_item(key,value)\\n\\n            annoy.build(self.num_trees)\\n            annoy.save(self.annoy_index_path)\\n</code></pre>\\n\\n<p>I expect the shape is <code>(512,)</code>,butI get <code>(256,)</code></p>\\n'}], 'has_more': False, 'quota_max': 300, 'quota_remaining': 101}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 100}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 99}\n",
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 98}\n",
      "{'items': [{'is_answered': True, 'view_count': 2153568, 'protected_date': 1531806256, 'accepted_answer_id': 944733, 'answer_count': 18, 'score': 1383, 'last_activity_date': 1647686548, 'creation_date': 1244035194, 'question_id': 944700, 'link': 'https://stackoverflow.com/questions/944700/how-can-i-check-for-nan-values', 'title': 'How can I check for NaN values?', 'body': \"<p><code>float('nan')</code> results in Nan (not a number). But how do I check for it? Should be very easy, but I cannot find it.</p>\\n\"}, {'is_answered': True, 'view_count': 71726, 'accepted_answer_id': 9033306, 'answer_count': 5, 'score': 776, 'last_activity_date': 1588772170, 'creation_date': 1327664990, 'question_id': 9032856, 'link': 'https://stackoverflow.com/questions/9032856/what-is-the-explanation-for-these-bizarre-javascript-behaviours-mentioned-in-the', 'title': 'What is the explanation for these bizarre JavaScript behaviours mentioned in the &#39;Wat&#39; talk for CodeMash 2012?', 'body': '<p>The <em><a href=\"https://www.destroyallsoftware.com/talks/wat\" rel=\"noreferrer\">\\'Wat\\' talk for CodeMash 2012</a></em> basically points out a few bizarre quirks with Ruby and JavaScript.</p>\\n\\n<p>I have made a JSFiddle of the results at <a href=\"http://jsfiddle.net/fe479/9/\" rel=\"noreferrer\">http://jsfiddle.net/fe479/9/</a>.</p>\\n\\n<p>The behaviours specific to JavaScript (as I don\\'t know Ruby) are listed below.</p>\\n\\n<p>I found in the JSFiddle that some of my results didn\\'t correspond with those in the video, and I am not sure why. I am, however, curious to know how JavaScript is handling working behind the scenes in each case.</p>\\n\\n<pre><code>Empty Array + Empty Array\\n[] + []\\nresult:\\n&lt;Empty String&gt;\\n</code></pre>\\n\\n<p>I am quite curious about the <code>+</code> operator when used with arrays in JavaScript.\\nThis matches the video\\'s result.</p>\\n\\n<pre><code>Empty Array + Object\\n[] + {}\\nresult:\\n[Object]\\n</code></pre>\\n\\n<p>This matches the video\\'s result. What\\'s going on here? Why is this an object. What does the <code>+</code> operator do?</p>\\n\\n<pre><code>Object + Empty Array\\n{} + []\\nresult:\\n[Object]\\n</code></pre>\\n\\n<p>This doesn\\'t match the video. The video suggests that the result is 0, whereas I get [Object].</p>\\n\\n<pre><code>Object + Object\\n{} + {}\\nresult:\\n[Object][Object]\\n</code></pre>\\n\\n<p>This doesn\\'t match the video either, and how does outputting a variable result in two objects? Maybe my JSFiddle is wrong.</p>\\n\\n<pre><code>Array(16).join(\"wat\" - 1)\\nresult:\\nNaNNaNNaNNaNNaNNaNNaNNaNNaNNaNNaNNaNNaNNaNNaNNaN\\n</code></pre>\\n\\n<p>Doing wat + 1 results in <code>wat1wat1wat1wat1</code>...</p>\\n\\n<p>I suspect this is just straightforward behaviour that trying to subtract a number from a string results in NaN.</p>\\n'}, {'is_answered': True, 'view_count': 1914414, 'protected_date': 1526542165, 'accepted_answer_id': 13786327, 'answer_count': 7, 'score': 734, 'last_activity_date': 1646802626, 'creation_date': 1355021438, 'question_id': 13784192, 'link': 'https://stackoverflow.com/questions/13784192/creating-an-empty-pandas-dataframe-then-filling-it', 'title': 'Creating an empty Pandas DataFrame, then filling it?', 'body': '<p>I\\'m starting from the pandas DataFrame docs here: <a href=\"http://pandas.pydata.org/pandas-docs/stable/dsintro.html\" rel=\"noreferrer\">http://pandas.pydata.org/pandas-docs/stable/dsintro.html</a></p>\\n\\n<p>I\\'d like to iteratively fill the DataFrame with values in a time series kind of calculation.\\nSo basically, I\\'d like to initialize the DataFrame with columns A, B and timestamp rows, all 0 or all NaN.</p>\\n\\n<p>I\\'d then add initial values and go over this data calculating the new row from the row before, say <code>row[A][t] = row[A][t-1]+1</code> or so.</p>\\n\\n<p>I\\'m currently using the code as below, but I feel it\\'s kind of ugly and there must be a  way to do this with a DataFrame directly, or just a better way in general.\\nNote: I\\'m using Python 2.7.</p>\\n\\n<pre><code>import datetime as dt\\nimport pandas as pd\\nimport scipy as s\\n\\nif __name__ == \\'__main__\\':\\n    base = dt.datetime.today().date()\\n    dates = [ base - dt.timedelta(days=x) for x in range(0,10) ]\\n    dates.sort()\\n\\n    valdict = {}\\n    symbols = [\\'A\\',\\'B\\', \\'C\\']\\n    for symb in symbols:\\n        valdict[symb] = pd.Series( s.zeros( len(dates)), dates )\\n\\n    for thedate in dates:\\n        if thedate &gt; dates[0]:\\n            for symb in valdict:\\n                valdict[symb][thedate] = 1+valdict[symb][thedate - dt.timedelta(days=1)]\\n\\n    print valdict\\n</code></pre>\\n'}, {'is_answered': True, 'view_count': 1357960, 'protected_date': 1539675375, 'accepted_answer_id': 13842286, 'answer_count': 23, 'score': 686, 'last_activity_date': 1633630590, 'creation_date': 1355323245, 'question_id': 13842088, 'link': 'https://stackoverflow.com/questions/13842088/set-value-for-particular-cell-in-pandas-dataframe-using-index', 'title': 'Set value for particular cell in pandas DataFrame using index', 'body': \"<p>I have created a Pandas DataFrame</p>\\n<pre><code>df = DataFrame(index=['A','B','C'], columns=['x','y'])\\n</code></pre>\\n<p>and have got this</p>\\n<pre>\\n    x    y\\nA  NaN  NaN\\nB  NaN  NaN\\nC  NaN  NaN\\n</pre>\\n<p>Now, I would like to assign a value to particular cell, for example to row <code>C</code> and column <code>x</code>.\\nI would expect to get this result:</p>\\n<pre>\\n    x    y\\nA  NaN  NaN\\nB  NaN  NaN\\nC  10  NaN\\n</pre>\\n<p>with this code:</p>\\n<pre><code>df.xs('C')['x'] = 10\\n</code></pre>\\n<p>However, the contents of <code>df</code> has not changed. The dataframe contains yet again only <code>NaN</code>s.</p>\\n<p>Any suggestions?</p>\\n\"}, {'is_answered': True, 'view_count': 1084096, 'accepted_answer_id': 29530601, 'answer_count': 26, 'score': 609, 'last_activity_date': 1642844040, 'creation_date': 1428556179, 'question_id': 29530232, 'link': 'https://stackoverflow.com/questions/29530232/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe', 'title': 'How to check if any value is NaN in a Pandas DataFrame', 'body': '<p>In Python Pandas, what\\'s the best way to check whether a DataFrame has one (or more) NaN values?</p>\\n\\n<p>I know about the function <code>pd.isnan</code>, but this returns a DataFrame of booleans for each element. <a href=\"https://stackoverflow.com/questions/27754891/python-nan-value-in-pandas\">This post</a> right here doesn\\'t exactly answer my question either.</p>\\n'}, {'is_answered': True, 'view_count': 1066739, 'protected_date': 1530966871, 'accepted_answer_id': 13295801, 'answer_count': 17, 'score': 576, 'last_activity_date': 1648593481, 'creation_date': 1352400639, 'question_id': 13295735, 'link': 'https://stackoverflow.com/questions/13295735/how-to-replace-nan-values-by-zeroes-in-a-column-of-a-pandas-dataframe', 'title': 'How to replace NaN values by Zeroes in a column of a Pandas Dataframe?', 'body': '<p>I have a Pandas Dataframe as below:</p>\\n<pre><code>      itm Date                  Amount \\n67    420 2012-09-30 00:00:00   65211\\n68    421 2012-09-09 00:00:00   29424\\n69    421 2012-09-16 00:00:00   29877\\n70    421 2012-09-23 00:00:00   30990\\n71    421 2012-09-30 00:00:00   61303\\n72    485 2012-09-09 00:00:00   71781\\n73    485 2012-09-16 00:00:00     NaN\\n74    485 2012-09-23 00:00:00   11072\\n75    485 2012-09-30 00:00:00  113702\\n76    489 2012-09-09 00:00:00   64731\\n77    489 2012-09-16 00:00:00     NaN\\n</code></pre>\\n<p>When I try to apply a function to the Amount column, I get the following error:</p>\\n<pre><code>ValueError: cannot convert float NaN to integer\\n</code></pre>\\n<p>I have tried applying a function using .isnan from the Math Module\\nI have tried the pandas .replace attribute\\nI tried the .sparse data attribute from pandas 0.9\\nI have also tried if NaN == NaN statement in a function.\\nI have also looked at this article <a href=\"https://stackoverflow.com/questions/8161836/how-do-i-replace-na-values-with-zeros-in-r\">How do I replace NA values with zeros in an R dataframe?</a> whilst looking at some other articles.\\nAll the methods I have tried have not worked or do not recognise NaN.\\nAny Hints or solutions would be appreciated.</p>\\n'}, {'is_answered': True, 'view_count': 488767, 'protected_date': 1562192230, 'accepted_answer_id': 26838140, 'answer_count': 8, 'score': 329, 'last_activity_date': 1642242291, 'creation_date': 1415600966, 'question_id': 26837998, 'link': 'https://stackoverflow.com/questions/26837998/pandas-replace-nan-with-blank-empty-string', 'title': 'Pandas Replace NaN with blank/empty string', 'body': '<p>I have a Pandas Dataframe as shown below:</p>\\n\\n<pre><code>    1    2       3\\n 0  a  NaN    read\\n 1  b    l  unread\\n 2  c  NaN    read\\n</code></pre>\\n\\n<p>I want to remove the NaN values with an empty string so that it looks like so:</p>\\n\\n<pre><code>    1    2       3\\n 0  a   \"\"    read\\n 1  b    l  unread\\n 2  c   \"\"    read\\n</code></pre>\\n'}, {'is_answered': True, 'view_count': 62096, 'protected_date': 1507028623, 'accepted_answer_id': 1573715, 'answer_count': 11, 'score': 321, 'last_activity_date': 1528041430, 'creation_date': 1255511957, 'question_id': 1565164, 'link': 'https://stackoverflow.com/questions/1565164/what-is-the-rationale-for-all-comparisons-returning-false-for-ieee754-nan-values', 'title': 'What is the rationale for all comparisons returning false for IEEE754 NaN values?', 'body': '<p>Why do comparisons of NaN values behave differently from all other values?\\nThat is, all comparisons with the operators ==, &lt;=, >=, &lt;, > where one or both values is NaN returns false, contrary to the behaviour of all other values.</p>\\n\\n<p>I suppose this simplifies numerical computations in some way, but I couldn\\'t find an explicitly stated reason, not even in the <a href=\"http://www.cs.berkeley.edu/~wkahan/ieee754status/IEEE754.PDF\">Lecture Notes on the Status of IEEE 754</a> by Kahan which discusses other design decisions in detail.</p>\\n\\n<p>This deviant behavior is causing trouble when doing simple data processing. For example, when sorting a list of records w.r.t. some real-valued field in a C program I need to write extra code to handle NaN as the maximal element, otherwise the sort algorithm could become confused.</p>\\n\\n<p><strong>Edit:</strong>\\nThe answers so far all argue that it is meaningless to compare NaNs.</p>\\n\\n<p>I agree, but that doesn\\'t mean that the correct answer is false,\\nrather it would be a Not-a-Boolean (NaB), which fortunately doesn\\'t exist.</p>\\n\\n<p>So the choice of returning true or false for comparisons is in my view arbitrary,\\nand for general data processing it would be advantageous if it obeyed the usual laws\\n(reflexivity of ==, trichotomy of &lt;, ==, >),\\nlest data structures which rely on these laws become confused.</p>\\n\\n<p>So I\\'m asking for some concrete advantage of breaking these laws, not just philosophical reasoning.</p>\\n\\n<p><strong>Edit 2:</strong>\\nI think I understand now why making NaN maximal would be a bad idea, it would mess up the computation of upper limits.</p>\\n\\n<p>NaN != NaN might be desirable to avoid detecting convergence in a loop such as</p>\\n\\n<pre><code>while (x != oldX) {\\n    oldX = x;\\n    x = better_approximation(x);\\n}\\n</code></pre>\\n\\n<p>which however should better be written by comparing the absolute difference with a small limit.\\nSo IMHO this is a relatively weak argument for breaking reflexivity at NaN.</p>\\n'}, {'is_answered': True, 'view_count': 579110, 'protected_date': 1562320498, 'accepted_answer_id': 11620982, 'answer_count': 12, 'score': 314, 'last_activity_date': 1615833371, 'creation_date': 1343079414, 'question_id': 11620914, 'link': 'https://stackoverflow.com/questions/11620914/removing-nan-values-from-an-array', 'title': 'Removing nan values from an array', 'body': '<p>I want to figure out how to remove nan values from my array. My array looks something like this: </p>\\n\\n<pre><code>x = [1400, 1500, 1600, nan, nan, nan ,1700] #Not in this exact configuration\\n</code></pre>\\n\\n<p>How can I remove the <code>nan</code> values from <code>x</code>?</p>\\n'}, {'is_answered': True, 'view_count': 241620, 'accepted_answer_id': 7540412, 'answer_count': 11, 'score': 296, 'last_activity_date': 1647436031, 'creation_date': 1316882963, 'question_id': 7540397, 'link': 'https://stackoverflow.com/questions/7540397/convert-nan-to-0-in-javascript', 'title': 'Convert NaN to 0 in JavaScript', 'body': '<p>Is there a way to convert NaN values to 0 without an <em>if</em> statement? Example:</p>\\n<pre><code>if (isNaN(a)) a = 0;\\n</code></pre>\\n<p>It is very annoying to check my variables every time.</p>\\n'}, {'is_answered': True, 'view_count': 540737, 'protected_date': 1530138521, 'accepted_answer_id': 18691949, 'answer_count': 12, 'score': 261, 'last_activity_date': 1625650884, 'creation_date': 1378684445, 'question_id': 18689823, 'link': 'https://stackoverflow.com/questions/18689823/pandas-dataframe-replace-nan-values-with-average-of-columns', 'title': 'pandas DataFrame: replace nan values with average of columns', 'body': '<p>I\\'ve got a pandas DataFrame filled mostly with real numbers, but there is a few <code>nan</code> values in it as well.</p>\\n\\n<p>How can I replace the <code>nan</code>s with averages of columns where they are?</p>\\n\\n<p>This question is very similar to this one: <a href=\"https://stackoverflow.com/questions/18689235/numpy-array-replace-nan-values-with-average-of-columns\">numpy array: replace nan values with average of columns</a>  but, unfortunately, the solution given there doesn\\'t work for a pandas DataFrame.</p>\\n'}, {'is_answered': True, 'view_count': 579259, 'accepted_answer_id': 9873379, 'answer_count': 18, 'score': 244, 'last_activity_date': 1648325300, 'creation_date': 1332769134, 'question_id': 9873197, 'link': 'https://stackoverflow.com/questions/9873197/how-to-convert-date-to-timestamp', 'title': 'How to convert date to timestamp?', 'body': '<p>I want to convert date to timestamp, my input is <code>26-02-2012</code>. I used </p>\\n\\n<pre><code>new Date(myDate).getTime();\\n</code></pre>\\n\\n<p>It says NaN.. Can any one tell how to convert this?</p>\\n'}, {'is_answered': True, 'view_count': 239997, 'protected_date': 1603855035, 'answer_count': 7, 'score': 227, 'last_activity_date': 1629919076, 'creation_date': 1377437321, 'question_id': 18429491, 'link': 'https://stackoverflow.com/questions/18429491/pandas-groupby-columns-with-nan-missing-values', 'title': 'pandas GroupBy columns with NaN (missing) values', 'body': \"<p>I have a DataFrame with many missing values in columns which I wish to groupby:</p>\\n\\n<pre><code>import pandas as pd\\nimport numpy as np\\ndf = pd.DataFrame({'a': ['1', '2', '3'], 'b': ['4', np.NaN, '6']})\\n\\nIn [4]: df.groupby('b').groups\\nOut[4]: {'4': [0], '6': [2]}\\n</code></pre>\\n\\n<p>see that Pandas has dropped the rows with NaN target values. (I want to include these rows!)</p>\\n\\n<p><em>Since I need many such operations (many cols have missing values), and use more complicated functions than just medians (typically random forests), I want to avoid writing too complicated pieces of code.</em></p>\\n\\n<p>Any suggestions? Should I write a function for this or is there a simple solution?</p>\\n\"}, {'is_answered': True, 'view_count': 287636, 'accepted_answer_id': 36226137, 'answer_count': 14, 'score': 224, 'last_activity_date': 1644326298, 'creation_date': 1458931836, 'question_id': 36226083, 'link': 'https://stackoverflow.com/questions/36226083/how-to-find-which-columns-contain-any-nan-value-in-pandas-dataframe', 'title': 'How to find which columns contain any NaN value in Pandas dataframe', 'body': '<p>Given a pandas dataframe containing possible NaN values scattered here and there:</p>\\n\\n<p><strong>Question:</strong> How do I determine which columns contain NaN values? In particular, can I get a list of the column names containing NaNs?</p>\\n'}, {'is_answered': True, 'view_count': 248769, 'accepted_answer_id': 14163209, 'answer_count': 14, 'score': 219, 'last_activity_date': 1646729028, 'creation_date': 1357323966, 'question_id': 14162723, 'link': 'https://stackoverflow.com/questions/14162723/replacing-pandas-or-numpy-nan-with-a-none-to-use-with-mysqldb', 'title': 'Replacing Pandas or Numpy Nan with a None to use with MysqlDB', 'body': \"<p>I am trying to write a Pandas dataframe (or can use a numpy array) to a mysql database using MysqlDB . MysqlDB doesn't seem understand 'nan' and my database throws out an error saying nan is not in the field list. I need to find a way to convert the 'nan' into a NoneType.</p>\\n\\n<p>Any ideas? </p>\\n\"}, {'is_answered': True, 'view_count': 85312, 'answer_count': 10, 'score': 203, 'last_activity_date': 1625139131, 'creation_date': 1252951866, 'question_id': 1423081, 'link': 'https://stackoverflow.com/questions/1423081/json-left-out-infinity-and-nan-json-status-in-ecmascript', 'title': 'JSON left out Infinity and NaN; JSON status in ECMAScript?', 'body': '<p>Any idea why JSON left out NaN and +/- Infinity? It puts Javascript in the strange situation where objects that would otherwise be serializable, are not, if they contain NaN or +/- infinity values.</p>\\n<p>Looks like this has been cast in stone: see <a href=\"https://www.rfc-editor.org/rfc/rfc4627\" rel=\"nofollow noreferrer\">RFC4627</a> and <a href=\"http://www.ecma-international.org/publications/files/ECMA-ST/Ecma-262.pdf\" rel=\"nofollow noreferrer\">ECMA-262</a> (section 24.5.2, JSON.stringify, NOTE 4, page 683 of the ECMA-262 pdf at last edit):</p>\\n<blockquote>\\n<p>Finite numbers are stringified as if by calling <code>ToString(number)</code>. <strong>NaN</strong> and Infinity regardless of sign are represented as the String <code>null</code>.</p>\\n</blockquote>\\n'}, {'is_answered': True, 'view_count': 6751, 'protected_date': 1342118616, 'closed_date': 1398333503, 'accepted_answer_id': 11066117, 'answer_count': 2, 'score': 203, 'last_activity_date': 1342118499, 'creation_date': 1339871713, 'question_id': 11066050, 'link': 'https://stackoverflow.com/questions/11066050/why-are-my-balls-disappearing', 'closed_reason': 'Not suitable for this site', 'title': 'Why are my balls disappearing?', 'body': '<p>Pardon the funny title. I\\'ve created a little graphic demo of 200 balls bouncing and colliding, both against the walls and each other. You can see what I have currently here: <a href=\"http://www.exeneva.com/html5/multipleBallsBouncingAndColliding/\" rel=\"noreferrer\">http://www.exeneva.com/html5/multipleBallsBouncingAndColliding/</a></p>\\n\\n<p>The problem is that whenever they collide with each other, they disappear. I\\'m not sure why. Can someone take a look and help me out?</p>\\n\\n<p>UPDATE: Apparently the balls array has balls with coordinates of NaN. Below is the code where I push balls to the array. I\\'m not entirely sure how the coordinates are getting NaN.</p>\\n\\n<pre><code>// Variables\\nvar numBalls = 200;  // number of balls\\nvar maxSize = 15;\\nvar minSize = 5;\\nvar maxSpeed = maxSize + 5;\\nvar balls = new Array();\\nvar tempBall;\\nvar tempX;\\nvar tempY;\\nvar tempSpeed;\\nvar tempAngle;\\nvar tempRadius;\\nvar tempRadians;\\nvar tempVelocityX;\\nvar tempVelocityY;\\n\\n// Find spots to place each ball so none start on top of each other\\nfor (var i = 0; i &lt; numBalls; i += 1) {\\n  tempRadius = 5;\\n  var placeOK = false;\\n  while (!placeOK) {\\n    tempX = tempRadius * 3 + (Math.floor(Math.random() * theCanvas.width) - tempRadius * 3);\\n    tempY = tempRadius * 3 + (Math.floor(Math.random() * theCanvas.height) - tempRadius * 3);\\n    tempSpeed = 4;\\n    tempAngle = Math.floor(Math.random() * 360);\\n    tempRadians = tempAngle * Math.PI/180;\\n    tempVelocityX = Math.cos(tempRadians) * tempSpeed;\\n    tempVelocityY = Math.sin(tempRadians) * tempSpeed;\\n\\n    tempBall = {\\n      x: tempX, \\n      y: tempY, \\n      nextX: tempX, \\n      nextY: tempY, \\n      radius: tempRadius, \\n      speed: tempSpeed,\\n      angle: tempAngle,\\n      velocityX: tempVelocityX,\\n      velocityY: tempVelocityY,\\n      mass: tempRadius\\n    };\\n    placeOK = canStartHere(tempBall);\\n  }\\n  balls.push(tempBall);\\n}\\n</code></pre>\\n'}, {'is_answered': True, 'view_count': 88908, 'accepted_answer_id': 51997100, 'answer_count': 9, 'score': 201, 'last_activity_date': 1648603592, 'creation_date': 1342636202, 'question_id': 11548005, 'link': 'https://stackoverflow.com/questions/11548005/numpy-or-pandas-keeping-array-type-as-integer-while-having-a-nan-value', 'title': 'NumPy or Pandas: Keeping array type as integer while having a NaN value', 'body': \"<p>Is there a preferred way to keep the data type of a <code>numpy</code> array fixed as <code>int</code> (or <code>int64</code> or whatever), while still having an element inside listed as <code>numpy.NaN</code>?</p>\\n\\n<p>In particular, I am converting an in-house data structure to a Pandas DataFrame. In our structure, we have integer-type columns that still have NaN's (but the dtype of the column is int). It seems to recast everything as a float if we make this a DataFrame, but we'd really like to be <code>int</code>.</p>\\n\\n<p>Thoughts?</p>\\n\\n<p><strong>Things tried:</strong></p>\\n\\n<p>I tried using the <code>from_records()</code> function under pandas.DataFrame, with <code>coerce_float=False</code> and this did not help. I also tried using NumPy masked arrays, with NaN fill_value, which also did not work. All of these caused the column data type to become a float.</p>\\n\"}, {'is_answered': True, 'view_count': 176144, 'accepted_answer_id': 28312011, 'answer_count': 7, 'score': 199, 'last_activity_date': 1619610810, 'creation_date': 1423011437, 'question_id': 28311655, 'link': 'https://stackoverflow.com/questions/28311655/ignoring-nans-with-str-contains', 'title': 'Ignoring NaNs with str.contains', 'body': '<p>I want to find rows that contain a string, like so:</p>\\n\\n<pre><code>DF[DF.col.str.contains(\"foo\")]\\n</code></pre>\\n\\n<p>However, this fails because some elements are NaN:</p>\\n\\n<blockquote>\\n  <p>ValueError: cannot index with vector containing NA / NaN values</p>\\n</blockquote>\\n\\n<p>So I resort to the obfuscated</p>\\n\\n<pre><code>DF[DF.col.notnull()][DF.col.dropna().str.contains(\"foo\")]\\n</code></pre>\\n\\n<p>Is there a better way?</p>\\n'}, {'is_answered': True, 'view_count': 83070, 'accepted_answer_id': 2801629, 'answer_count': 21, 'score': 198, 'last_activity_date': 1600731830, 'creation_date': 1273483831, 'question_id': 2801601, 'link': 'https://stackoverflow.com/questions/2801601/why-does-typeof-nan-return-number', 'title': 'Why does typeof NaN return &#39;number&#39;?', 'body': \"<p>Just out of curiosity. </p>\\n\\n<p>It doesn't seem very logical that <code>typeof NaN</code> is number. Just like <code>NaN === NaN</code> or <code>NaN == NaN</code> returning false, by the way. Is this one of the peculiarities of javascript, or would there be a reason for this?</p>\\n\\n<p>Edit: thanks for your answers. It's not an easy thing to get ones head around though. Reading answers and the wiki I understood more, but still, a sentence like </p>\\n\\n<blockquote>\\n  <p>A comparison with a NaN always returns an unordered result even when comparing with itself. The comparison predicates are either signaling or non-signaling, the signaling versions signal an invalid exception for such comparisons. The equality and inequality predicates are non-signaling so x = x returning false can be used to test if x is a quiet NaN. </p>\\n</blockquote>\\n\\n<p>just keeps my head spinning. If someone can translate this in human (as opposed to, say, mathematician) readable language, I would be gratefull.</p>\\n\"}, {'is_answered': True, 'view_count': 252167, 'accepted_answer_id': 20027386, 'answer_count': 6, 'score': 195, 'last_activity_date': 1640012252, 'creation_date': 1384649769, 'question_id': 20025882, 'link': 'https://stackoverflow.com/questions/20025882/add-a-string-prefix-to-each-value-in-a-string-column-using-pandas', 'title': 'add a string prefix to each value in a string column using Pandas', 'body': \"<p>I would like to append a string to the start of each value in a said column of a pandas dataframe (elegantly).\\nI already figured out how to kind-of do this and I am currently using:</p>\\n\\n<pre><code>df.ix[(df['col'] != False), 'col'] = 'str'+df[(df['col'] != False), 'col']\\n</code></pre>\\n\\n<p>This seems one hell of an inelegant thing to do - do you know any other way (which maybe also adds the character to rows where that column is 0 or NaN)?</p>\\n\\n<p>In case this is yet unclear, I would like to turn:</p>\\n\\n<pre><code>    col \\n1     a\\n2     0\\n</code></pre>\\n\\n<p>into:</p>\\n\\n<pre><code>       col \\n1     stra\\n2     str0\\n</code></pre>\\n\"}, {'is_answered': True, 'view_count': 173798, 'protected_date': 1562192420, 'accepted_answer_id': 12307162, 'answer_count': 7, 'score': 179, 'last_activity_date': 1639736236, 'creation_date': 1346959945, 'question_id': 12307099, 'link': 'https://stackoverflow.com/questions/12307099/modifying-a-subset-of-rows-in-a-pandas-dataframe', 'title': 'Modifying a subset of rows in a pandas dataframe', 'body': \"<p>Assume I have a pandas DataFrame with two columns, A and B. I'd like to modify this DataFrame (or create a copy) so that B is always NaN whenever A is 0. How would I achieve that?</p>\\n\\n<p>I tried the following</p>\\n\\n<pre><code>df['A'==0]['B'] = np.nan\\n</code></pre>\\n\\n<p>and</p>\\n\\n<pre><code>df['A'==0]['B'].values.fill(np.nan)\\n</code></pre>\\n\\n<p>without success.</p>\\n\"}, {'is_answered': True, 'view_count': 54560, 'closed_date': 1400109850, 'accepted_answer_id': 10059796, 'answer_count': 6, 'score': 152, 'last_activity_date': 1577750053, 'creation_date': 1333651381, 'question_id': 10034149, 'link': 'https://stackoverflow.com/questions/10034149/why-is-nan-not-equal-to-nan', 'closed_reason': 'Duplicate', 'title': 'Why is NaN not equal to NaN?', 'body': '<p>The relevant IEEE standard defines a numeric constant NaN (not a number) and prescribes that NaN should compare as not equal to itself. Why is that?</p>\\n\\n<p>All the languages I\\'m familiar with implement this rule. But it often causes significant problems, for example unexpected behavior when NaN is stored in a container, when NaN is in the data that is being sorted, etc. Not to mention, the vast majority of programmers expect any object to be equal to itself (before they learn about NaN), so surprising them adds to the bugs and confusion.</p>\\n\\n<p>IEEE standards are well thought out, so I am sure there is a good reason why NaN comparing as equal to itself would be bad. I just can\\'t figure out what it is.</p>\\n\\n<p>Edit: please refer to <a href=\"https://stackoverflow.com/questions/1565164/what-is-the-rationale-for-all-comparisons-returning-false-for-ieee754-nan-values\">What is the rationale for all comparisons returning false for IEEE754 NaN values?</a> as the authoritative answer.</p>\\n'}, {'is_answered': True, 'view_count': 224522, 'protected_date': 1599212072, 'closed_date': 1593865349, 'accepted_answer_id': 24040239, 'answer_count': 4, 'score': 148, 'last_activity_date': 1594109304, 'creation_date': 1401889150, 'question_id': 24039023, 'link': 'https://stackoverflow.com/questions/24039023/add-column-with-constant-value-to-pandas-dataframe', 'closed_reason': 'Duplicate', 'title': 'Add column with constant value to pandas dataframe', 'body': \"<p>Given a DataFrame:</p>\\n\\n<pre><code>np.random.seed(0)\\ndf = pd.DataFrame(np.random.randn(3, 3), columns=list('ABC'), index=[1, 2, 3])\\ndf\\n\\n          A         B         C\\n1  1.764052  0.400157  0.978738\\n2  2.240893  1.867558 -0.977278\\n3  0.950088 -0.151357 -0.103219\\n</code></pre>\\n\\n<p>What is the simplest way to add a new column containing a constant value eg 0?</p>\\n\\n<pre><code>          A         B         C  new\\n1  1.764052  0.400157  0.978738    0\\n2  2.240893  1.867558 -0.977278    0\\n3  0.950088 -0.151357 -0.103219    0\\n</code></pre>\\n\\n<hr>\\n\\n<p>This is my solution, but I don't know why this puts NaN into 'new' column?</p>\\n\\n<pre><code>df['new'] = pd.Series([0 for x in range(len(df.index))])\\n\\n          A         B         C  new\\n1  1.764052  0.400157  0.978738  0.0\\n2  2.240893  1.867558 -0.977278  0.0\\n3  0.950088 -0.151357 -0.103219  NaN\\n</code></pre>\\n\"}, {'is_answered': True, 'view_count': 59195, 'protected_date': 1393010318, 'accepted_answer_id': 115696, 'answer_count': 9, 'score': 147, 'last_activity_date': 1644271457, 'creation_date': 1222097546, 'question_id': 115548, 'link': 'https://stackoverflow.com/questions/115548/why-is-isnannull-false-in-js', 'title': 'Why is isNaN(null) == false in JS?', 'body': '<p>This code in JS gives me a popup saying \"i think null is a number\", which I find slightly disturbing. What am I missing?</p>\\n\\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\\r\\n<div class=\"snippet-code\">\\r\\n<pre class=\"snippet-code-js lang-js prettyprint-override\"><code>if (isNaN(null)) {\\r\\n  alert(\"null is not a number\");\\r\\n} else {\\r\\n  alert(\"i think null is a number\");\\r\\n}</code></pre>\\r\\n</div>\\r\\n</div>\\r\\n</p>\\n\\n<p>I\\'m using Firefox 3. Is that a browser bug?</p>\\n\\n<p>Other tests:</p>\\n\\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\\r\\n<div class=\"snippet-code\">\\r\\n<pre class=\"snippet-code-js lang-js prettyprint-override\"><code>console.log(null == NaN);   // false\\r\\nconsole.log(isNaN(\"text\")); // true\\r\\nconsole.log(NaN == \"text\"); // false</code></pre>\\r\\n</div>\\r\\n</div>\\r\\n</p>\\n\\n<p>So, the problem seems not to be an exact comparison with NaN?</p>\\n\\n<p><i><b>Edit:</b> Now the question has been answered, I have cleaned up my post to have a better version for the archive. However, this renders some comments and even some answers a little incomprehensible. Don\\'t blame their authors. Among the things I changed was:</p>\\n\\n<ul>\\n<li>Removed a note saying that I had screwed up the headline in the first place by reverting its meaning</li>\\n<li>Earlier answers showed that I didn\\'t state clearly enough why I thought the behaviour was weird, so I added the examples that check a string and do a manual comparison.\\n</i></li>\\n</ul>\\n'}, {'is_answered': True, 'view_count': 142446, 'accepted_answer_id': 11005208, 'answer_count': 5, 'score': 144, 'last_activity_date': 1638289059, 'creation_date': 1338683917, 'question_id': 10867028, 'link': 'https://stackoverflow.com/questions/10867028/get-pandas-read-csv-to-read-empty-values-as-empty-string-instead-of-nan', 'title': 'Get pandas.read_csv to read empty values as empty string instead of nan', 'body': '<p>I\\'m using the pandas library to read in some CSV data.  In my data, certain columns contain strings.  The string <code>\"nan\"</code> is a possible value, as is an empty string.  I managed to get pandas to read \"nan\" as a string, but I can\\'t figure out how to get it not to read an empty value as NaN.  Here\\'s sample data and output</p>\\n\\n<pre><code>One,Two,Three\\na,1,one\\nb,2,two\\n,3,three\\nd,4,nan\\ne,5,five\\nnan,6,\\ng,7,seven\\n\\n&gt;&gt;&gt; pandas.read_csv(\\'test.csv\\', na_values={\\'One\\': [], \"Three\": []})\\n    One  Two  Three\\n0    a    1    one\\n1    b    2    two\\n2  NaN    3  three\\n3    d    4    nan\\n4    e    5   five\\n5  nan    6    NaN\\n6    g    7  seven\\n</code></pre>\\n\\n<p>It correctly reads \"nan\" as the string \"nan\\', but still reads the empty cells as NaN.  I tried passing in <code>str</code> in the <code>converters</code> argument to read_csv (with <code>converters={\\'One\\': str})</code>), but it still reads the empty cells as NaN.</p>\\n\\n<p>I realize I can fill the values after reading, with fillna, but is there really no way to tell pandas that an empty cell in a particular CSV column should be read as an empty string instead of NaN?</p>\\n'}, {'is_answered': True, 'view_count': 181150, 'accepted_answer_id': 26510251, 'answer_count': 9, 'score': 143, 'last_activity_date': 1642631119, 'creation_date': 1413883585, 'question_id': 26483254, 'link': 'https://stackoverflow.com/questions/26483254/python-pandas-insert-list-into-a-cell', 'title': 'Python pandas insert list into a cell', 'body': \"<p>I have a list 'abc' and a dataframe 'df':</p>\\n\\n<pre><code>abc = ['foo', 'bar']\\ndf =\\n    A  B\\n0  12  NaN\\n1  23  NaN\\n</code></pre>\\n\\n<p>I want to insert the list into cell 1B, so I want this result:</p>\\n\\n<pre><code>    A  B\\n0  12  NaN\\n1  23  ['foo', 'bar']\\n</code></pre>\\n\\n<p>Ho can I do that?</p>\\n\\n<p>1) If I use this:</p>\\n\\n<pre><code>df.ix[1,'B'] = abc\\n</code></pre>\\n\\n<p>I get the following error message:</p>\\n\\n<pre><code>ValueError: Must have equal len keys and value when setting with an iterable\\n</code></pre>\\n\\n<p>because it tries to insert the list (that has two elements) into a row / column but not into a cell.</p>\\n\\n<p>2) If I use this:</p>\\n\\n<pre><code>df.ix[1,'B'] = [abc]\\n</code></pre>\\n\\n<p>then it inserts a list that has only one element that is the 'abc' list ( <code>[['foo', 'bar']]</code> ).</p>\\n\\n<p>3) If I use this:</p>\\n\\n<pre><code>df.ix[1,'B'] = ', '.join(abc)\\n</code></pre>\\n\\n<p>then it inserts a string: ( <code>foo, bar</code> ) but not a list.</p>\\n\\n<p>4) If I use this:</p>\\n\\n<pre><code>df.ix[1,'B'] = [', '.join(abc)]\\n</code></pre>\\n\\n<p>then it inserts a list but it has only one element ( <code>['foo, bar']</code> ) but not two as I want ( <code>['foo', 'bar']</code> ).</p>\\n\\n<p>Thanks for help!</p>\\n\\n<hr>\\n\\n<h2>EDIT</h2>\\n\\n<p>My new dataframe and the old list:</p>\\n\\n<pre><code>abc = ['foo', 'bar']\\ndf2 =\\n    A    B         C\\n0  12  NaN      'bla'\\n1  23  NaN  'bla bla'\\n</code></pre>\\n\\n<p>Another dataframe:</p>\\n\\n<pre><code>df3 =\\n    A    B         C                    D\\n0  12  NaN      'bla'  ['item1', 'item2']\\n1  23  NaN  'bla bla'        [11, 12, 13]\\n</code></pre>\\n\\n<p>I want insert the 'abc' list into <code>df2.loc[1,'B']</code> and/or <code>df3.loc[1,'B']</code>.</p>\\n\\n<p>If the dataframe has columns only with integer values and/or NaN values and/or list values then inserting a list into a cell works perfectly. If the dataframe has columns only with string values and/or NaN values and/or list values then inserting a list into a cell works perfectly. But if the dataframe has columns with integer and string values and other columns then the error message appears if I use this: <code>df2.loc[1,'B'] = abc</code> or <code>df3.loc[1,'B'] = abc</code>.</p>\\n\\n<p>Another dataframe:</p>\\n\\n<pre><code>df4 =\\n          A     B\\n0      'bla'  NaN\\n1  'bla bla'  NaN\\n</code></pre>\\n\\n<p>These inserts work perfectly: <code>df.loc[1,'B'] = abc</code> or <code>df4.loc[1,'B'] = abc</code>.</p>\\n\"}, {'is_answered': True, 'view_count': 178225, 'accepted_answer_id': 6736970, 'answer_count': 8, 'score': 142, 'last_activity_date': 1629296827, 'creation_date': 1311009005, 'question_id': 6736590, 'link': 'https://stackoverflow.com/questions/6736590/fast-check-for-nan-in-numpy', 'title': 'Fast check for NaN in NumPy', 'body': '<p>I\\'m looking for the fastest way to check for the occurrence of NaN (<code>np.nan</code>) in a NumPy array <code>X</code>. <code>np.isnan(X)</code> is out of the question, since it builds a boolean array of shape <code>X.shape</code>, which is potentially gigantic.</p>\\n\\n<p>I tried <code>np.nan in X</code>, but that seems not to work because <code>np.nan != np.nan</code>. Is there a fast and memory-efficient way to do this at all?</p>\\n\\n<p>(To those who would ask \"how gigantic\": I can\\'t tell. This is input validation for library code.)</p>\\n'}, {'is_answered': True, 'view_count': 4965, 'accepted_answer_id': 17269376, 'answer_count': 1, 'score': 137, 'last_activity_date': 1503571368, 'creation_date': 1372051096, 'question_id': 17268468, 'link': 'https://stackoverflow.com/questions/17268468/why-is-nan-only-on-the-client-side-why-not-in-node-js', 'title': 'Why {} + {} is NaN only on the client side? Why not in Node.js?', 'body': '<p>While <code>[] + []</code> is an empty string, <code>[] + {}</code> is <code>\"[object Object]\"</code>, and <code>{} + []</code> is <code>0</code>. Why is <code>{} + {}</code> NaN?</p>\\n\\n<pre><code>&gt; {} + {}\\n  NaN\\n</code></pre>\\n\\n<p>My question isn\\'t why <code>({} + {}).toString()</code> is <code>\"[object Object][object Object]\"</code> while <code>NaN.toString()</code> is <code>\"NaN\"</code>, <a href=\"https://stackoverflow.com/a/9033306/1348195\">this part has an answer here already</a>.</p>\\n\\n<p>My question is why does this happen only on the client side? On the server side (<a href=\"http://en.wikipedia.org/wiki/Node.js\" rel=\"noreferrer\">Node.js</a>) <code>{} + {}</code> is <code>\"[object Object][object Object]\"</code>.</p>\\n\\n<pre><code>&gt; {} + {}\\n\\'[object Object][object Object]\\'\\n</code></pre>\\n\\n<hr>\\n\\n<p><strong>Summarizing</strong>:</p>\\n\\n<p>On the client side:</p>\\n\\n<pre><code> [] + []              // Returns \"\"\\n [] + {}              // Returns \"[object Object]\"\\n {} + []              // Returns 0\\n {} + {}              // Returns NaN\\n\\n NaN.toString()       // Returns \"NaN\"\\n ({} + {}).toString() // Returns \"[object Object][object Object]\"\\n var a = {} + {};     // \\'a\\' will be \"[object Object][object Object]\"\\n</code></pre>\\n\\n<p>In Node.js:</p>\\n\\n<pre><code> [] + []   // Returns \"\" (like on the client)\\n [] + {}   // Returns \"[object Object]\" (like on the client)\\n {} + []   // Returns \"[object Object]\" (not like on the client)\\n {} + {}   // Returns \"[object Object][object Object]\" (not like on the client)\\n</code></pre>\\n'}, {'is_answered': True, 'view_count': 305825, 'protected_date': 1562942734, 'answer_count': 14, 'score': 134, 'last_activity_date': 1643150362, 'creation_date': 1389242819, 'question_id': 21011777, 'link': 'https://stackoverflow.com/questions/21011777/how-can-i-remove-nan-from-list-python-numpy', 'title': 'How can I remove Nan from list Python/NumPy', 'body': \"<p>I have a list that countain values, one of the values I got is 'nan'</p>\\n\\n<pre><code>countries= [nan, 'USA', 'UK', 'France']\\n</code></pre>\\n\\n<p>I tried to remove it, but I everytime get an error </p>\\n\\n<pre><code>cleanedList = [x for x in countries if (math.isnan(x) == True)]\\nTypeError: a float is required\\n</code></pre>\\n\\n<p>When I tried this one : </p>\\n\\n<pre><code>cleanedList = cities[np.logical_not(np.isnan(countries))]\\ncleanedList = cities[~np.isnan(countries)]\\n\\nTypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\\n</code></pre>\\n\"}], 'has_more': True, 'quota_max': 300, 'quota_remaining': 97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:31<00:00,  1.06s/it]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12308/3362073580.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnumpy_replacement_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmissing_api\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnumpy_depre_apis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mnumpy_replacement_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmissing_api\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetStackQuestionsv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_api\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\User\\Desktop\\IS706 Software Mining and Analysis\\Group Project\\Stack Overflow API\\StackOverflowAPI\\stack_replacement.py\u001b[0m in \u001b[0;36mgetStackQuestionsv2\u001b[1;34m(missing_api, top_only)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson_normalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilteredArr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"possible_replacement\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"answer_body\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpossible_replacement_api\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_api\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtop_only\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0mtop_candidate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"possible_replacement\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4355\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4356\u001b[0m         \"\"\"\n\u001b[1;32m-> 4357\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4359\u001b[0m     def _reduce(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1100\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m                     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1102\u001b[1;33m                     \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m                 )\n\u001b[0;32m   1104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\pandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\Desktop\\IS706 Software Mining and Analysis\\Group Project\\Stack Overflow API\\StackOverflowAPI\\stack_replacement.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson_normalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilteredArr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"possible_replacement\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"answer_body\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpossible_replacement_api\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_api\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtop_only\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0mtop_candidate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"possible_replacement\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\Desktop\\IS706 Software Mining and Analysis\\Group Project\\Stack Overflow API\\StackOverflowAPI\\stack_replacement.py\u001b[0m in \u001b[0;36mpossible_replacement_api\u001b[1;34m(missing_api, answer_body)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer_body\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"html.parser\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[0mcandidate_apis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"code\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m     \u001b[0mmissing_api_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_to_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_api\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m     \u001b[0mcosine_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_apis\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\Desktop\\IS706 Software Mining and Analysis\\Group Project\\Stack Overflow API\\StackOverflowAPI\\stack_replacement.py\u001b[0m in \u001b[0;36mtext_to_vector\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \"\"\"\n\u001b[0;32m     16\u001b[0m     \u001b[0mWORD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"\\w+\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWORD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "numpy_df = pd.read_csv(\"Labeling - numpy-final.csv\")\n",
    "numpy_depre_apis = numpy_df[\"DEPRECATED_API\"].to_list()\n",
    "numpy_replacement_dict = {}\n",
    "for missing_api in numpy_depre_apis:\n",
    "    numpy_replacement_dict[missing_api] = getStackQuestionsv2(missing_api, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    }
   ],
   "source": [
    "numpy_replacement_dict\n",
    "print(len(numpy_replacement_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy.testing.nosetester\n",
      "numpy.asscalar\n",
      "numpy.fv\n",
      "numpy.nper\n",
      "numpy.ppmt\n",
      "numpy.random.random_integers\n"
     ]
    }
   ],
   "source": [
    "for missing_api, replacement_api in numpy_replacement_dict.items():\n",
    "    if replacement_api != \"\":\n",
    "        print(missing_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scipy Deprecated API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 299}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1/60 [00:01<01:08,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 298}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/60 [00:02<01:05,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 297}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 3/60 [00:03<01:02,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [{'is_answered': True, 'view_count': 2640, 'accepted_answer_id': 26033933, 'answer_count': 2, 'score': 3, 'last_activity_date': 1411669833, 'creation_date': 1411632892, 'question_id': 26033672, 'link': 'https://stackoverflow.com/questions/26033672/creating-a-2-dimensional-numpy-array-with-the-euclidean-distance-from-the-center', 'title': 'Creating a 2-dimensional Numpy array with the euclidean distance from the center', 'body': \"<p>I'm trying to create a 2-dimensional array in Scipy/Numpy where each value represents the euclidean distance from the center. It's supposed to have the same shape as the first two dimensions of a 3-dimensional array (an image, created via <em>scipy.misc.fromimage</em>).</p>\\n\\n<p>Here's one approach that works:</p>\\n\\n<pre><code>def get_distance_1(y, x):\\n    mid_x, mid_y = (scipy.array(image_array.shape[:2]) - 1) / float(2)\\n    return ((y - mid_y) ** 2 + (x - mid_x) ** 2) ** 0.5\\n\\ndistances = scipy.fromfunction(get_distance_1, image_array.shape[:2])\\n</code></pre>\\n\\n<p>This method is fairly fast, but I'm very new to Scipy, and would like to know if there's a more elegant, idiomatic way of doing the same thing. I found the <em>scipy.spatial.distance.cdist</em> function, which seems promising, but I'm at a loss regarding how to fit it into this problem.</p>\\n\\n<pre><code>def get_distance_2(y, x):\\n    mid = ...  # needs to be a array of the shape (rows, cols, 2)?\\n    return scipy.spatial.distance.cdist(scipy.dstack((y, x)), mid)\\n</code></pre>\\n\\n<p>Just to clarify, what I'm looking for is something like this (for a 6 x 6 array)</p>\\n\\n<pre><code>[[ 3.53553391  2.91547595  2.54950976  2.54950976  2.91547595  3.53553391]\\n [ 2.91547595  2.12132034  1.58113883  1.58113883  2.12132034  2.91547595]\\n [ 2.54950976  1.58113883  0.70710678  0.70710678  1.58113883  2.54950976]\\n [ 2.54950976  1.58113883  0.70710678  0.70710678  1.58113883  2.54950976]\\n [ 2.91547595  2.12132034  1.58113883  1.58113883  2.12132034  2.91547595]\\n [ 3.53553391  2.91547595  2.54950976  2.54950976  2.91547595  3.53553391]]\\n</code></pre>\\n\"}], 'has_more': False, 'quota_max': 300, 'quota_remaining': 296}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.07s/it]\n",
      "  7%|▋         | 4/60 [00:05<01:16,  1.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [{'is_answered': True, 'view_count': 8580, 'accepted_answer_id': 12485079, 'answer_count': 2, 'score': 16, 'last_activity_date': 1560014628, 'creation_date': 1347960698, 'question_id': 12474182, 'link': 'https://stackoverflow.com/questions/12474182/asynchronously-read-and-process-an-image-in-python', 'title': 'Asynchronously read and process an image in python', 'body': '<p><strong>Context</strong></p>\\n\\n<p>I often found myself in the following situation:</p>\\n\\n<ul>\\n<li>I have a list of image filenames I need to process</li>\\n<li>I read each image sequentially using for instance scipy.misc.imread</li>\\n<li>Then I do some kind of processing on each image and return a result</li>\\n<li>I save the result along the image filename into a Shelf</li>\\n</ul>\\n\\n<p>The problem is that simply reading the image takes a non negligible amount of time, sometime comparable or even longer than the image processing.</p>\\n\\n<p><strong>Question</strong></p>\\n\\n<p>So I was thinking that ideally I could read image n  + 1 while processing image n. Or even better processing and reading multiple images at once in an automagically determined optimal way ?</p>\\n\\n<p>I have read about multiprocessing, threads, twisted, gevent and the like but I can\\'t figure out which one to use and how to implement this idea. Does anyone have a solution to this kind of issue ?</p>\\n\\n<p><strong>Minimal example</strong></p>\\n\\n<pre><code># generate a list of images\\nscipy.misc.imsave(\"lena.png\", scipy.misc.lena())\\nfiles = [\\'lena.png\\'] * 100\\n\\n# a simple image processing task\\ndef process_image(im, threshold=128):\\n    label, n = scipy.ndimage.label(im &gt; threshold)\\n    return n\\n\\n# my current main loop\\nfor f in files:\\n    im = scipy.misc.imread(f)\\n    print process_image(im)\\n</code></pre>\\n'}, {'is_answered': True, 'view_count': 37398, 'accepted_answer_id': 55473910, 'answer_count': 6, 'score': 12, 'last_activity_date': 1602846500, 'creation_date': 1548511974, 'question_id': 54379176, 'link': 'https://stackoverflow.com/questions/54379176/valueerror-could-not-find-a-format-to-read-the-specified-file-in-mode-i', 'title': 'ValueError: Could not find a format to read the specified file in mode &#39;i&#39;', 'body': '<p>I am trying to read a png file into a python-flask application running in docker and am getting an error that says</p>\\n\\n<blockquote>\\n  <p>ValueError: Could not find a format to read the specified file in mode\\n  \\'i\\'</p>\\n</blockquote>\\n\\n<p>i have uploaded a file using an HTML file and now i am trying to read it for further processing. i see that scipy.misc.imread is deprecated and i am trying to replace this with imageio.imread</p>\\n\\n<pre><code>if request.method==\\'POST\\':\\n    file = request.files[\\'image\\']\\n    if not file: \\n        return render_template(\\'index.html\\', label=\"No file\")\\n    #img = misc.imread(file)\\n    img = imageio.imread(file)\\n</code></pre>\\n\\n<p>i get this error :</p>\\n\\n<pre><code>File \"./appimclass.py\", line 34, in make_prediction\\n\\nimg = imageio.imread(file)\\n\\nFile \"/usr/local/lib/python3.6/site-packages/imageio/core/functions.py\", line 221, in imread\\n\\nreader = read(uri, format, \"i\", **kwargs)\\n\\nFile \"/usr/local/lib/python3.6/site-packages/imageio/core/functions.py\", line 139, in get_reader\\n\\n\"Could not find a format to read the specified file \" \"in mode %r\" % mode\\n</code></pre>\\n'}, {'is_answered': True, 'view_count': 4059, 'accepted_answer_id': 57755495, 'answer_count': 4, 'score': 7, 'last_activity_date': 1567420701, 'creation_date': 1512151249, 'question_id': 47599012, 'link': 'https://stackoverflow.com/questions/47599012/how-to-convert-a-wand-image-object-to-numpy-array-without-opencv', 'title': 'How to convert a wand image object to numpy array (without OpenCV)?', 'body': '<p>I am converting pdf files to image using <a href=\"http://docs.wand-py.org/en/0.4.4/\" rel=\"noreferrer\">Wand</a>. Then, I do further image processing using ndimage. </p>\\n\\n<p>I would like to directly convert the Wand image into a ndarray... I have seen the answer <a href=\"https://stackoverflow.com/a/37032551/8069403\">here</a>, but it use OpenCV. Is this possible without using OpenCV?</p>\\n\\n<p>For the moment I save a temporary file, which is re-opened with scipy.misc.imread()</p>\\n'}, {'is_answered': True, 'view_count': 3586, 'accepted_answer_id': 13508999, 'answer_count': 1, 'score': 4, 'last_activity_date': 1353612673, 'creation_date': 1353564747, 'question_id': 13506814, 'link': 'https://stackoverflow.com/questions/13506814/efficient-processing-of-pixel-neighborhood-in-numpy-image', 'title': 'Efficient processing of pixel + neighborhood in numpy image', 'body': '<p>I have a range image of a scene.  I traverse the image and calculate the average change in depth under the detection window.  The detection windows changes size based on the average depth of the surrounding pixels of the current location.  I accumulate the average change to produce a simple response image.</p>\\n\\n<p>Most of the time is spent in the for loop, it is taking about 40+s for a 512x52 image on my machine. I was hoping for some speed up.  Is there a more efficient/faster way to traverse the image?  Is there a better pythonic/numpy/scipy way to visit each pixel?  Or shall I go learn cython?</p>\\n\\n<p><strong>EDIT:</strong> I have reduced running time to about 18s by using scipy.misc.imread() instead of skimage.io.imread().  Not sure what the difference is, I will try to investigate.</p>\\n\\n<p>Here is a simplified version of the code:</p>\\n\\n<pre><code>import matplotlib.pylab as plt\\nimport numpy as np\\nfrom skimage.io import imread\\nfrom skimage.transform import integral_image, integrate\\nimport time\\n\\ndef intersect(a, b):\\n    \\'\\'\\'Determine the intersection of two rectangles\\'\\'\\'\\n    rect = (0,0,0,0)\\n    r0 = max(a[0],b[0])\\n    c0 = max(a[1],b[1])\\n    r1 = min(a[2],b[2])\\n    c1 = min(a[3],b[3])\\n    # Do we have a valid intersection?\\n    if r1 &gt; r0 and  c1 &gt; c0: \\n         rect = (r0,c0,r1,c1)\\n    return rect\\n\\n# Setup data\\ndepth_src = imread(\"test.jpg\", as_grey=True)\\ndepth_intg = integral_image(depth_src)   # integrate to find sum depth in region\\ndepth_pts = integral_image(depth_src &gt; 0)  # integrate to find num points which have depth\\nboundary = (0,0,depth_src.shape[0]-1,depth_src.shape[1]-1) # rectangle to intersect with\\n\\n# Image to accumulate response\\nout_img = np.zeros(depth_src.shape)\\n\\n# Average dimensions of bbox/detection window per unit length of depth\\nmodel = (0.602,2.044)  # width, height\\n\\nstart_time = time.time()\\nfor (r,c), junk in np.ndenumerate(depth_src):\\n    # Find points around current pixel      \\n    r0, c0, r1, c1 = intersect((r-1, c-1, r+1, c+1), boundary)\\n\\n    # Calculate average of depth of points around current pixel\\n    scale =  integrate(depth_intg, r0, c0, r1, c1) * 255 / 9.0 \\n\\n    # Based on average depth, create the detection window\\n    r0 = r - (model[0] * scale/2)\\n    c0 = c - (model[1] * scale/2)\\n    r1 = r + (model[0] * scale/2)\\n    c1 = c + (model[1] * scale/2)\\n\\n    # Used scale optimised detection window to extract features\\n    r0, c0, r1, c1 = intersect((r0,c0,r1,c1), boundary)\\n    depth_count = integrate(depth_pts,r0,c0,r1,c1)\\n    if depth_count:\\n         depth_sum = integrate(depth_intg,r0,c0,r1,c1)\\n         avg_change = depth_sum / depth_count\\n         # Accumulate response\\n         out_img[r0:r1,c0:c1] += avg_change\\nprint time.time() - start_time, \" seconds\"\\n\\nplt.imshow(out_img)\\nplt.gray()\\nplt.show()\\n</code></pre>\\n'}, {'is_answered': True, 'view_count': 5177, 'accepted_answer_id': 16612617, 'answer_count': 1, 'score': 4, 'last_activity_date': 1368803971, 'creation_date': 1368802994, 'question_id': 16612293, 'link': 'https://stackoverflow.com/questions/16612293/scipy-misc-imread-creates-an-image-with-no-size-or-shape', 'title': 'scipy.misc.imread creates an image with no size or shape', 'body': '<p>I\\'m trying to get scipy.misc.imread to just read in an image and tell me it size, which I\\'ve been able to do fine on other computers but it\\'s failing for me on a new Mac:</p>\\n\\n<pre><code>Python 2.7.4 (default, May 16 2013, 16:40:58) \\n[GCC 4.2.1 Compatible Apple Clang 4.1 ((tags/Apple/clang-421.11.65))] on darwin\\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\\n&gt;&gt;&gt; import scipy.misc\\n&gt;&gt;&gt; scipy.__version__\\n\\'0.11.0\\'\\n&gt;&gt;&gt; image = scipy.misc.imread(\\'test.jpg\\')\\n&gt;&gt;&gt; image\\narray(&lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=870x1110 at 0x10BEC3F80&gt;, dtype=object)\\n&gt;&gt;&gt; image.size\\n1\\n&gt;&gt;&gt; image.shape\\n()\\n</code></pre>\\n\\n<p>So the image is loaded, and the size (870x1110) seems to be present, but when I do image.size or image.shape I get garbage.</p>\\n\\n<p>I use the exact same code on the same image on another machine and get sensible results:</p>\\n\\n<pre><code>&gt;&gt;&gt; image.size\\n237456\\n&gt;&gt;&gt; image.shape\\n(318,249,3)\\n</code></pre>\\n\\n<p>(And incidentally, when I print image on the good machine, it shows as an array of uint8 instead of an array of objects).</p>\\n\\n<p>The bad machine is a Mac, running Mountain Lion, with python 2.7.4 and scipy version 0.11 (I downgraded from 0.12 in case that was the problem). I\\'m running in a virtual environment using venv.</p>\\n\\n<p>Anyone know what\\'s going on? Have I installed PIL badly, or something?</p>\\n'}, {'is_answered': True, 'view_count': 1812, 'answer_count': 1, 'score': 3, 'last_activity_date': 1620154888, 'creation_date': 1457996817, 'question_id': 35999759, 'link': 'https://stackoverflow.com/questions/35999759/python-imread-bug-unsupported-bmp-bitfields-layout', 'title': 'Python imread bug: &quot;Unsupported BMP bitfields layout&quot;', 'body': '<p>I seem to have encountered a bug in scipy.misc.imread, and I\\'m looking for a workaround.  Here\\'s a clip of the error report:</p>\\n\\n<pre><code>from scipy.misc import imread\\nim = imread(\\'380.bmp\\')\\n...\\n\\nC:\\\\Anaconda3\\\\lib\\\\site-packages\\\\PIL\\\\BmpImagePlugin.py in _bitmap(self, header, offset)\\n145                     raw_mode = MASK_MODES[(file_info[\\'bits\\'], file_info[\\'rgb_mask\\'])]\\n146                 else:\\n--&gt; 147                     raise IOError(\"Unsupported BMP bitfields layout\")\\n148             else:\\n149                 raise IOError(\"Unsupported BMP bitfields layout\")\\n\\nOSError: Unsupported BMP bitfields layout\\n</code></pre>\\n\\n<p>I can open the image without problems in an image viewer, so I\\'m sure it\\'s not corrupted. </p>\\n\\n<p>The main question is: What\\'s the best alternative to imread, so I can get around this issue?  Alternatively, if you know a way to fix imread, that would also be good. </p>\\n\\n<p>By the way, I\\'m using Python 3.5.1 in Anaconda 2.4.1 (64 bit)</p>\\n'}, {'is_answered': True, 'view_count': 2381, 'answer_count': 1, 'score': 1, 'last_activity_date': 1576806229, 'creation_date': 1443107253, 'question_id': 32764851, 'link': 'https://stackoverflow.com/questions/32764851/loading-a-32-bit-integer-image-with-opencv', 'title': 'Loading a 32 bit integer image with opencv', 'body': \"<p>I have 32 bit grayscale image data saved in tif files. It stems from background corrected CCD data so the values can also be negative.</p>\\n\\n<p>Working in Python, scipy.misc.imread can load the file with no problems. With opencv however, I didn't manage to get the correct data, it loads all values as NaN, no matter what options I pass imread.</p>\\n\\n<p>My main code is in C++, so I cannot use SciPy and have so far been happy with OpenCV but here it fails me.</p>\\n\\n<p>Any suggestions?</p>\\n\"}, {'is_answered': True, 'view_count': 517, 'accepted_answer_id': 45607603, 'answer_count': 2, 'score': 1, 'last_activity_date': 1502355177, 'creation_date': 1502351118, 'question_id': 45607481, 'link': 'https://stackoverflow.com/questions/45607481/convert-list-to-n-dimension-array-to-feed-to-tflearn', 'title': 'Convert list to n-dimension-array to feed to TFlearn', 'body': '<p>I am developing a CNN to classify images by using TFlearn based on tensorflow, and now I create data sets by using scipy.misc.imread, and I set the image size to 150x150, channels = 3, and now I get a list which contains 4063(the number of my images) (150, 150, 3) array, and now I want to convert it to n-d-array (4063, 150, 150, 3), I don\\'t know how to solve it, please help me. Thank you in advance!</p>\\n\\n<pre><code>import numpy as np\\nimport os\\nimport tensorflow as tf\\nfrom scipy import misc\\nfrom PIL import Image\\n\\nIMAGE_SIZE = 150\\nimage_path = \"dragonfly\"\\n\\nlabels = np.zeros((4063, 1))\\nlabels [0:2363] = 1\\nlabels [2364:4062] = 0\\ntest_labels = np.zeros((200, 1))\\ntest_labels [0:99] = 1\\ntest_labels [100:199] = 0\\n\\nfset = []\\nfns=[os.path.join(root,fn) for root,dirs,files in os.walk(image_path) for fn in files]\\nfor f in fns:\\n    fset.append(f)\\n\\ndef create_train_data():\\n    train_data = []\\n    fns=[os.path.join(root,fn) for root,dirs,files in os.walk(image_path) for fn in files]\\n    for f in fns:\\n        image = misc.imread(f)\\n        image = misc.imresize(image, (IMAGE_SIZE, IMAGE_SIZE, 3))\\n        train_data.append(np.array(image))\\n    return train_data\\n\\ntrain_data = create_train_data()\\nprint (len(train_data))\\n\\ntraining_data = train_data[0:2264] + train_data[2364:3963]\\ntrain_labels = np.concatenate((labels[0:2264], labels[2364:3963]))\\ntest_data = train_data[2264:2364] + train_data[3963:4063]\\n</code></pre>\\n\\n<p>The train_data is what I got, and it\\'s the list which I want to convert</p>\\n'}, {'is_answered': True, 'view_count': 639, 'accepted_answer_id': 47624926, 'answer_count': 1, 'score': 1, 'last_activity_date': 1512347656, 'creation_date': 1512347020, 'question_id': 47624842, 'link': 'https://stackoverflow.com/questions/47624842/slicing-3d-numpy-array-and-setting-rgb-values-of-pixels', 'title': 'Slicing 3d numpy array and setting rgb values of pixels', 'body': \"<p>\\nWhat I want to do is take an image file that I read in (scipy.misc.imread(file)) and change every individual RGB value to an average of the three values for that pixel. </p>\\n\\n<p>For example, I can do this on one individual pixel doing something like this:</p>\\n\\n<pre><code>import numpy as np\\nfrom scipy import misc\\nimport matplotlib.pyplot as plt\\nfrom skimage import data\\n\\nimg = misc.imread('./path/to/file.jpg')\\nprint(img[200, 200]) #[145 165 155]\\nprint(img[200, 200]) = int(np.sum(img[200, 200])/3) # sets RGB values at img[200, 200] to the average of the RGB values in this case, 155\\nprint(img[200, 200]) # changed to [155 155 155]\\n</code></pre>\\n\\n\\n\\n<p>However, I'm new to numpy and ndarrays and I want to know how to apply this to every pixel in the image using slicing. Is this possible? I'm having trouble understanding how to iterate over the entire ndarray and reference the appropriate values to sum and set.</p>\\n\\n<p>Any help is welcome!</p>\\n\\n\\n\\n\\n\"}, {'is_answered': False, 'view_count': 910, 'answer_count': 0, 'score': 1, 'last_activity_date': 1545598985, 'creation_date': 1545598985, 'question_id': 53907137, 'link': 'https://stackoverflow.com/questions/53907137/why-scipy-is-removing-imread-imresize-and-other-functions', 'title': 'Why Scipy is removing imread, imresize and other functions?', 'body': '<p>Today, when I was using scipy.misc.imread function I got the following warning:</p>\\n\\n<pre><code>/home/user/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: `imread` is deprecated!\\n`imread` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\\nUse ``imageio.imread`` instead.\\n  \"\"\"Entry point for launching an IPython kernel.\\n</code></pre>\\n\\n<p>I kind of find this disappointing. For me, this function was performing fine and I have used this in many codes. I come from the MATLAB community and this was a familiar function to me. Nevertheless, I became more disappointed after going to the <a href=\"https://imageio.readthedocs.io/en/stable/userapi.html#imageio.imread\" rel=\"nofollow noreferrer\">imageio imread documentation</a>:</p>\\n\\n<pre><code>imageio.imread(uri, format=None, **kwargs)\\n</code></pre>\\n\\n<p>In the description, there is no indication what the <code>**kwargs</code> are. The full description is probably somewhere in the documentation. But I hope you guys get my point. Besides, this does not have the same functionality as the PIL one.</p>\\n\\n<p>Comparing this to the <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.imread.html\" rel=\"nofollow noreferrer\">scipy imread documentation</a> or the <a href=\"https://www.mathworks.com/help/matlab/ref/imread.html\" rel=\"nofollow noreferrer\">MATLAB imread documentation</a> the documentation of imageio is just pointless.</p>\\n\\n<p>That said, what are the reasons scipy is deprecating these functions?</p>\\n'}, {'is_answered': True, 'view_count': 835, 'accepted_answer_id': 61100058, 'answer_count': 2, 'score': 1, 'last_activity_date': 1586346886, 'creation_date': 1586345980, 'question_id': 61099789, 'link': 'https://stackoverflow.com/questions/61099789/an-alternative-to-read-image-from-scipy-to-cv2', 'title': 'An alternative to read image from scipy to cv2', 'body': \"<p>the following python script I want to convert using opencv python, how do I make it converted?</p>\\n\\n<p>script:  scipy.misc.imread(path, mode='RGB').astype(np.float)</p>\\n\\n<p>I want to convert it using cv2 and what would be alternative for astype(np.float) with this?</p>\\n\\n<pre><code>import cv2\\nimport scipy.misc\\n\\nimg = scipy.misc.imread(path, mode='RGB').astype(np.float)\\n</code></pre>\\n\"}, {'is_answered': False, 'view_count': 615, 'answer_count': 0, 'score': 0, 'last_activity_date': 1511346361, 'creation_date': 1479541287, 'question_id': 40690442, 'link': 'https://stackoverflow.com/questions/40690442/error-when-using-scipy-misc-imread-attributeerror-float', 'title': 'Error when using scipy.misc.imread : AttributeError: __float__', 'body': '<p>I am using scipy.misc.imread to convert png files into an array of floats using:</p>\\n\\n<pre><code>scipy.misc.imread(rawImagePath).astype(scipy.float_)\\n</code></pre>\\n\\n<p>where rawImagePath is the complete path of the file.</p>\\n\\n<p>This is working perfectly the vast majority of the time. However, sometimes I get the error:</p>\\n\\n<pre><code>...\\nreturn scipy.misc.imread(rawImagePath).astype(scipy.float_)\\nFile \"C:\\\\Python27\\\\lib\\\\site-packages\\\\PIL\\\\Image.py\", line 622, in __getattr__\\nraise AttributeError(name)\\nAttributeError: __float__\\n</code></pre>\\n\\n<p>If I then re-run the function on the same file (i.e. same rawImagePath) then it works fine. I\\'ve upgraded to the most recent version of pillow (3.4.2) and scipy (0.18.1). I\\'ve had a look inside Image.py and scipy.misc.imread but I can\\'t work out where it would try to get the float attribute of an Image object.</p>\\n\\n<p>Extra Information:</p>\\n\\n<p>I am running a thread that regularly checks a folder for new files. when it finds a new file it then calls a function which attempts to read the image as an array using the above scipy.misc.imread(...). Could the issue perhaps be something to do with the new file not being complete?</p>\\n\\n<p>Any help greatly appreciated.</p>\\n'}, {'is_answered': False, 'view_count': 305, 'answer_count': 0, 'score': 0, 'last_activity_date': 1502300492, 'creation_date': 1502098013, 'question_id': 45543559, 'link': 'https://stackoverflow.com/questions/45543559/an-error-occured-when-using-scipy-misc-imread', 'title': 'An error occured when using scipy.misc.imread', 'body': \"<p>I am developing a CNN to classify images, and my images are stored in a folder which contains some subfolders, and now I want to use scipy.misc.imread to read images then to feed to tensorflow</p>\\n\\n<p>My codes is as follows:</p>\\n\\n<pre><code>import numpy as np\\nimport os\\nimport tensorflow as tf\\nimport pandas as pd\\nfrom scipy import misc\\nfrom sklearn.preprocessing import LabelEncoder\\n\\n\\n\\nIMAGE_SIZE = 150\\ndata = pd.read_excel('class.xlsx')\\nroot = 'dragonfly'\\n\\n\\n\\n\\ndef label_image(image):\\n    le = LabelEncoder().fit(data.种) \\n    classes = list(le.classes_)\\n    for i, item in enumerate (classes):\\n        if os.path.dirname(image) == item:\\n            label = i\\n    return label\\n\\n\\ndef create_train_data():\\n    train_data = []\\n    for files in os.walk(root):\\n        for file in files:\\n            image = misc.imread(file)\\n            label = label_image(file)\\n            image = misc.imresize(image, (IMAGE_SIZE, IMAGE_SIZE))\\n            train_data.append(np.array(image),np.array(label))\\n    return train_data\\n\\n\\ntrain_data = create_train_data()\\n</code></pre>\\n\\n<p>The 'dragonfly' is the folder which contains all images I have. And now an error occured: </p>\\n\\n<p>PermissionError: [Errno 13] Permission denied: 'dragonfly'</p>\\n\"}, {'is_answered': False, 'view_count': 106, 'answer_count': 0, 'score': 0, 'last_activity_date': 1516566985, 'creation_date': 1516566985, 'question_id': 48371470, 'link': 'https://stackoverflow.com/questions/48371470/scipy-how-to-load-an-image-from-memory-into-scipy-misc-imread', 'title': 'Scipy - How to load an image from memory into scipy.misc.imread?', 'body': \"<p>I am trying to make a REST API on top of Scipy.  I need to use the function scipy.misc.imread(file, 'RGB') to load an image.  However, I don't want to save it to disk because I already have it loaded in memory - I just want to load it from in-memory.</p>\\n\\n<p>How can I do this?</p>\\n\"}, {'is_answered': True, 'view_count': 359, 'accepted_answer_id': 51651289, 'answer_count': 1, 'score': 0, 'last_activity_date': 1533207637, 'creation_date': 1533204762, 'question_id': 51650983, 'link': 'https://stackoverflow.com/questions/51650983/images-feeding-to-deep-neural-network', 'title': 'Images feeding to deep neural network', 'body': \"<p>Let's say you have a folder where your dataset is ( a lot of images ). \\nYou want to feed these images to a Deep neural network for training ( for me I use Tensorflow for now). </p>\\n\\n<p>The first solution that comes to the mind ( very unclassy and beginner solution ) is to store the images in an array. This is Ok for a small dataset but when the dataset is big and pictures are big this is not a viable solution because you we'll not have enough memory. </p>\\n\\n<p>The solution is to read data in batches. </p>\\n\\n<p>I m trying to implement that. The dataset I m interested in is the cultech's Caltech-UCSD Birds 200. The dataset is provide with a text file in which each line contains the path to each image. \\nThis facilitates things. \\nMy solution ( That I m trying to implement ) is to define a class. The template is : </p>\\n\\n<pre><code>class Dataset : \\n          attributes : \\n                  images_paths\\n                  labels \\n                  current_batch_index\\n                  batch_size\\n                  classes_names\\n          methodes : \\n                  get_next_batch() \\n                  shuffle()\\n                  normalize()\\n</code></pre>\\n\\n<p>As soon as I instanciate an object of this class, the paths to all images are stored in the variable images_paths and the ground_truth labels are stored in the labels ( one_hot_encoded).\\nThe method get_next_batch() will use the current_batch_index to return ana array where the we store the actual images using the paths. The size of the array is the batch_size and the index read from the images_path and labes are ( current_batch_index,current_batch_index+batch_size). ( I read images using scipy.misc.imread and reshape them to a fixed shape ( 200x200 ) usig scipy.misc.reshape ).</p>\\n\\n<p>This way I ll use the object to store only a batch in memory and use it in the training loop to feed it to the network.</p>\\n\\n<p>Questions : what do you think of this ? how do you feed your images to the netword normally ? Are there tools to that ? are there tools to split your dataset? </p>\\n\\n<p>F.Y.I : I m using python and tensorflow. Would be interesting to know the answers for these questions for C++ too. </p>\\n\\n<p>THank you and sorry for the long post</p>\\n\"}, {'is_answered': True, 'view_count': 390, 'accepted_answer_id': 65733602, 'answer_count': 1, 'score': 0, 'last_activity_date': 1610703218, 'creation_date': 1610523604, 'question_id': 65697518, 'link': 'https://stackoverflow.com/questions/65697518/imread-of-imageio-to-convert-into-rgb-mode', 'title': 'imread of imageio to convert into RGB mode', 'body': '<p>The deprecated version of SciPy has <a href=\"https://docs.scipy.org/doc/scipy-1.2.1/reference/generated/scipy.misc.imread.html\" rel=\"nofollow noreferrer\">mode</a> option in its arguments.</p>\\n<p>I can use the following line of code</p>\\n<blockquote>\\n<p>scipy.misc.imread(path, mode=\\'RGB\\').astype(np.float)</p>\\n</blockquote>\\n<p>Where <code>path</code> is the path to the image.</p>\\n<p>But, now I am using imageio . Is there any option to convert my image into RGB using imageio or is there any other inbuilt function that can do it?</p>\\n'}], 'has_more': False, 'quota_max': 300, 'quota_remaining': 294}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/16 [00:00<?, ?it/s]\n",
      "  6%|▋         | 1/16 [00:01<00:16,  1.10s/it]\n",
      " 12%|█▎        | 2/16 [00:02<00:14,  1.07s/it]\n",
      " 19%|█▉        | 3/16 [00:03<00:13,  1.07s/it]\n",
      " 25%|██▌       | 4/16 [00:04<00:12,  1.07s/it]\n",
      " 31%|███▏      | 5/16 [00:05<00:11,  1.07s/it]\n",
      " 38%|███▊      | 6/16 [00:06<00:10,  1.07s/it]\n",
      " 44%|████▍     | 7/16 [00:07<00:09,  1.08s/it]\n",
      " 50%|█████     | 8/16 [00:08<00:08,  1.08s/it]\n",
      "  8%|▊         | 5/60 [00:16<02:59,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [{'is_answered': True, 'view_count': 1859, 'answer_count': 1, 'score': 5, 'last_activity_date': 1480813111, 'creation_date': 1447604789, 'question_id': 33721988, 'link': 'https://stackoverflow.com/questions/33721988/scipy-ndimage-zoom-result-depends-on-image-size', 'title': 'scipy.ndimage.zoom result depends on image size', 'body': '<p>I noticed that the result of scipy.ndimage.zoom depends on the size of the original image. In the following code sample a checkerboard image is generated and then zoomed with ndimage.zoom. If one checkerboard tile is just 2x2 pixels, the zoom factor seems to be too large and the resulting image gets cropped. In contrast, if the tile has dimensions 10x10, the result looks good.</p>\\n\\n<pre><code>from __future__ import division\\n\\nimport numpy as np\\nfrom scipy import ndimage, misc\\nimport wx\\n\\ny,x = 2,2   # change tile size here\\nimgdata = np.zeros((y,x),dtype=\\'uint8\\')\\nimgdata[y/2:,x/2:] = 255\\nimgdata[:y/2,:x/2] = 255\\nimgdata = np.tile(imgdata,(4,4))\\nimgdata = np.array((imgdata,imgdata,imgdata))\\nd,y,x = imgdata.shape\\n\\nzoom = 200.0/y\\n\\nw, h = int(x*zoom), int(y*zoom)\\n\\napp = wx.App(None)\\n\\nzoomed = np.ascontiguousarray(ndimage.interpolation.zoom(imgdata,[1,zoom, zoom],order=0).transpose((1,2,0)), dtype=\\'uint8\\')\\nimage = wx.ImageFromBuffer(w, h, zoomed)\\nimage.SaveFile(\\'zoomed.png\\',wx.BITMAP_TYPE_PNG)\\n</code></pre>\\n\\n<p>02x02 tile: <a href=\"https://i.stack.imgur.com/fp8F3.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/fp8F3.png\" alt=\"2x2 tile\"></a></p>\\n\\n<p>10x10 tile: <a href=\"https://i.stack.imgur.com/sTQqq.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/sTQqq.png\" alt=\"10x10 tile\"></a></p>\\n\\n<p>Up to know I have been using scipy.misc.imresize which does not show this behaviour but I want to avoid the additional dependency on PIL.</p>\\n\\n<p>Am I doing something wrong or is this a bug in zoom?</p>\\n'}, {'is_answered': True, 'view_count': 358, 'closed_date': 1352426577, 'accepted_answer_id': 13080349, 'answer_count': 1, 'score': 2, 'last_activity_date': 1351222241, 'creation_date': 1351222047, 'question_id': 13080327, 'link': 'https://stackoverflow.com/questions/13080327/resizing-small-images-in-python-numpy-to-a-multiple-of-the-original-size-accu', 'closed_reason': 'exact duplicate', 'title': 'Resizing small images in Python (numpy) to a multiple of the original size, accurately', 'body': '<blockquote>\\n  <p><strong>Possible Duplicate:</strong><br>\\n  <a href=\"https://stackoverflow.com/questions/7656665/how-to-repeat-along-two-axis\">how to repeat along two axis</a>  </p>\\n</blockquote>\\n\\n\\n\\n<p>Let\\'s suppose we have the following matrix/image:</p>\\n\\n<pre><code>x = array([[1, 0, 1],\\n           [0, 1, 0],\\n           [1, 0, 1]])\\n</code></pre>\\n\\n<p>What I\\'d like to get is a 9x9 matrix that is a 3x magnified version of the above, having 3x3 ones in the top left corner, 3x3 0s in the middle top, etc.</p>\\n\\n<p>The things I\\'ve already tried are:</p>\\n\\n<p><strong>scipy.ndimage.interpolation.zoom</strong>(x, 3, order=(anything)), for example order=0 returns this:</p>\\n\\n<pre><code>array([[1, 1, 0, 0, 0, 0, 1, 1, 1],\\n       [1, 1, 0, 0, 0, 0, 1, 1, 1],\\n       [0, 0, 1, 1, 1, 1, 0, 0, 0],\\n       [0, 0, 1, 1, 1, 1, 0, 0, 0],\\n       [0, 0, 1, 1, 1, 1, 0, 0, 0],\\n       [0, 0, 1, 1, 1, 1, 0, 0, 0],\\n       [1, 1, 0, 0, 0, 0, 1, 1, 1],\\n       [1, 1, 0, 0, 0, 0, 1, 1, 1],\\n       [1, 1, 0, 0, 0, 0, 1, 1, 1]])\\n</code></pre>\\n\\n<p><strong>scipy.misc.imresize</strong>(x, (9,9), interp=\"nearest\") (effectively from PIL), that comes up with a different creative (but wrong) solution.</p>\\n\\n<p>Meanwhile, the MATLAB imresize solves the problem perfectly...</p>\\n\\n<p>Any ideas? (note: all of these solutions <em>should</em> work, so before submitting, try it out :))</p>\\n'}, {'is_answered': False, 'view_count': 257, 'answer_count': 1, 'score': 0, 'last_activity_date': 1539085352, 'creation_date': 1524563410, 'question_id': 49998519, 'link': 'https://stackoverflow.com/questions/49998519/which-imresize-is-fastest-in-python', 'title': 'which imresize is fastest in Python', 'body': '<p>Currently, we need a interpolation in a Image. I have used the scipy.misc.imresize. This function has two drawback:</p>\\n\\n<ol>\\n<li><p>it can only output interger matrix, but I need a float result. </p></li>\\n<li><p>the speed of scipy.misc.imresize is a little slow</p></li>\\n</ol>\\n'}, {'is_answered': True, 'view_count': 133, 'accepted_answer_id': 51596212, 'answer_count': 1, 'score': 0, 'last_activity_date': 1533111221, 'creation_date': 1532879496, 'question_id': 51582024, 'link': 'https://stackoverflow.com/questions/51582024/predicting-on-new-datas-with-tensorflow', 'title': 'Predicting on new datas with TensorFlow', 'body': '<p>I\\'m trying to use tensorflow thanks to a tutorial, but I\\'m really struggling with the way you have to use it.</p>\\n\\n<p>For the moment I trained a model with these functions :</p>\\n\\n<pre><code>def initialize_parameters(beta=0.05):\\n\\n    tf.set_random_seed(1)\\n\\n    #Regularization\\n    if beta!=0:\\n        regularizer = tf.contrib.layers.l2_regularizer(scale=beta)\\n    else: regularizer=None\\n\\n    W1 = tf.get_variable(\\'W1\\',[4,4,3,8],initializer=tf.contrib.layers.xavier_initializer(seed = 0), regularizer=regularizer)\\n    W2 = tf.get_variable(\\'W2\\',[2,2,8,16],initializer=tf.contrib.layers.xavier_initializer(seed = 0), regularizer=regularizer)\\n\\n    parameters = {\"W1\": W1,\\n                  \"W2\": W2}\\n\\n    return parameters, regularizer\\n\\ndef forward_propagation(X, parameters, regularizer=None):\\n\\n    # Retrieve the parameters from the dictionary \"parameters\" \\n    W1 = parameters[\\'W1\\']\\n    W2 = parameters[\\'W2\\']\\n\\n    # CONV2D: stride of 1, padding \\'SAME\\'\\n    Z1 = tf.nn.conv2d(X,W1,strides=[1,1,1,1],padding=\\'SAME\\')\\n    # RELU\\n    A1 = tf.nn.relu(Z1)\\n    # MAXPOOL: window 8x8, sride 8, padding \\'SAME\\'\\n    P1 = tf.nn.max_pool(A1, ksize=[1,8,8,1], strides=[1,8,8,1],padding=\\'SAME\\')\\n    # CONV2D: filters W2, stride 1, padding \\'SAME\\'\\n    Z2 = tf.nn.conv2d(P1,W2,strides=[1,1,1,1],padding=\\'SAME\\')\\n    # RELU\\n    A2 = tf.nn.relu(Z2)\\n    # MAXPOOL: window 4x4, stride 4, padding \\'SAME\\'\\n    P2 = tf.nn.max_pool(A2,ksize=[1,4,4,1],strides=[1,4,4,1],padding=\\'SAME\\')\\n    # FLATTEN\\n    P2 = tf.contrib.layers.flatten(P2)\\n    # FULLY-CONNECTED without non-linear activation function (not not call softmax).\\n    # 6 neurons in output layer. Hint: one of the arguments should be \"activation_fn=None\" \\n    Z3 = tf.contrib.layers.fully_connected(P2, num_outputs=6,activation_fn=None,weights_regularizer=regularizer)\\n    return Z3\\n\\ndef compute_cost(Z3, Y, regularizer=None):\\n\\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Z3, labels = Y))\\n\\n\\n    #Regularize\\n    if regularizer is not None:\\n        reg_variables = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\\n        reg_term = tf.contrib.layers.apply_regularization(regularizer, reg_variables)\\n    else:\\n        reg_term = 0\\n\\n    cost += reg_term\\n\\n    return cost\\n</code></pre>\\n\\n<p>After that, I call everything with my model function :</p>\\n\\n<pre><code>def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.009,\\n          num_epochs = 100, minibatch_size = 64, print_cost = True):\\n    \"\"\"\\n    Implements a three-layer ConvNet in Tensorflow:\\n    CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; CONV2D -&gt; RELU -&gt; MAXPOOL -&gt; FLATTEN -&gt; FULLYCONNECTED\\n\\n    Arguments:\\n    X_train -- training set, of shape (None, 64, 64, 3)\\n    Y_train -- test set, of shape (None, n_y = 6)\\n    X_test -- training set, of shape (None, 64, 64, 3)\\n    Y_test -- test set, of shape (None, n_y = 6)\\n    learning_rate -- learning rate of the optimization\\n    num_epochs -- number of epochs of the optimization loop\\n    minibatch_size -- size of a minibatch\\n    print_cost -- True to print the cost every 100 epochs\\n\\n    Returns:\\n    train_accuracy -- real number, accuracy on the train set (X_train)\\n    test_accuracy -- real number, testing accuracy on the test set (X_test)\\n    parameters -- parameters learnt by the model. They can then be used to predict.\\n    \"\"\"\\n\\n    ops.reset_default_graph()                         # to be able to rerun the model without overwriting tf variables\\n    tf.set_random_seed(1)                             # to keep results consistent (tensorflow seed)\\n    seed = 3                                          # to keep results consistent (numpy seed)\\n    (m, n_H0, n_W0, n_C0) = X_train.shape             \\n    n_y = Y_train.shape[1]                            \\n    costs = []                                        # To keep track of the cost\\n\\n    # Create Placeholders of the correct shape\\n    X, Y = tf.placeholder(dtype=tf.float32,shape=(None, n_H0, n_W0, n_C0),name=\"X\"),tf.placeholder(dtype=tf.float32,shape=(None,6),name=\"Y\")\\n\\n    # Initialize parameters\\n    parameters = initialize_parameters()\\n\\n    # Forward propagation: Build the forward propagation in the tensorflow graph\\n    Z3 = forward_propagation(X, parameters)\\n\\n    # Cost function: Add cost function to tensorflow graph\\n    cost = compute_cost(Z3, Y)\\n\\n    # Backpropagation: Define the tensorflow optimizer. Use an AdamOptimizer that minimizes the cost.\\n    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\\n\\n    # Initialize all the variables globally\\n    init = tf.global_variables_initializer()\\n\\n    # Start the session to compute the tensorflow graph\\n    with tf.Session() as sess:\\n\\n        # Run the initialization\\n        sess.run(init)\\n\\n        # Do the training loop\\n        for epoch in range(num_epochs):\\n\\n            minibatch_cost = 0.\\n            num_minibatches = int(m / minibatch_size) # number of minibatches of size minibatch_size in the train set\\n            seed = seed + 1\\n            minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\\n\\n            for minibatch in minibatches:\\n\\n                # Select a minibatch\\n                (minibatch_X, minibatch_Y) = minibatch\\n\\n                # Run the session to execute the optimizer and the cost, the feedict should contain a minibatch for (X,Y).\\n                _ , temp_cost = sess.run([optimizer, cost],feed_dict={X:minibatch_X,Y:minibatch_Y})\\n\\n                minibatch_cost += temp_cost / num_minibatches\\n\\n\\n            # Print the cost every epoch\\n            if print_cost == True and epoch % 5 == 0:\\n                print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\\n            if print_cost == True and epoch % 1 == 0:\\n                costs.append(minibatch_cost)\\n\\n\\n        # plot the cost\\n        plt.plot(np.squeeze(costs))\\n        plt.ylabel(\\'cost\\')\\n        plt.xlabel(\\'iterations (per tens)\\')\\n        plt.title(\"Learning rate =\" + str(learning_rate))\\n        plt.show()\\n\\n        # Calculate the correct predictions\\n        predict_op = tf.argmax(Z3, 1)\\n        correct_prediction = tf.equal(predict_op, tf.argmax(Y, 1))\\n\\n        # Calculate accuracy on the test set\\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\\n        print(accuracy)\\n        train_accuracy = accuracy.eval({X: X_train, Y: Y_train})\\n        test_accuracy = accuracy.eval({X: X_test, Y: Y_test})\\n        print(\"Train Accuracy:\", train_accuracy)\\n        print(\"Test Accuracy:\", test_accuracy)\\n\\n        return train_accuracy, test_accuracy, parameters\\n</code></pre>\\n\\n<p>So far so good, but I can\\'t predict my model on new datas.\\nFor the moment I\\'ve tried things like this :</p>\\n\\n<p>X = scipy.misc.imresize(my_image, size=(64,64)).reshape((1,64,64,3))/255.</p>\\n\\n<pre><code>with tf.Session() as sess:\\n    x = tf.placeholder(tf.float32, shape=(None,64, 64, 3))\\n    z3 = forward_propagation(x, parameters)\\n    #soft = tf.nn.softmax(z3)\\n    p = tf.argmax(z3,axis=1)\\n\\n    #init = tf.global_variables_initializer()\\n    #sess.run(init)\\n\\n    prediction = sess.run(p, feed_dict = {x: X})\\n    print(sess.run(z3, feed_dict = {x: X}))\\n    print(prediction)\\n</code></pre>\\n\\n<p>Returning with the error : <strong>Attempting to use uninitialized value fully_connected_1/biases\\n     [[Node: fully_connected_1/biases/read = IdentityT=DT_FLOAT, _class=[\"loc:@fully_connected_1/biases\"], _device=\"/job:localhost/replica:0/task:0/cpu:0\"]]</strong></p>\\n\\n<p>Or even :</p>\\n\\n<pre><code>def predict(X, parameters):\\n    tf.reset_default_graph()\\n\\n    W1 = parameters[\"W1\"]\\n    W2 = parameters[\"W2\"]\\n\\n    params = {\"W1\": W1,\\n              \"W2\": W2}\\n\\n    x = tf.placeholder(tf.float32, shape=(None,64, 64, 3))\\n\\n    z3 = forward_propagation(x, params)\\n    p = tf.argmax(z3)\\n\\n    sess = tf.Session()\\n    prediction = sess.run(p, feed_dict = {x: X})\\n\\n    return prediction\\n</code></pre>\\n\\n<p>But when I run the function with my image, I have the error <strong>Tensor(\"W1:0\", shape=(4, 4, 3, 8), dtype=float32_ref) must be from the same graph as Tensor(\"Placeholder:0\", shape=(?, 64, 64, 3), dtype=float32)</strong></p>\\n\\n<p>How should I do to use my trained models (I have the parameters stored in a variable parameters, I guess ?)Thanks</p>\\n'}, {'is_answered': False, 'view_count': 688, 'answer_count': 1, 'score': 0, 'last_activity_date': 1573839704, 'creation_date': 1573755308, 'question_id': 58863374, 'link': 'https://stackoverflow.com/questions/58863374/scipy-misc-imresize-vs-cv2-resize', 'title': 'Scipy.misc.imresize() vs cv2.resize()', 'body': '<p>I\\'ve been following many tutorials on Neural Networks and Image Classification, many of them use those 2 commands to resize the input image in order to match it with the input size of the Neural Network. </p>\\n\\n<p>What is the difference between the two? Are they equivalent? \\nHow do i decide which one is better? </p>\\n\\n<p>Looking at the documentation of each command, they seem pretty much the same.</p>\\n\\n<p><strong>Documentation</strong>: <a href=\"https://www.tutorialkart.com/opencv/python/opencv-python-resize-image/\" rel=\"nofollow noreferrer\">cv2.resize()</a> , <a href=\"https://docs.scipy.org/doc/scipy-0.19.1/reference/generated/scipy.misc.imresize.html\" rel=\"nofollow noreferrer\">scipy.misc.imresize()</a></p>\\n'}, {'is_answered': True, 'view_count': 604, 'accepted_answer_id': 63517199, 'answer_count': 1, 'score': -1, 'last_activity_date': 1597989074, 'creation_date': 1597920768, 'question_id': 63503521, 'link': 'https://stackoverflow.com/questions/63503521/skimage-transform-resize-is-slower-than-scipy-misc-imresize', 'title': 'skimage.transform.resize is slower than scipy.misc.imresize', 'body': \"<p>I am training a neural network, while training I came across errors in a data preparation section. where this line:</p>\\n<pre><code>img = np.double(scipy.misc.imresize(img, [height, width, channels], interp='bilinear', mode = 'RGB'))\\n</code></pre>\\n<p>and this line:</p>\\n<pre><code>img2 = np.double(scipy.misc.imresize(img2, [height, width], interp='bilinear'))\\n</code></pre>\\n<p>showed errors, as <strong>scipy.misc.imresize</strong> is deprecated in newer versions of scipy. What I did is I used <strong>skimage.transform.resize</strong> instead of <strong>scipy.misc.imresize</strong>, that's what is recommended, as:</p>\\n<pre><code>img = np.double(resize(img, (height, width, channels)))\\n</code></pre>\\n<p>and</p>\\n<pre><code>img2 = np.double(resize(img2, (height, width)))\\n</code></pre>\\n<p>It worked.\\nBut the problem is the data reading process has become very slow as compared to the <strong>scipy.misc.imresize</strong>. I had tested it before on old version of <strong>scipy.misc.imresize</strong>.\\nAny help in this regard would be very much appreciated. Thanks!</p>\\n<p>P.S. I am training the model on Google Colab.</p>\\n\"}], 'has_more': False, 'quota_max': 300, 'quota_remaining': 284}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/6 [00:00<?, ?it/s]\n",
      "\n",
      " 17%|█▋        | 1/6 [00:01<00:05,  1.09s/it]\n",
      "\n",
      " 33%|███▎      | 2/6 [00:02<00:04,  1.07s/it]\n",
      "\n",
      " 10%|█         | 6/60 [00:19<02:56,  3.26s/it]\n",
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\User\\Anaconda3\\envs\\mlenv\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\User\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\tqdm\\_monitor.py\", line 62, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"C:\\Users\\User\\Anaconda3\\envs\\mlenv\\lib\\_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [{'is_answered': False, 'view_count': 524, 'answer_count': 1, 'score': 0, 'last_activity_date': 1580630006, 'creation_date': 1519980677, 'question_id': 49065617, 'link': 'https://stackoverflow.com/questions/49065617/beginner-cannot-access-pythons-imrotate', 'title': 'Beginner cannot access python&#39;s imrotate', 'body': '<p>I Googled \"python rotate image\" and it sent me to scipy.misc.imrotate. I then imported scipy and scipy.misc</p>\\n\\n<pre><code>import scipy.misc as misc\\n</code></pre>\\n\\n<p>failed. I don\\'t know why, but it worked when I didn\\'t try renaming.\\nhelp(\\'scipy.misc\\') lists imrotate as an available function, but said that \"pillow\" is needed. So I went back into the Macos Unix shell and typed </p>\\n\\n<pre><code>pip3 install pillow\\n</code></pre>\\n\\n<p>which worked. but I still get an error from python3</p>\\n\\n<pre><code>scipy.misc.imrotate\\n&gt;&gt;&gt; scipy.misc.imrotate\\nTraceback (most recent call last):\\n  File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;\\nAttributeError: module \\'scipy.misc\\' has no attribute \\'imrotate\\'\\n</code></pre>\\n'}], 'has_more': False, 'quota_max': 300, 'quota_remaining': 281}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      " 12%|█▏        | 7/60 [00:20<02:36,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [{'is_answered': True, 'view_count': 19743, 'accepted_answer_id': 25814423, 'answer_count': 5, 'score': 13, 'last_activity_date': 1633462510, 'creation_date': 1409975856, 'question_id': 25696615, 'link': 'https://stackoverflow.com/questions/25696615/can-i-save-a-numpy-array-as-a-16-bit-image-using-normal-enthought-python', 'title': 'Can I save a numpy array as a 16-bit image using &quot;normal&quot; (Enthought) python?', 'body': '<p>Is there any way to save a numpy array as a 16 bit image (tif, png) using any of the commonly available python packages? <a href=\"https://stackoverflow.com/questions/24240039/save-numpy-array-as-image-with-high-precision-16-bits-with-scikit-image\">This</a> is the only way that I could get to work in the past, but I needed to install the FreeImage package, which is a little annoying.</p>\\n\\n<p>This seems like a pretty basic task, so I would expect that it should be covered by scipy, but scipy.misc.imsave only does 8-bits.</p>\\n\\n<p>Any ideas?</p>\\n'}, {'is_answered': True, 'view_count': 17514, 'accepted_answer_id': 56488269, 'answer_count': 3, 'score': 7, 'last_activity_date': 1591237213, 'creation_date': 1559856389, 'question_id': 56485301, 'link': 'https://stackoverflow.com/questions/56485301/import-error-scipy-misc-import-imsave-on-google-colaboratory', 'title': 'Import Error: &#39;scipy.misc import imsave&#39; on Google Colaboratory', 'body': \"<p><strong>Not able to use scipy.misc.imsave on Colaboratory</strong></p>\\n\\n<pre><code>from scipy.misc import imsave\\n</code></pre>\\n\\n<blockquote>\\n  <p>ImportError: cannot import name 'imsave'</p>\\n</blockquote>\\n\\n<p>Tried to install <strong>Pillow</strong> and <strong>Scipy</strong> again on <strong>Colab</strong>, but the requirements are already satisfied, so <strong>Colab</strong> does not install these packages</p>\\n\"}, {'is_answered': True, 'view_count': 7687, 'accepted_answer_id': 50519943, 'answer_count': 3, 'score': 5, 'last_activity_date': 1578914931, 'creation_date': 1527157130, 'question_id': 50506782, 'link': 'https://stackoverflow.com/questions/50506782/imgaug-load-and-save-images', 'title': 'imgaug: load and save images', 'body': '<p>I am using Python+Tensorflow for CNN training on a high-performance computing cluster. I am training a convolutional neural network, but have a relatively small dataset. So I am implementing techniques to augment it. Now this is the first time i am working on a core computer vision problem so am relatively new to it.</p>\\n\\n<p>As raising the number of images for training, the imgaug package (<a href=\"https://github.com/aleju/imgaug\" rel=\"noreferrer\">https://github.com/aleju/imgaug</a>) which requires cv2 seems to be perfect and a simple solution to multiply the number of images. I installed opencv2 using python 2.7, all required python packages are running locally (no env/Anaconda/Docker are used here).</p>\\n\\n<p>Practically, I’ am trying to run this code from <a href=\"http://imgaug.readthedocs.io/en/latest/source/examples_basics.html\" rel=\"noreferrer\">here</a>:</p>\\n\\n<pre><code>import os\\nimport imgaug as ia\\nfrom imgaug import augmenters as iaa\\nimport scipy\\nimport cv2\\nimport numpy as np\\n# optional check my hint import scipy.misc import imwrite\\n# optional check my hint import scipy.misc import imsave\\n\\nia.seed(1)\\n\\n# Example batch of images.\\n# The array has shape (32, 64, 64, 3) and dtype uint8.\\nimages = np.array(\\n    [ia.quokka(size=(64, 64)) for _ in range(32)],\\n    dtype=np.uint8\\n)\\n\\nseq = iaa.Sequential([\\n    iaa.Fliplr(0.5), # horizontal flips\\n    iaa.Crop(percent=(0, 0.1)), # random crops\\n    # Small gaussian blur with random sigma between 0 and 0.5.\\n    # But we only blur about 50% of all images.\\n    iaa.Sometimes(0.5,\\n        iaa.GaussianBlur(sigma=(0, 0.5))\\n    ),\\n    # Strengthen or weaken the contrast in each image.\\n    iaa.ContrastNormalization((0.75, 1.5)),\\n    # Add gaussian noise.\\n    # For 50% of all images, we sample the noise once per pixel.\\n    # For the other 50% of all images, we sample the noise per pixel AND\\n    # channel. This can change the color (not only brightness) of the\\n    # pixels.\\n    iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\\n    # Make some images brighter and some darker.\\n    # In 20% of all cases, we sample the multiplier once per channel,\\n    # which can end up changing the color of the images.\\n    iaa.Multiply((0.8, 1.2), per_channel=0.2),\\n    # Apply affine transformations to each image.\\n    # Scale/zoom them, translate/move them, rotate them and shear them.\\n    iaa.Affine(\\n        scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\\n        translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\\n        rotate=(-25, 25),\\n        shear=(-8, 8)\\n    )\\n], random_order=True) # apply augmenters in random order\\n\\nimages_aug = seq.augment_images(images)\\n</code></pre>\\n\\n<p>After creating a .py file with this code, I have copied this file into a folder of 50 images (.jpg).\\nThe code itself runs perfectly (no error encounter), but it seems like that the .jpg are not loaded into the.py file, even if I set the folder path, it will not load the images. Also, no new files were written.</p>\\n\\n<p>Question 1: I guess I must load the images as an array into the .py, or into the cv2 but I can’t find any example how to load and write them, as I have never done something like this. Any help? </p>\\n\\n<p>Question 2: What is the best interface to use in this context? All the images are stored in folders or .xlsx files.  </p>\\n\\n<p>(Hint: May be this is an option unfortunately its based on Keras (<a href=\"https://stackoverflow.com/questions/43209530/image-data-agumentation-tequniques-using-keras-preprocessing-image-imagedatagene/43555465#43555465\">Link</a>)? Or these 2 option I’ve found, but I am not able to use them <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.imread.html\" rel=\"noreferrer\">scipy.ndimage.imread</a> and  <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.imsave.html\" rel=\"noreferrer\">scipy.misc.imsave</a>)</p>\\n'}, {'is_answered': True, 'view_count': 5876, 'accepted_answer_id': 21384352, 'answer_count': 2, 'score': 3, 'last_activity_date': 1413361507, 'creation_date': 1390598727, 'question_id': 21342758, 'link': 'https://stackoverflow.com/questions/21342758/converting-files-to-digital-image-failing-with-tile-cannot-extend-outside-image', 'title': 'Converting files to digital image failing with &quot;tile cannot extend outside image&quot;', 'body': '<p>I am trying to recreate some of the work from the blog posting <a href=\"http://sarvamblog.blogspot.com/2013/04/clustering-malware-corpus.html\" rel=\"nofollow\">http://sarvamblog.blogspot.com/2013/04/clustering-malware-corpus.html</a> </p>\\n\\n<pre><code>import itertools\\nimport glob\\nimport numpy,scipy, os, array\\nfrom scipy.misc import imsave\\n\\nfor filename in list(glob.glob(\\'file/*.file\\')):\\n    f = open(filename,\\'rb\\');\\n        #just want to make sure I get the right file\\'\\n    print filename\\n    ln = os.path.getsize(filename); # length of file in bytes\\n    width = 256;\\n    rem = ln%width; \\n    a = array.array(\"B\"); # uint8 array\\n    a.fromfile(f,ln-rem);\\n    f.close(); \\n    g = numpy.reshape(a,(len(a)/width,width));\\n    g = numpy.uint8(g);\\n    fpng = filename + \".png\"\\n        # make sure the png process and everything else is going\\'\\n    print fpng\\n    scipy.misc.imsave(fpng,g);`\\n</code></pre>\\n\\n<p>And although this runs great on 1 or 2 files, I run into problems on once I expand to dozens</p>\\n\\n<pre><code>Traceback (most recent call last):\\n  File \"&lt;stdin&gt;\", line 14, in &lt;module&gt;\\n  File \"/usr/lib/python2.7/dist-packages/scipy/misc/pilutil.py\", line 120, in imsave\\n   im = toimage(arr)\\n  File \"/usr/lib/python2.7/dist-packages/scipy/misc/pilutil.py\", line 183, in toimage\\n    image = Image.fromstring(\\'L\\',shape,bytedata.tostring())\\n  File \"/usr/lib/python2.7/dist-packages/PIL/Image.py\", line 1797, in fromstring\\n    im.fromstring(data, decoder_name, args)\\n  File \"/usr/lib/python2.7/dist-packages/PIL/Image.py\", line 590, in fromstring\\n    d.setimage(self.im)\\n  ValueError: tile cannot extend outside image\\n</code></pre>\\n\\n<p>I assume that my issue is with not either A: closing the scipy.misc.imsave or B: not resetting the arrarys. Any help would be greatly appreciated</p>\\n'}, {'is_answered': True, 'view_count': 1474, 'accepted_answer_id': 37484620, 'answer_count': 2, 'score': 3, 'last_activity_date': 1464362374, 'creation_date': 1464354138, 'question_id': 37484254, 'link': 'https://stackoverflow.com/questions/37484254/why-do-values-in-my-3d-numpy-array-change-when-i-write-it-to-file', 'title': 'Why do values in my 3D numpy array change when I write it to file?', 'body': \"<p>strange problem in that I have a 3D array full of labels (lets say from 1-36) called labelled_stack. These are just areas where the value in the array is equal to the label given. A quick 2D example with 5 labels would be something like:</p>\\n\\n<pre><code>labelled_stack = (0 0 0 0 0 0 0 0 0 0)\\n                 (0 1 1 0 0 0 2 2 2 0)\\n                 (0 1 0 0 3 0 0 2 2 0)\\n                 (0 0 0 3 3 0 0 0 0 0)\\n                 (0 4 0 0 3 0 0 0 5 0)\\n                 (0 4 0 0 0 0 5 5 5 0)\\n                 (0 0 0 0 0 0 0 0 0 0)\\n</code></pre>\\n\\n<p>But imagine its a numpy array...</p>\\n\\n<p>I have tried using cv2.imwrite and scipy.misc.imsave to save the stack, but when i do this and then open them, their values have changed so that the values = 5, now equal 255, and the values equal to 1, equal 51 etc. This is not acceptable. I need the values to stay as they are.. increasing values of integers in steps of 1. i know cv2 and scipy.misc both write as 8bit images but that shouldnt mean I get this error.</p>\\n\\n<p>I have even re-read the images back into python to check this is going on and it is.</p>\\n\\n<p>My labelled_stack is of type np.uint32</p>\\n\\nEdit to include save commands:\\n\\n<pre><code>for j in np.arange(0,l,1):\\n     misc.imsave(save_path+Folder+'/Labelled/slice_{:04d}of{:}.tif'.format(j+1,l),labelled_stack[:,:,j])\\n</code></pre>\\n\\n<p>or...</p>\\n\\n<pre><code>cv2.imwrite(save_path+Folder+'/Labelled/slice_{:04d}of{:}.tif'.format(j+1,l),labelled_stack[:,:,j])\\n</code></pre>\\n\"}, {'is_answered': True, 'view_count': 6053, 'accepted_answer_id': 41892565, 'answer_count': 1, 'score': 3, 'last_activity_date': 1609337196, 'creation_date': 1485474267, 'question_id': 41884996, 'link': 'https://stackoverflow.com/questions/41884996/cv2-imwrite-changes-color-when-saving-from-np-array-to-jpg', 'title': 'cv2.imwrite changes color when saving from np.array to JPG', 'body': '<p>I am currently working on an edge detection program for a class (concretely I am detecting street lines).</p>\\n<p>I read in an image and detect some edges (via hough). The detected edges are then laid onto the original image. In the process, image data is stored as numpy.ndarry.\\nAt the very end, i want to save the image to disc (as jpg).</p>\\n<p>If I write the images to disc via scipy.misc.imsave, everything works fine.\\nIf I do it via cv2.imwrite, the colors get distorted into a sort of thermal picture.</p>\\n<p>I have read several topics on different color channels and necessary scaling / conversion, but I cannot find a solution from numpy.ndarray to .jpg.\\nI know that .jpg may only have 8bit or 16bit color channels, but don\\'t know where to go from there. In any case i have blindly tried to divide by 255 (--&gt; image almost black) and multiply by 255 (--&gt; whitish thermal picture :-) ).</p>\\n<p>Any ideas? These are the relevant sections of code:</p>\\n<p>Reading the image (function is later called in a loop):</p>\\n<pre><code>def read(img):\\n    image = mpimg.imread(img)\\n    return image\\n</code></pre>\\n<p>Draw function to be called in a loop to draw all lines defined in an array:</p>\\n<pre><code>def draw_lines(img, line_coordinates, color=[255, 0, 0], thickness=4):\\n    for line in line_coordinates:\\n        for x1, y1, x2, y2 in line:\\n            cv2.line(img, (x1, y1), (x2, y2), color, thickness)\\n</code></pre>\\n<p>This function calls the draw function and returns an image with all lines drawn</p>\\n<pre class=\"lang-py prettyprint-override\"><code>def depict_lines(img, array_of_coordinates): \\n    line_img = np.zeros((img.shape[0], img.shape[1], 3), dtype=np.uint8)\\n    draw_lines(line_img, array_of_coordinates)\\n    return line_img\\n</code></pre>\\n<p>Next, I combine the original image and the one containing the edges:</p>\\n<pre class=\"lang-py prettyprint-override\"><code>def weighted_img(line_img, initial_img, α=0.95, β=1., λ=0.):\\n    return cv2.addWeighted(initial_img, α, line_img, β, λ)\\n\\ncombined_images = weighted_img(depicted_lines, original_image)\\n</code></pre>\\n<p>Finally, I save the image to disc. This call produces weird colors:</p>\\n<pre class=\"lang-py prettyprint-override\"><code>cv2.imwrite(new_file_name, combined_images)\\n</code></pre>\\n<p>whereas this one works fine:</p>\\n<pre class=\"lang-py prettyprint-override\"><code>scipy.misc.imsave(\\'outfile.jpg\\', combined_images)\\n</code></pre>\\n<p>This is an unprocessed image:\\n<a href=\"https://i.stack.imgur.com/vWpwD.jpg\" rel=\"nofollow noreferrer\">natural colors</a></p>\\n<p>And a processed one:\\n<a href=\"https://i.stack.imgur.com/781dF.jpg\" rel=\"nofollow noreferrer\">distorted colors</a></p>\\n<p>Your help would be appreciated!</p>\\n'}, {'is_answered': True, 'view_count': 9611, 'accepted_answer_id': 57253092, 'answer_count': 1, 'score': 2, 'last_activity_date': 1564400379, 'creation_date': 1564400193, 'question_id': 57253048, 'link': 'https://stackoverflow.com/questions/57253048/scipy-misc-has-no-attribute-imsave', 'title': '&#39;scipy.misc&#39; has no attribute &#39;imsave&#39;', 'body': \"<p>When i try to save an image using Scipy.misc.imsave('path',image). </p>\\n\\n<p>I got this Error : module 'scipy.misc' has no attribute.</p>\\n\\n<p>I am working on Ubuntu 17.10 and i have both pillow and scipy installed.\\nCan you help me please.</p>\\n\"}, {'is_answered': True, 'view_count': 951, 'accepted_answer_id': 28225707, 'answer_count': 1, 'score': 1, 'last_activity_date': 1422569892, 'creation_date': 1422569422, 'question_id': 28225600, 'link': 'https://stackoverflow.com/questions/28225600/skimage-io-imsave-destroys-grayscale-image', 'title': 'skimage.io.imsave &quot;destroys&quot; grayscale image?', 'body': \"<p>I have an array of graysale image read in from a color one. If I use matplotlib to imshow the grayscale image, it looks just fine. But when I io.imsave it, it's ruined (by an outrageous amount of noise). However, if I numpy.around it first before io.imsave-ing, then it's significantly better, but black and white are still all swapped (dark regions appear white, and bright regions appear dark)</p>\\n\\n<p>scipy.misc.imsave, on the other hand, works perfectly.</p>\\n\\n<p>Thank you.</p>\\n\"}, {'is_answered': True, 'view_count': 1469, 'accepted_answer_id': 49221585, 'answer_count': 2, 'score': 1, 'last_activity_date': 1556590175, 'creation_date': 1520606644, 'question_id': 49196195, 'link': 'https://stackoverflow.com/questions/49196195/how-to-remove-normalization-from-scipy-imsave-function', 'title': 'How to remove normalization from scipy imsave function', 'body': '<p>I have a simple task to accomplish, but the current save functions I am finding are not helping me at all. What I have to do is simply convert a gray-scale image to another one which has intensities belonging to a smaller interval (specifically between 120 and 180). </p>\\n\\n<p>I implemented the conversion (like a changing different temperature scales), but when I save the image, scipy.misc.imsave normalizes it. The conversion is correct because I created histograms to display the intensities before the saving, and they are all lying between the specified range. </p>\\n\\n<p>I\\'ve tried other tools like: </p>\\n\\n<pre><code>imageio.imwrite(path, img)\\nnumpy.save(path, img)\\nscipy.misc.toimage(img, cmin=120, cmax=180, mode=\\'L\\').save(path)\\n</code></pre>\\n\\n<p>I confess this last one I don\\'t understand the parameters quite well (I have a guess), and the <a href=\"https://docs.scipy.org/doc/scipy-0.18.1/reference/generated/scipy.misc.toimage.html\" rel=\"nofollow noreferrer\">documentation</a> didn\\'t help. Could anybody help me with this issue? </p>\\n\\n<p>EDIT: I am posting the code</p>\\n\\n<pre><code>def ex3():\\n    I = misc.imread(imgname)\\n    N =  numpy.multiply(I , float(12.0/51.0))\\n    N = numpy.add(N, 120)\\n    NEG = I\\n    NEG = numpy.add(NEG, -255)\\n    NEG = numpy.absolute(NEG)\\n    misc.imsave(path, N)\\n    misc.imsave(os.getcwd()+\"/a0/results/\"+file.replace(path, NEG)\\n</code></pre>\\n\\n<p>The image was perfectly rendered when I switched to OpenCV. But I would like to stick with Scipy if possible.</p>\\n'}, {'is_answered': True, 'view_count': 574, 'accepted_answer_id': 44737074, 'answer_count': 1, 'score': 0, 'last_activity_date': 1517963027, 'creation_date': 1498309952, 'question_id': 44736769, 'link': 'https://stackoverflow.com/questions/44736769/jpg-image-size-reduced-on-imsave', 'title': 'JPG image size reduced on imsave', 'body': '<p>I am building up a library called <a href=\"http://hips.readthedocs.io/\" rel=\"nofollow noreferrer\">hips</a> where one module is involved with fetching tile images and storing them on disk. The problem here is that I fetch a tile from a <a href=\"http://alasky.unistra.fr/2MASS/H/Norder6/Dir30000/Npix30889.jpg\" rel=\"nofollow noreferrer\">remote URL</a> and save it using <a href=\"https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.misc.imsave.html\" rel=\"nofollow noreferrer\">scipy.misc.imsave</a> function in a temporary directory. The saved file size is <code>41.0 kB</code>, however, if I save the file manually from the <a href=\"http://alasky.unistra.fr/2MASS/H/Norder6/Dir30000/Npix30889.jpg\" rel=\"nofollow noreferrer\">remote URL</a>, its size is <code>119.7 kB</code>.</p>\\n\\n<p>I have copied the failed test case below:</p>\\n\\n<pre><code>def test_fetch_read_write_jpg(self, tmpdir):\\n    meta = HipsTileMeta( ... )\\n    url = \\'http://alasky.unistra.fr/2MASS/H/Norder6/Dir30000/Npix30889.jpg\\'\\n    tile = HipsTile.fetch(meta, url)\\n\\n    filename = str(tmpdir / \\'Npix30889.jpg\\')\\n    tile.write(filename)\\n    tile2 = HipsTile.read(meta, filename=filename)\\n    print(tile.data.shape)\\n    print(tile2.data.shape)\\n\\n    assert tile == tile2 \\n</code></pre>\\n\\n<p>Here is the failed assertion:</p>\\n\\n<blockquote>\\n  <p>----------------------------------Captured stdout call-------------------------------------- </p>\\n  \\n  <p>(512, 512, 3)</p>\\n  \\n  <p>(512, 512, 3)</p>\\n  \\n  <p>False</p>\\n</blockquote>\\n\\n<p>The code involved with tile storing is shown below:</p>\\n\\n<pre><code>from scipy.misc import imsave\\ndef write(self, filename: str = None) -&gt; None:\\n    path = Path(filename) if filename else self.meta.full_path\\n    imsave(str(path), self.data)\\n</code></pre>\\n\\n<p>I also tried saving the file using <a href=\"http://pillow.readthedocs.io/en/3.4.x/reference/Image.html\" rel=\"nofollow noreferrer\">PIL.Image</a> library, using this code:</p>\\n\\n<pre><code>from PIL import Image\\nimage = Image.fromarray(self.data)\\nimage.save(str(path))\\n</code></pre>\\n\\n<p>But, it produces the same results. I tried printing out the tile data at index <code>[0][0]</code> which came to be <code>[10, 10, 10]</code> for both cases. Also, I displayed the image using <code>matplotlib</code>, and the results were identical. But, I can\\'t figure out the reason for the reduction in size / quality.</p>\\n'}, {'is_answered': True, 'view_count': 947, 'accepted_answer_id': 48758712, 'answer_count': 1, 'score': 0, 'last_activity_date': 1518490316, 'creation_date': 1518479555, 'question_id': 48757456, 'link': 'https://stackoverflow.com/questions/48757456/cant-import-a-package-in-spyder-while-can-import-in-python-terminal', 'title': 'Can&#39;t import a package in Spyder while can import in python terminal', 'body': \"<p>When I use the Python terminal, there is no import problems at all. But when I try to import, for example, keras or try to use scipy.misc.imsave() Spyder says <code>ModuleNotFound</code> and <code>AttributeError: module 'scipy.misc' has no attribute 'imsave'</code> respectively. Below is the sys.path of python terminal and Spyder.I would appreciate any fix for this problem. </p>\\n\\n<p>Python 3.6.3 terminal</p>\\n\\n<pre><code>    ['', \\n'C:\\\\\\\\Users\\\\\\\\kaany\\\\\\\\AppData\\\\\\\\Local\\\\\\\\Programs\\\\\\\\Python\\\\\\\\Python36\\\\\\\\python36.zip',\\n    'C:\\\\\\\\Users\\\\\\\\kaany\\\\\\\\AppData\\\\\\\\Local\\\\\\\\Programs\\\\\\\\Python\\\\\\\\Python36\\\\\\\\DLLs',\\n    'C:\\\\\\\\Users\\\\\\\\kaany\\\\\\\\AppData\\\\\\\\Local\\\\\\\\Programs\\\\\\\\Python\\\\\\\\Python36\\\\\\\\lib',\\n    'C:\\\\\\\\Users\\\\\\\\kaany\\\\\\\\AppData\\\\\\\\Local\\\\\\\\Programs\\\\\\\\Python\\\\\\\\Python36', \\n    'C:\\\\\\\\Users\\\\\\\\kaany\\\\\\\\AppData\\\\\\\\Local\\\\\\\\Programs\\\\\\\\Python\\\\\\\\Python36\\\\\\\\lib\\\\\\\\site-packages', \\n    'C:\\\\\\\\Users\\\\\\\\kaany\\\\\\\\AppData\\\\\\\\Local\\\\\\\\Programs\\\\\\\\Python\\\\\\\\Python36\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\mudicom-0.1.2-py3.6.egg']\\n</code></pre>\\n\\n<p>Spyder</p>\\n\\n<pre><code>['',\\n 'C:\\\\\\\\Users\\\\\\\\kaany\\\\\\\\Anaconda3\\\\\\\\python36.zip',\\n 'C:\\\\\\\\Users\\\\\\\\kaany\\\\\\\\Anaconda3\\\\\\\\DLLs',\\n 'C:\\\\\\\\Users\\\\\\\\kaany\\\\\\\\Anaconda3\\\\\\\\lib',\\n 'C:\\\\\\\\Users\\\\\\\\kaany\\\\\\\\Anaconda3',\\n 'C:\\\\\\\\Users\\\\\\\\kaany\\\\\\\\Anaconda3\\\\\\\\lib\\\\\\\\site-packages',\\n 'C:\\\\\\\\Users\\\\\\\\kaany\\\\\\\\Anaconda3\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\Babel-2.5.0-py3.6.egg',\\n 'C:\\\\\\\\Users\\\\\\\\kaany\\\\\\\\Anaconda3\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\Mako-1.0.7-py3.6.egg',\\n 'C:\\\\\\\\Users\\\\\\\\kaany\\\\\\\\Anaconda3\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\win32',\\n 'C:\\\\\\\\Users\\\\\\\\kaany\\\\\\\\Anaconda3\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\win32\\\\\\\\lib',\\n 'C:\\\\\\\\Users\\\\\\\\kaany\\\\\\\\Anaconda3\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\Pythonwin',\\n 'C:\\\\\\\\Users\\\\\\\\kaany\\\\\\\\Anaconda3\\\\\\\\lib\\\\\\\\site-packages\\\\\\\\IPython\\\\\\\\extensions',\\n 'C:\\\\\\\\Users\\\\\\\\kaany\\\\\\\\.ipython']\\n</code></pre>\\n\"}, {'is_answered': True, 'view_count': 449, 'accepted_answer_id': 52287805, 'answer_count': 1, 'score': 0, 'last_activity_date': 1536728853, 'creation_date': 1536659680, 'question_id': 52273313, 'link': 'https://stackoverflow.com/questions/52273313/write-binary-numpy-array-of-zeros-and-ones-to-file-using-cv2-or-pillow', 'title': 'Write binary numpy array of zeros and ones to file using cv2 or Pillow', 'body': \"<p>Is it possible to write binary numpy array containing 0 and 1 to file using opencv (cv2) or Pillow? I was using scipy.misc.imsave and it worked well, but i read it's depreciated so i wanted to switch to other modules, but when trying to write such an array i see only black image. I need to have 0/1 values, and not 0/255 for further processing.</p>\\n\"}, {'is_answered': False, 'view_count': 223, 'answer_count': 0, 'score': 0, 'last_activity_date': 1568695278, 'creation_date': 1568639083, 'question_id': 57957501, 'link': 'https://stackoverflow.com/questions/57957501/why-does-image-size-increase-when-i-save-using-cv2-imwrite', 'title': 'Why Does Image Size Increase When I Save Using cv2.imwrite()', 'body': '<p>I did some processing on an input image and saved the final image using cv2.imwrite method, input image size in 1.2MB (3120 * 4160 pixels) and the saved image is of size 2.8MB (3111 * 4151), which is more than double the size of input image. </p>\\n\\n<p>After some research I\\'ve found that, opencv saves the image in 16bit rate, so I used scipy.misc.imsave for saving the image, this solves my problem, the resultant size decreased to 1.1MB but the colormap changes, original and processed images are not in the same color.</p>\\n\\n<p>Can anyone please shed some light on why this happens? why did the image size almost double incase of cv2 write method and why did i lose the color using scipy write method?</p>\\n\\n<pre><code>from matplotlib import pyplot\\nimport cv2\\nfrom scipy.misc import imsave\\n\\'\\'\\'\\nread the input image here (jpg format) and image segmentation code here\\n\\'\\'\\'\\npyplot.imsave(\\'py_result.jpg\\', final_img) #this one doubles the image size and also changes the color, why is there a size bump?   \\ncv2.imwrite(\\'cv2_result.jpg\\', final_img) #this one almost doubles the image size\\nimsave(\\'scipy_result.jpg\\', final_img) #this one doesnot bump the image size but changes the color in output image\\n</code></pre>\\n\\n<p>Here are the images for reference:</p>\\n\\n<p>This is the input image.</p>\\n\\n<p><img src=\"https://i.imgur.com/IoDbePT.jpg\" alt=\"inputimage\"></p>\\n\\n<p>The processed image saved using scipy.imsave() as you can see the blue text in the input image has changed to red.</p>\\n\\n<p><img src=\"https://i.imgur.com/WsIwSNj.jpg\" alt=\"scipy written image\"></p>\\n'}, {'is_answered': True, 'view_count': 423, 'answer_count': 1, 'score': 0, 'last_activity_date': 1569408313, 'creation_date': 1569407788, 'question_id': 58096474, 'link': 'https://stackoverflow.com/questions/58096474/reading-an-image-and-saving-it-increases-the-image-size', 'title': 'Reading An Image And Saving It, Increases the Image Size', 'body': '<p>I\\'ve loaded an image using CV2.imread, and saved it using cv2.imwrite() and scipy.misc.imsave(). In both these cases output image size is bumped. Why is this?</p>\\n\\n<p>Both input and output images are of file type .jpg</p>\\n\\n<pre><code>img = cv2.imread(img_src)\\nscipy.misc.imsave(img, \"scipy_original.jpg\")\\ncv2.imwrite(\"cv2_original.jpg\", img)\\n</code></pre>\\n\\n<p>Input file size is 309kb\\nOutput file size in cv2 is 690kb\\noutput file size in scipy is 399kb</p>\\n\\n<p>this is the image if you want reference: <a href=\"https://i.imgur.com/0J8ClQn.jpg\" rel=\"nofollow noreferrer\">https://i.imgur.com/0J8ClQn.jpg</a></p>\\n'}], 'has_more': False, 'quota_max': 300, 'quota_remaining': 280}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/14 [00:00<?, ?it/s]\n",
      "  7%|▋         | 1/14 [00:01<00:14,  1.09s/it]\n",
      " 14%|█▍        | 2/14 [00:02<00:13,  1.09s/it]\n",
      " 21%|██▏       | 3/14 [00:03<00:11,  1.08s/it]\n",
      " 29%|██▊       | 4/14 [00:04<00:12,  1.23s/it]\n",
      " 36%|███▌      | 5/14 [00:05<00:10,  1.20s/it]\n",
      " 43%|████▎     | 6/14 [00:07<00:09,  1.18s/it]\n",
      " 50%|█████     | 7/14 [00:08<00:08,  1.16s/it]\n",
      " 57%|█████▋    | 8/14 [00:09<00:06,  1.15s/it]\n",
      " 64%|██████▍   | 9/14 [00:10<00:05,  1.14s/it]\n",
      " 71%|███████▏  | 10/14 [00:11<00:04,  1.14s/it]\n",
      " 79%|███████▊  | 11/14 [00:12<00:03,  1.18s/it]\n",
      " 86%|████████▌ | 12/14 [00:14<00:02,  1.17s/it]\n",
      " 13%|█▎        | 8/60 [00:35<03:53,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [{'is_answered': True, 'view_count': 8196, 'answer_count': 3, 'score': 6, 'last_activity_date': 1522997121, 'creation_date': 1477355794, 'question_id': 40229630, 'link': 'https://stackoverflow.com/questions/40229630/scipy-misc-imshow-runtimeerrorcould-not-execute-image-view', 'title': 'scipy.misc.imshow RuntimeError(&#39;Could not execute image view&#39;)', 'body': '<p>I am testing <a href=\"https://docs.scipy.org/doc/scipy-0.15.1/reference/generated/scipy.misc.imshow.html\" rel=\"noreferrer\">scipy.misc.imshow</a> and I got <strong>RuntimeError: Could not execute image viewer</strong>.</p>\\n\\n<p>I am using Python3.4 and running it on CentOS 7.</p>\\n\\n<pre><code>import scipy.misc\\nimg = scipy.misc.imread(\\'Data/cat.jpg\\')\\nassert len(img.shape) == 3\\nimg_resized = scipy.misc.imresize(img, (224, 224))\\nimg_answer = (img_resized/255.0).astype(\\'float32\\')\\nscipy.misc.imshow(img_answer)\\n</code></pre>\\n\\n<p>And I got an error:</p>\\n\\n<pre><code>sh: see: command not found\\nTraceback (most recent call last):\\n  File \"/usr/local/pycharm/helpers/pydev/pydev_run_in_console.py\", line 71, in &lt;module&gt;\\n    globals = run_file(file, None, None)\\n  File \"/usr/local/pycharm/helpers/pydev/pydev_run_in_console.py\", line 31, in run_file\\n    pydev_imports.execfile(file, globals, locals)  # execute the script\\n  File \"/usr/local/pycharm/helpers/pydev/_pydev_imps/_pydev_execfile.py\", line 18, in execfile\\n    exec(compile(contents+\"\\\\n\", file, \\'exec\\'), glob, loc)\\n  File \"/root/PycharmProjects/myVQA/testgood.py\", line 6, in &lt;module&gt;\\n    scipy.misc.imshow(img_answer)\\n  File \"/usr/lib64/python3.4/site-packages/scipy/misc/pilutil.py\", line 442, in imshow\\n    raise RuntimeError(\\'Could not execute image viewer.\\')\\nRuntimeError: Could not execute image viewer.\\n</code></pre>\\n\\n<p>It says that the <code>see</code> command is not found. Where is the <code>see</code> command installed on CentOS7? How can I fix the problem?</p>\\n\\n<p>I tried to add <code>SCIPY_PIL_IMAGE_VIEWER=/bin/eog</code> to <code>/etc/profile</code>\\nbut it seems to be no help.</p>\\n'}, {'is_answered': True, 'view_count': 2128, 'answer_count': 2, 'score': 0, 'last_activity_date': 1522996952, 'creation_date': 1492522451, 'question_id': 43473742, 'link': 'https://stackoverflow.com/questions/43473742/scipy-imshow-runtimeerror-could-not-execute-image-viewer', 'title': 'scipy.imshow (RuntimeError: Could not execute image viewer)', 'body': '<p>I installed scipy with anaconda2. When I test scipy.imshow, I got <strong>RuntimeError: Could not execute image viewer</strong></p>\\n\\n<pre><code>import scipy.misc as mi\\nimg = mi.imread(\\'F:\\\\Jupyter\\\\opencv\\\\quokka.jpg\\')\\nmi.imshow(img)\\n</code></pre>\\n\\n<p>The error is:</p>\\n\\n<pre><code>Traceback (most recent call last):\\n\\nFile \"&lt;ipython-input-4-9b0818c43d12&gt;\", line 1, in &lt;module&gt;\\nrunfile(\\'F:/Jupyter/opencv/show.py\\', wdir=\\'F:/Jupyter/opencv\\')\\n\\nFile \"C:\\\\Users\\\\l\\\\Anaconda2\\\\lib\\\\site-packages\\\\spyder\\\\utils\\\\site\\\\sitecustomize.py\", line 866, in runfile\\nexecfile(filename, namespace)\\n\\nFile \"C:\\\\Users\\\\l\\\\Anaconda2\\\\lib\\\\site-packages\\\\spyder\\\\utils\\\\site\\\\sitecustomize.py\", line 87, in execfile\\nexec(compile(scripttext, filename, \\'exec\\'), glob, loc)\\n\\nFile \"F:/Jupyter/opencv/show.py\", line 10, in &lt;module&gt;\\nmi.imshow(img)\\n\\nFile \"C:\\\\Users\\\\l\\\\Anaconda2\\\\lib\\\\site-packages\\\\scipy\\\\misc\\\\pilutil.py\", line 443, in imshow\\nraise RuntimeError(\\'Could not execute image viewer.\\')\\n\\nRuntimeError: Could not execute image viewer.\\n</code></pre>\\n\\n<p><a href=\"https://stackoverflow.com/questions/40229630/scipy-misc-imshow-runtimeerrorcould-not-execute-image-view\">scipy.misc.imshow RuntimeError(&#39;Could not execute image view&#39;)</a> has the similar question. It suggested to add SCIPY_PIL_IMAGE_VIEWER. However, I don\\'t know how to do this in windows7. \\nCan anyone shed some light on this question?</p>\\n'}], 'has_more': False, 'quota_max': 300, 'quota_remaining': 267}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|█████     | 1/2 [00:01<00:01,  1.22s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.13s/it]\n",
      " 15%|█▌        | 9/60 [00:39<03:43,  4.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [{'is_answered': True, 'view_count': 2201, 'answer_count': 1, 'score': 1, 'last_activity_date': 1542379853, 'creation_date': 1542377012, 'question_id': 53339373, 'link': 'https://stackoverflow.com/questions/53339373/how-to-create-png-image-file-object-from-numpy-array-in-python-without-saving-to', 'title': 'How to create png image file-object from numpy array in python without saving to disk (for http request)', 'body': \"<p>I need to submit PNG-images to a blackbox server with a http request. I use python3 to generate images in a numpy 64x64x3 array. What I currently do is:</p>\\n\\n<ol>\\n<li>Generate image</li>\\n<li>Save image with scipy.misc.toimage to disk</li>\\n<li>Open saved image file from disk</li>\\n<li>Use requests module to send http requests with the image opened image file object in it</li>\\n</ol>\\n\\n<p>This works perfectly fine, but I want to get rid of step 2 and 3, so I do not need to save my object to disk first and then load it again. Instead I would like to convert my numpy array in a file-object that is compatible with the http server and send it directly. (Like one you get from open() )</p>\\n\\n<p>I know it is easy to convert from numpy array to PNG-image with PIL for example, but I only find how to do that combined with saving to disk in one function.</p>\\n\\n<p>Thank you very much for any help!</p>\\n\\n<p>This is my code so far:</p>\\n\\n<pre><code>import numpy as np\\nimport requests\\nfrom scipy.misc import toimage\\n\\narr = generate64x64x3ImageWithNumpy()\\ntoimage(arr, cmin=0.0, cmax=255.0).save('tmp.png')\\nd = {'key':API_KEY}\\nf= {'image': open('tmp.png', 'rb')}\\nresult = requests.post(SERVER_URL, files=f, data=d)\\n</code></pre>\\n\\n<p>I want this:</p>\\n\\n<pre><code>arr = generate64x64x3ImageWithNumpy()\\n\\nnot_on_disk = numpyArrayToPNGImageWithoutSavingOnDisk(arr)\\n\\nd = {'key':API_KEY}\\nf = {'image': not_on_disk}\\nresult = requests.post(SERVER_URL, files=f, data=d)\\n</code></pre>\\n\"}], 'has_more': False, 'quota_max': 300, 'quota_remaining': 264}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.10s/it]\n",
      " 17%|█▋        | 10/60 [00:41<03:28,  4.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [{'is_answered': True, 'view_count': 5684, 'accepted_answer_id': 46378809, 'answer_count': 6, 'score': 23, 'last_activity_date': 1646231735, 'creation_date': 1506120365, 'question_id': 46374185, 'link': 'https://stackoverflow.com/questions/46374185/does-python-have-a-function-which-computes-multinomial-coefficients', 'title': 'Does Python have a function which computes multinomial coefficients?', 'body': '<p>I was looking for a Python library function which computes <a href=\"https://en.wikipedia.org/wiki/Multinomial_theorem#Multinomial_coefficients\" rel=\"noreferrer\">multinomial coefficients</a>.</p>\\n\\n<p>I could not find any such function in any of the standard libraries. \\nFor binomial coefficients (of which multinomial coefficients are a generalization) there is <a href=\"https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.special.binom.html\" rel=\"noreferrer\">scipy.special.binom</a> and also <a href=\"https://docs.scipy.org/doc/scipy-0.19.1/reference/generated/scipy.misc.comb.html\" rel=\"noreferrer\">scipy.misc.comb</a>. Also, <a href=\"https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.random.multinomial.html\" rel=\"noreferrer\">numpy.random.multinomial</a> draws samples from a multinomial distribution, and <a href=\"http://docs.sympy.org/latest/_modules/sympy/ntheory/multinomial.html\" rel=\"noreferrer\">sympy.ntheory.multinomial.multinomial_coefficients</a> returns a dictionary related to multinomial coefficients.</p>\\n\\n<p>However, I could <em>not</em> find a multinomial coefficients function proper, which given <em>a,b,...,z</em> returns <em>(a+b+...+z)!/(a! b! ... z!).</em> Did I miss it? Is there a good reason there is none available?</p>\\n\\n<p>I would be happy to contribute an efficient implementation to SciPy say. (I would have to figure out how to contribute, as I have never done this).</p>\\n\\n<p>For background, they do come up when expanding <em>(a+b+...+z)^n.</em> Also, they count the ways of depositing <em>a+b+...+z</em> distinct objects into distinct bins such that the first bin contains <em>a</em> objects, etc. I need them occasionally for a Project Euler problem.</p>\\n\\n<p>BTW, other languages do offer this function: <a href=\"http://reference.wolfram.com/language/ref/Multinomial.html\" rel=\"noreferrer\">Mathematica</a>, <a href=\"https://de.mathworks.com/matlabcentral/fileexchange/6786-multcoef\" rel=\"noreferrer\">MATLAB</a>, <a href=\"https://de.mathworks.com/matlabcentral/fileexchange/6786-multcoef\" rel=\"noreferrer\">Maple</a>.</p>\\n'}, {'is_answered': True, 'view_count': 3737, 'accepted_answer_id': 21538085, 'answer_count': 1, 'score': 12, 'last_activity_date': 1391462682, 'creation_date': 1391455271, 'question_id': 21535852, 'link': 'https://stackoverflow.com/questions/21535852/whats-the-difference-between-scipy-special-binom-and-scipy-misc-comb', 'title': 'What&#39;s the difference between scipy.special.binom and scipy.misc.comb?', 'body': '<p>What is the difference between scipy.special.binom and scipy.misc.comb?</p>\\n\\n<p>In ipython I can see they return different types and also have different accuracy.</p>\\n\\n<pre><code>scipy.special.binom(4,3)\\n4.0\\n\\nscipy.misc.comb(4,3)\\narray(4.000000000000001)\\n</code></pre>\\n\\n<p>However what exactly are they doing differently?</p>\\n\\n<hr>\\n\\n<p>Looking at <a href=\"https://github.com/scipy/scipy/blob/master/scipy/special/generate_ufuncs.py\">https://github.com/scipy/scipy/blob/master/scipy/special/generate_ufuncs.py</a> , scipy.special.binom says</p>\\n\\n<pre><code>binom -- binom: dd-&gt;d                                      -- orthogonal_eval.pxd\\n</code></pre>\\n\\n<p>scipy.misc.comb calls scipy.special.gammaln whose line in generate_ufuncs.py says</p>\\n\\n<pre><code>gammaln -- lgam: d-&gt;d, clngamma_wrap: D-&gt;D                 -- cephes.h, specfun_wrappers.h\\n</code></pre>\\n'}, {'is_answered': True, 'view_count': 10516, 'accepted_answer_id': 3056276, 'answer_count': 6, 'score': 9, 'last_activity_date': 1423787717, 'creation_date': 1276713488, 'question_id': 3056179, 'link': 'https://stackoverflow.com/questions/3056179/binomial-test-in-python-for-very-large-numbers', 'title': 'Binomial test in Python for very large numbers', 'body': '<p>I need to do a binomial test in Python that allows calculation for \\'n\\' numbers of the order of 10000.</p>\\n\\n<p>I have implemented a quick binomial_test function using scipy.misc.comb, however, it is pretty much limited around n = 1000, I guess because it reaches the biggest representable number while computing factorials or the combinatorial itself. Here is my function:</p>\\n\\n<pre><code>from scipy.misc import comb\\ndef binomial_test(n, k):\\n    \"\"\"Calculate binomial probability\\n    \"\"\"\\n    p = comb(n, k) * 0.5**k * 0.5**(n-k)\\n    return p\\n</code></pre>\\n\\n<p>How could I use a native python (or numpy, scipy...) function in order to calculate that binomial probability? If possible, I need scipy 0.7.2 compatible code.</p>\\n\\n<p>Many thanks!</p>\\n'}, {'is_answered': True, 'view_count': 190, 'accepted_answer_id': 28997846, 'answer_count': 1, 'score': 4, 'last_activity_date': 1426112023, 'creation_date': 1426109562, 'question_id': 28997794, 'link': 'https://stackoverflow.com/questions/28997794/list-comprehension-with-repeated-computation', 'title': 'List comprehension with repeated computation', 'body': '<p>I am currently playing about with Project Euler problem 53 in Python. The solution is trivially simple but involved the following list comprehension:</p>\\n\\n<pre><code>[scipy.misc.comb(n, r, exact=True)\\n for n in range(1,maxn+1)\\n for r in range(0,n+1)\\n if scipy.misc.comb(n, r, exact=True) &gt; threshold]\\n</code></pre>\\n\\n<p>My concern is though that the scipy.misc.comb() function would be called twice per iteration. Is there any way of replacing one or other occurrence of it with some sort of reference; alternatively is the interpreter smart enough to realise that the two instances will evaluate to the same thing?</p>\\n'}, {'is_answered': True, 'view_count': 1757, 'answer_count': 2, 'score': 3, 'last_activity_date': 1392369982, 'creation_date': 1392333481, 'question_id': 21767690, 'link': 'https://stackoverflow.com/questions/21767690/python-log-n-choose-k', 'title': 'python log n choose k', 'body': \"<p>scipy.misc.comb, returning n choose k, is implemented using the gammaln function. Is there a function that stays in log space? I see there is no scipy.misc.combln or any similar. It is trivial to implement myself, but it would be convenient if it were already in a package somewhere. I don't see it in scipy.misc, and it just feels wasteful to convert to normal space and then back to log.</p>\\n\"}, {'is_answered': True, 'view_count': 638, 'accepted_answer_id': 35638553, 'answer_count': 1, 'score': 3, 'last_activity_date': 1456435559, 'creation_date': 1456430675, 'question_id': 35637223, 'link': 'https://stackoverflow.com/questions/35637223/scipy-calculate-of-multinomial-coefficents', 'title': 'Scipy calculate of multinomial coefficents', 'body': '<p>Short version:\\nTitle kind of says it all.</p>\\n\\n<p>Log Version:</p>\\n\\n<p>I\\'m calculating multinomial coefficents for the first few elements in. Looking at a <a href=\"https://en.wikipedia.org/wiki/Multinomial_theorem\" rel=\"nofollow\">wiki</a>, the math is fairly simple. </p>\\n\\n<pre><code>N! / (k1! * k2! * ....)\\n</code></pre>\\n\\n<p>For mediums sized N the numbers get fairly silly quickly if you just brute force it. For example,</p>\\n\\n<pre><code>500!/ (495! * 4! * 1!)\\n</code></pre>\\n\\n<p>has 500! in it, and <code>math.log(factorial(500), 10) ~= 1134</code> when the expression reduces too:</p>\\n\\n<pre><code>500!/ (495! * 4! * 1!) = 500 * 499 * 498 * 497 * 496 / 24\\n</code></pre>\\n\\n<p>I tried playing with <a href=\"http://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.comb.html#scipy.misc.comb\" rel=\"nofollow\">scipy.misc.comb</a>, which works great for binomial. For my use case k1 is much larger then ki (i != 1), so I could calculate the binomial coefficient and then convert that to the corresponding multinomial coefficient, but that seems a little round about.</p>\\n\\n<p>I assume there is a much better way.</p>\\n'}, {'is_answered': True, 'view_count': 901, 'answer_count': 4, 'score': 1, 'last_activity_date': 1418830378, 'creation_date': 1418781539, 'question_id': 27517186, 'link': 'https://stackoverflow.com/questions/27517186/calculate-very-large-number-using-python', 'title': 'Calculate very large number using python', 'body': \"<p>I'm trying to calculate (3e28 choose 2e28)/2^(3e28). I tried scipy.misc.comb to calculate 3e28 choose 2e28 but it gave me inf. When I calculate 2^(3e28), it raised OverflowError: (34, 'Result too large'). How can I compute or estimate (3e28 choose 2e28)/2^(3e28)?</p>\\n\"}], 'has_more': False, 'quota_max': 300, 'quota_remaining': 262}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/7 [00:00<?, ?it/s]\n",
      " 14%|█▍        | 1/7 [00:01<00:06,  1.10s/it]\n",
      " 29%|██▊       | 2/7 [00:02<00:05,  1.12s/it]\n",
      " 43%|████▎     | 3/7 [00:03<00:04,  1.16s/it]\n",
      " 57%|█████▋    | 4/7 [00:04<00:03,  1.15s/it]\n",
      " 71%|███████▏  | 5/7 [00:05<00:02,  1.15s/it]\n",
      " 86%|████████▌ | 6/7 [00:06<00:01,  1.15s/it]\n",
      "100%|██████████| 7/7 [00:07<00:00,  1.14s/it]\n",
      " 18%|█▊        | 11/60 [00:50<03:46,  4.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [{'is_answered': True, 'view_count': 195, 'accepted_answer_id': 41439748, 'answer_count': 1, 'score': 1, 'last_activity_date': 1483447357, 'creation_date': 1483432682, 'question_id': 41439421, 'link': 'https://stackoverflow.com/questions/41439421/scipy-misc-factorial-the-truth-value-of-an-array-with-more-than-one-element-is', 'title': 'scipy.misc.factorial : The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()', 'body': '<p>I followed this code on <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.misc.factorial.html\" rel=\"nofollow noreferrer\">scipy.misc.factorial</a>.</p>\\n\\n<p>However, I got this error.</p>\\n\\n<pre><code>if n &lt; 0:\\nValueError: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\\n</code></pre>\\n\\n<hr>\\n\\n<p>Here is the code:</p>\\n\\n<pre><code>import numpy as np\\nfrom scipy.special import factorial\\narr = np.array([3, 4, 5])\\nfactorial(arr, exact=True)\\n</code></pre>\\n\\n<p>While the following code has no error.</p>\\n\\n<pre><code>arr = np.array([3, 4, 5])\\nfactorial(arr, exact=False)  # exact=False\\n</code></pre>\\n'}], 'has_more': False, 'quota_max': 300, 'quota_remaining': 254}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.73s/it]\n",
      " 20%|██        | 12/60 [00:53<03:34,  4.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 252}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 13/60 [00:54<03:18,  4.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 251}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 14/60 [00:55<03:03,  4.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 250}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 15/60 [00:57<02:51,  3.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 249}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 16/60 [00:58<02:40,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 248}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 17/60 [00:59<02:30,  3.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 247}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 18/60 [01:00<02:21,  3.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 246}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 19/60 [01:02<02:15,  3.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 245}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 20/60 [01:03<02:07,  3.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [{'is_answered': True, 'view_count': 2492, 'accepted_answer_id': 25094555, 'answer_count': 2, 'score': 5, 'last_activity_date': 1570693646, 'creation_date': 1406846324, 'question_id': 25070086, 'link': 'https://stackoverflow.com/questions/25070086/percentiles-from-counts-of-values', 'title': 'percentiles from counts of values', 'body': '<p>I want to calculate <a href=\"http://en.wikipedia.org/wiki/Percentile\" rel=\"nofollow\">percentiles</a> from an ensemble of multiple large vectors in Python. Instead of trying to concatenate the vectors and then putting the resulting huge vector through <a href=\"http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.percentile.html\" rel=\"nofollow\">numpy.percentile</a>, is there a more efficient way?</p>\\n\\n<p>My idea would be, first, counting the frequencies of different values (e.g. using <a href=\"http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.itemfreq.html\" rel=\"nofollow\">scipy.stats.itemfreq</a>), second, combining those item frequencies for the different vectors, and finally, calculating the percentiles from the counts. </p>\\n\\n<p>Unfortunately I haven\\'t been able to find functions either for combining the frequency tables (it is not very simple, as different tables may cover different items), or for calculating percentiles from an item frequency table. Do I need to implement these, or can I use existing Python functions? What would those functions be?</p>\\n'}, {'is_answered': True, 'view_count': 949, 'accepted_answer_id': 36936280, 'answer_count': 3, 'score': 3, 'last_activity_date': 1461931856, 'creation_date': 1461924444, 'question_id': 36935485, 'link': 'https://stackoverflow.com/questions/36935485/frequency-count-per-sub-array-or-slice-in-a-3d-numpy-array', 'title': 'Frequency count per sub array or slice in a 3D NumPy array', 'body': '<p>I am trying to get the frequency count (without the zeros) per sub-array in a numpy 3d-array. However, the scipy.stats.itemfreq tool returns the frequency count in a 2d array.</p>\\n\\n<p>What I get is:</p>\\n\\n<pre><code>array_3d= array([[[1, 0, 0],\\n    [1, 0, 0],\\n    [0, 2, 0]],\\n\\n   [[0, 0, 0],\\n    [0, 0, 3],\\n    [3, 3, 3]],\\n\\n   [[0, 0, 4],\\n    [0, 0, 4],\\n    [0, 0, 4]]])\\n\\n&gt;&gt;&gt; itemfreq(array_3d)[1:,]\\n# outputs\\narray([ 1,  2],\\n   [ 2,  1],\\n   [ 3,  4],\\n   [ 4,  3]], dtype=int64)\\n</code></pre>\\n\\n<p>While I would like the output:</p>\\n\\n<pre><code>array([[ 1,  2, 2, 1],\\n   [ 3,  4],\\n   [ 4,  3]], dtype=object)\\n</code></pre>\\n\\n<p>The idea is that the uneven number is always the unique value and the even number the frequency. </p>\\n\\n<p>Another output could be:</p>\\n\\n<pre><code>array([ 1,  2, 0],\\n   [ 2,  1, 0],\\n   [ 3,  4, 1],\\n   [ 4,  3, 2]], dtype=int64)\\n</code></pre>\\n\\n<p>Where the third column represents the subset number in the 3d array.</p>\\n\\n<p>I am also open to other outputs/solutions!</p>\\n\\n<p>Thanks in advance!</p>\\n'}], 'has_more': False, 'quota_max': 300, 'quota_remaining': 244}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|█████     | 1/2 [00:01<00:01,  1.16s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.13s/it]\n",
      " 35%|███▌      | 21/60 [01:07<02:04,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 241}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 22/60 [01:08<01:58,  3.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 240}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 23/60 [01:09<01:51,  3.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 239}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 24/60 [01:10<01:45,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 238}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 25/60 [01:11<01:40,  2.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 237}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 26/60 [01:12<01:35,  2.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 236}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 27/60 [01:13<01:30,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 236}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 28/60 [01:15<01:25,  2.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 234}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 29/60 [01:16<01:21,  2.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 233}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 30/60 [01:17<01:17,  2.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 232}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 31/60 [01:18<01:13,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 231}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 32/60 [01:19<01:09,  2.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 230}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 33/60 [01:20<01:05,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 229}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 34/60 [01:21<01:02,  2.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 229}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 35/60 [01:22<00:58,  2.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 227}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 36/60 [01:23<00:55,  2.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 226}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 37/60 [01:24<00:52,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 225}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 38/60 [01:25<00:49,  2.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 224}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 39/60 [01:26<00:46,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [{'is_answered': True, 'view_count': 1578, 'answer_count': 3, 'score': 1, 'last_activity_date': 1439989530, 'creation_date': 1439983956, 'question_id': 32094305, 'link': 'https://stackoverflow.com/questions/32094305/selecting-elements-of-a-pandas-dataframe-that-fall-above-a-critical-threshold', 'title': 'Selecting elements of a pandas dataframe that fall above a critical threshold', 'body': \"<p>I have a pandas.df and I'm trying to remove all hypotheses that can be rejected. </p>\\n\\n<p>Here is a snippet of the df in question:</p>\\n\\n<pre><code>    best value   p_value  \\n0      11.9549  0.986927  \\n1      11.9588  0.986896  \\n2      12.1185  0.985588  \\n3      12.1682  0.985161  \\n4      12.3907  0.983131  \\n5      12.4148  0.982899  \\n6      12.6273  0.980750  \\n7      12.9020  0.977680  \\n8      13.4576  0.970384  \\n9      13.5058  0.969679  \\n10     13.5243  0.969405  \\n11     13.5886  0.968439  \\n12     13.8025  0.965067  \\n13     13.9840  0.962011  \\n14     14.1896  0.958326  \\n15     14.3939  0.954424  \\n16     14.6229  0.949758  \\n17     14.6689  0.948783  \\n18     14.9464  0.942626  \\n19     15.1216  0.938494  \\n20     15.5326  0.928039  \\n21     17.7720  0.851915  \\n22     17.8668  0.847993  \\n23     17.9662  0.843822  \\n24     19.2481  0.785072  \\n25     19.5257  0.771242  \\n</code></pre>\\n\\n<p>I want to remove the elements with a p_value greater then a critical threshold <code>alpha</code> by selecting the ones fall below <code>alpha</code>. The p value is calculated using scipy.stats.chisqprob(chisq,df) where <code>chisq</code> is the chi squared statistic and <code>df</code> is the degrees of freedom. This is all done using the custom method <code>self.get_p_values</code> shown below.</p>\\n\\n<pre><code>def reject_null_hypothesis(self,alpha,df):\\n    assert alpha&gt;0\\n    assert alpha&lt;1\\n    p_value=self.get_p_values(df)  #calculates the data frame above\\n    return p_value.loc[p_value['best value']\\n</code></pre>\\n\\n<p>Im then calling this method using:</p>\\n\\n<pre><code>PE=Modelling_Tools.PE_Results(PE_file)   #Modelling.Tools is the module and PE_Results is the class which is given the data 'PE_file' \\nprint PE.reject_null_hypothesis(0.5,25) \\n</code></pre>\\n\\n<p>From what I've read this should do what I want but I'm new to pandas.df and this code returns the unchanged</p>\\n\"}], 'has_more': False, 'quota_max': 300, 'quota_remaining': 223}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.11s/it]\n",
      " 67%|██████▋   | 40/60 [01:28<00:44,  2.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 221}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 41/60 [01:30<00:41,  2.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 220}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 42/60 [01:31<00:39,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 220}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 43/60 [01:32<00:36,  2.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 218}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 44/60 [01:33<00:33,  2.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [{'is_answered': False, 'view_count': 91, 'answer_count': 0, 'score': 0, 'last_activity_date': 1586517225, 'creation_date': 1586517225, 'question_id': 61139113, 'link': 'https://stackoverflow.com/questions/61139113/hanning-window-bandwidth', 'title': 'Hanning Window Bandwidth', 'body': '<p>I am using scipy\\'s \"scipy.signal.hanning\" to determine a hanning window. This method returns an array with the window and specific number of points based on your input. </p>\\n\\n<p>More info:\\n<a href=\"https://docs.scipy.org/doc/scipy-1.0.0/reference/generated/scipy.signal.hanning.html\" rel=\"nofollow noreferrer\">https://docs.scipy.org/doc/scipy-1.0.0/reference/generated/scipy.signal.hanning.html</a></p>\\n\\n<p>My question:</p>\\n\\n<p>Is there a connection between the desirable window points output and the bandwidth of the Window?</p>\\n\\n<p>Thanks</p>\\n'}], 'has_more': False, 'quota_max': 300, 'quota_remaining': 217}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      " 75%|███████▌  | 45/60 [01:34<00:31,  2.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 216}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 46/60 [01:35<00:29,  2.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 215}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 47/60 [01:36<00:26,  2.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 214}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 48/60 [01:37<00:24,  2.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [{'is_answered': True, 'view_count': 1753, 'accepted_answer_id': 49570892, 'answer_count': 1, 'score': 12, 'last_activity_date': 1522397948, 'creation_date': 1500381980, 'question_id': 45167237, 'link': 'https://stackoverflow.com/questions/45167237/matrix-exponentiation-with-scipy-expm-expm2-and-expm3', 'title': 'Matrix exponentiation with scipy: expm, expm2 and expm3', 'body': '<p>Matrix exponentiation can be performed in python using functions within the <code>scipy.linalg</code> library, namely <code>expm, expm2, expm3</code>. <code>expm</code> makes use of a Pade approximation; <code>expm2</code> uses the eigenvalue decomposition approach and <code>expm3</code> makes use of a Taylor series with a default number of terms of 20.</p>\\n\\n<p>In SciPy 0.13.0 release notes it is stated that: </p>\\n\\n<blockquote>\\n  <p>The matrix exponential functions scipy.linalg.expm2 and scipy.linalg.expm3 are deprecated. All users should use the numerically more robust scipy.linalg.expm function instead.</p>\\n</blockquote>\\n\\n<p>Although <code>expm2</code> and <code>expm3</code> are deprecated since release version SciPy 0.13.0, I have found that in many situations these implementations are faster than <code>expm</code>.\\nFrom this, some questions arise: </p>\\n\\n<p>In what situations could expm2 and expm3 result in numerical instabilities? </p>\\n\\n<p>In what situations (e.g. sparse matrices, symmetric, ...) is each of the algorithms faster/more precise?</p>\\n'}, {'is_answered': True, 'view_count': 2489, 'accepted_answer_id': 16082744, 'answer_count': 3, 'score': 2, 'last_activity_date': 1366377410, 'creation_date': 1366284752, 'question_id': 16081797, 'link': 'https://stackoverflow.com/questions/16081797/diagonal-matrix-exponential-in-python', 'title': 'Diagonal Matrix Exponential in Python', 'body': \"<p>I'm writing a numerical algorithm with speed in mind. I've come across the two matrix exponential functions in scipy/numpy (scipy.linalg.expm2, scipy.linalg.expm).  However I have a matrix that I know to be diagonal beforehand. Do these scipy functions check if the matrix is diagonal before they run? Obviously the exponentiation algorithm can be much faster for a diagonal matrix, and I just want to make sure that these are doing something smart with that - if they aren't, is there an easy way to do it?</p>\\n\"}], 'has_more': False, 'quota_max': 300, 'quota_remaining': 213}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|█████     | 1/2 [00:01<00:01,  1.10s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.10s/it]\n",
      " 82%|████████▏ | 49/60 [01:40<00:22,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [{'is_answered': True, 'view_count': 1753, 'accepted_answer_id': 49570892, 'answer_count': 1, 'score': 12, 'last_activity_date': 1522397948, 'creation_date': 1500381980, 'question_id': 45167237, 'link': 'https://stackoverflow.com/questions/45167237/matrix-exponentiation-with-scipy-expm-expm2-and-expm3', 'title': 'Matrix exponentiation with scipy: expm, expm2 and expm3', 'body': '<p>Matrix exponentiation can be performed in python using functions within the <code>scipy.linalg</code> library, namely <code>expm, expm2, expm3</code>. <code>expm</code> makes use of a Pade approximation; <code>expm2</code> uses the eigenvalue decomposition approach and <code>expm3</code> makes use of a Taylor series with a default number of terms of 20.</p>\\n\\n<p>In SciPy 0.13.0 release notes it is stated that: </p>\\n\\n<blockquote>\\n  <p>The matrix exponential functions scipy.linalg.expm2 and scipy.linalg.expm3 are deprecated. All users should use the numerically more robust scipy.linalg.expm function instead.</p>\\n</blockquote>\\n\\n<p>Although <code>expm2</code> and <code>expm3</code> are deprecated since release version SciPy 0.13.0, I have found that in many situations these implementations are faster than <code>expm</code>.\\nFrom this, some questions arise: </p>\\n\\n<p>In what situations could expm2 and expm3 result in numerical instabilities? </p>\\n\\n<p>In what situations (e.g. sparse matrices, symmetric, ...) is each of the algorithms faster/more precise?</p>\\n'}], 'has_more': False, 'quota_max': 300, 'quota_remaining': 210}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n",
      "100%|██████████| 1/1 [00:01<00:00,  1.13s/it]\n",
      " 83%|████████▎ | 50/60 [01:43<00:20,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 208}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 51/60 [01:44<00:18,  2.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 207}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 52/60 [01:45<00:16,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 206}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 53/60 [01:46<00:14,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 205}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 54/60 [01:47<00:11,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 204}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 55/60 [01:48<00:09,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 204}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 56/60 [01:49<00:07,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [{'is_answered': True, 'view_count': 875, 'accepted_answer_id': 31385411, 'answer_count': 1, 'score': 4, 'last_activity_date': 1436797429, 'creation_date': 1436352921, 'question_id': 31290597, 'link': 'https://stackoverflow.com/questions/31290597/python-get-surrounding-area-of-line-coordinates', 'title': 'Python - get surrounding area of line (coordinates)', 'body': '<p>I have my coords saved in numpy arrays x and y.\\nNow all I want is to get a polygon (respectively arrays of points) that defines the surrounding area with a given width parameter. </p>\\n\\n<p>The problem I have is that I need to have a polygon without(!) intersections. But this does occur, when there is a narrow curve. For my application it would be best to identify these points and omit them. Is there a way to easily find these points?</p>\\n\\n<p>So far I have:</p>\\n\\n<pre><code># x contains x coords\\n# y contains y coords\\n# angle contains the current moving direction (radian)\\nimport numpy as np\\n\\nphi = np.pi/2 + angle\\nx_left = x + dist*np.cos( phi )\\ny_left = y + dist*np.sin( phi )\\n\\nx_right = x - dist*np.cos( phi )\\ny_right = y - dist*np.sin( phi )\\n\\nx_total = hstack((x_left, x_right[::-1]))\\ny_total = hstack((y_left, y_right[::-1]))        \\n\\n##----- Omit Points with minimal dist &lt; threshold to ANY point on traj\\nx_res = []\\ny_res = []\\nfor idx in range(len(x_total)):\\n    m = np.min( np.sqrt( (x-x_total[idx])**2 + (y-y_total[idx])**2) )\\n    if m &gt; dist-epsilon:\\n        x_res.append( x_total[idx] )\\n        y_res.append( y_total[idx] )    \\npoints = np.vstack( (x_res, y_res) ).T\\n</code></pre>\\n\\n<p>Now \"points\" does contain the polygon surrounding the coord.-line that has the desired distance to the coord.-line. However there can still be some intersections in the polygon. I tried to get rid of them by interpolate (e.g. with scipy.interpolate.spline). But I couldn\\'t manage it to work properly.</p>\\n\\n<p>Can anyone please help =) ? </p>\\n'}, {'is_answered': True, 'view_count': 2173, 'accepted_answer_id': 43649817, 'answer_count': 3, 'score': 2, 'last_activity_date': 1493402854, 'creation_date': 1493241965, 'question_id': 43644790, 'link': 'https://stackoverflow.com/questions/43644790/how-to-improve-the-performance-when-2d-interpolating-smoothing-lines-using-scipy', 'title': 'How to improve the performance when 2d interpolating/smoothing lines using scipy?', 'body': '<p>I have a moderate size data set, namely 20000 x 2 floats in a two column matrix. The first column is the the x column which represents the distance to the original point along a trajectory, another column is the y column which represents the work has done to the object. This data set is obtained from lab operations, so it\\'s fairly arbitrary. I\\'ve already turned this structure into numpy array. I want to plot y vs x in a figure with a smooth curve. So I hope the following code could help me:</p>\\n\\n<pre><code>x_smooth = np.linspace(x.min(),x.max(), 20000)\\ny_smooth = spline(x, y, x_smooth)\\nplt.plot(x_smooth, y_smooth)\\nplt.show()\\n</code></pre>\\n\\n<p>However, when my program execute the line <code>y_smooth = spline(x,y,x_smooth)</code>, it takes a very long time,say 10 min, and even sometimes it will blow my memory that I have to restart my machine. I tried to reduce the chunk number to 200 and 2000 and none of them works. Then I checked the official scipy reference: <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.spline.html\" rel=\"nofollow noreferrer\">scipy.interpolate.spline</a> here. And they said that <code>spline</code> is deprecated in v 0.19, but I\\'m not using the new version. If <code>spline</code> is deprecated for quite a bit of the time, how to use the equivalent <code>Bspline</code> now? If <code>spline</code> is still functioning, then what causes the slow performance </p>\\n\\n<p>One portion of my data could look like this:</p>\\n\\n<pre><code>13.202      0.0\\n13.234738      -0.051354643759\\n12.999116      0.144464320836\\n12.86252      0.07396528119\\n13.1157      0.10019738758\\n13.357109      -0.30288563381\\n13.234004      -0.045792536285\\n12.836279      0.0362257166275\\n12.851597      0.0542649286915\\n13.110691      0.105297378401\\n13.220619      -0.0182963209185\\n13.092143      0.116647353635\\n12.545676      -0.641112204849\\n12.728248      -0.147460703493\\n12.874176      0.0755861585235\\n12.746764      -0.111583725833\\n13.024995      0.148079528382\\n13.106033      0.119481137144\\n13.327233      -0.197666132456\\n13.142423      0.0901867159545\\n</code></pre>\\n'}, {'is_answered': True, 'view_count': 1108, 'accepted_answer_id': 51457541, 'answer_count': 1, 'score': 0, 'last_activity_date': 1532187262, 'creation_date': 1532185876, 'question_id': 51457342, 'link': 'https://stackoverflow.com/questions/51457342/interpolate-curve-between-three-values', 'title': 'interpolate curve between three values', 'body': '<p>I have the following script that plots a graph:</p>\\n\\n<pre><code>x = np.array([0,1,2])\\ny = np.array([5, 4.31, 4.01])\\nplt.plot(x, y)\\nplt.show()\\n</code></pre>\\n\\n<p>The problem is, that the line goes straight from point to point, but I want to smooth the line between the points. \\n<a href=\"https://i.stack.imgur.com/mhV02.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/mhV02.png\" alt=\"enter image description here\"></a></p>\\n\\n<p>If I use scipy.interpolate.spline to smooth my data I got following result:</p>\\n\\n<pre><code> order = np.array([0,1,2])\\n y = np.array([5, 4.31, 4.01])\\n xnew = np.linspace(order.min(), order.max(), 300)\\n smooth = spline(order, y, xnew)\\n plt.plot(xnew, smooth)\\n plt.show()\\n</code></pre>\\n\\n<p><a href=\"https://i.stack.imgur.com/xvsmt.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/xvsmt.png\" alt=\"enter image description here\"></a></p>\\n\\n<p>But I want to have the same result like in that given <a href=\"https://stackoverflow.com/questions/5283649/plot-smooth-line-with-pyplot\">example</a></p>\\n'}, {'is_answered': False, 'view_count': 38, 'answer_count': 0, 'score': 0, 'last_activity_date': 1647340667, 'creation_date': 1647338078, 'question_id': 71480167, 'link': 'https://stackoverflow.com/questions/71480167/python-seabornsns-lineplot-with-fitting-smooth-how-to-do-this', 'title': 'python seaborn(SNS) lineplot with fitting smooth, how to do this?', 'body': '<p>I have a dataframe like this. I drawed Seaborn Line plot but the graph is not that i want.</p>\\n<p>This is my dataframe and code.</p>\\n<pre><code>import matplotlib.pyplot as plt\\nimport pandas as pd\\nimport seaborn as sns\\nimport datetime as dt\\n\\ndata = {\\'year\\': [\\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\',\\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\',\\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\',\\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\',\\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\',\\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\',\\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\',\\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\',\\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\',\\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\',\\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\',\\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\', \\'2001\\'],\\n        \\'month\\': [\\'1\\',\\'1\\',\\'1\\',\\'1\\',\\'1\\',\\'1\\',\\'2\\', \\'2\\', \\'2\\', \\'2\\', \\'2\\',\\'2\\',  \\'3\\', \\'3\\', \\'3\\', \\'3\\', \\'3\\', \\'3\\', \\'4\\', \\'4\\', \\'4\\', \\'4\\', \\'4\\', \\'4\\', \\'5\\', \\'5\\', \\'5\\', \\'5\\', \\'5\\', \\'5\\', \\'6\\', \\'6\\', \\'6\\', \\'6\\', \\'6\\', \\'6\\',\\'7\\',\\'7\\',\\'7\\',\\'7\\',\\'7\\',\\'7\\', \\'8\\', \\'8\\', \\'8\\', \\'8\\', \\'8\\', \\'8\\', \\'9\\', \\'9\\', \\'9\\', \\'9\\', \\'9\\', \\'9\\', \\'10\\', \\'10\\', \\'10\\', \\'10\\', \\'10\\', \\'10\\', \\'11\\', \\'11\\', \\'11\\', \\'11\\', \\'11\\', \\'11\\', \\'12\\', \\'12\\', \\'12\\', \\'12\\', \\'12\\', \\'12\\'],\\n        \\'unemployment\\': [0, 0, 0, 0, 0,1,0, 0, 0, 0, 0,1,0, 0, 0, 0, 0,1,0, 1, 1, 0, 0,1,0, 0, 0, 0, 0,1,0, 1, 1, 1, 0,1,0, 0, 0, 1, 0,1,0, 0, 1, 0, 0,1,0,1 , 1, 0, 0,1,0, 1, 1, 1, 1,1,0, 0,1, 1, 1,1,0, 0, 1, 1, 1,1 ]\\n        }\\n\\ndf = pd.DataFrame(data)\\n\\nprint (df)\\n\\ndate=pd.to_datetime(df[\\'year\\'].astype(str)  + df[\\'month\\'].astype(str), format=\\'%Y%m\\')\\n\\ndf[\\'date\\']=date\\n\\n# set figure size\\nplt.figure( figsize = ( 12, 5))\\n\\n# plot a simple time series plot\\n# using seaborn.lineplot()\\nsns.lineplot( x = \\'date\\',\\n            y = \\'unemployment\\',\\n            data = df,\\n            color=&quot;blue&quot;)\\n</code></pre>\\n<p>Output is this.</p>\\n<p><a href=\"https://i.stack.imgur.com/IwkqM.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/IwkqM.png\" alt=\"enter image description here\" /></a></p>\\n<p>However, I want to fit smooth line plot like this.</p>\\n<p><a href=\"https://i.stack.imgur.com/9ZD05.png%5C\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/9ZD05.png%5C\" alt=\"enter image description here\" /></a></p>\\n<p>I tried to use &quot;scipy.interpolate.spline&quot; but my code didn\\'t work.Any Idea?Thanks in advance.</p>\\n'}], 'has_more': False, 'quota_max': 300, 'quota_remaining': 202}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      " 25%|██▌       | 1/4 [00:01<00:03,  1.04s/it]\n",
      " 50%|█████     | 2/4 [00:02<00:02,  1.04s/it]\n",
      " 75%|███████▌  | 3/4 [00:03<00:01,  1.08s/it]\n",
      " 95%|█████████▌| 57/60 [01:54<00:06,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [], 'has_more': False, 'quota_max': 300, 'quota_remaining': 198}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 58/60 [01:55<00:03,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [{'is_answered': True, 'view_count': 2492, 'accepted_answer_id': 25094555, 'answer_count': 2, 'score': 5, 'last_activity_date': 1570693646, 'creation_date': 1406846324, 'question_id': 25070086, 'link': 'https://stackoverflow.com/questions/25070086/percentiles-from-counts-of-values', 'title': 'percentiles from counts of values', 'body': '<p>I want to calculate <a href=\"http://en.wikipedia.org/wiki/Percentile\" rel=\"nofollow\">percentiles</a> from an ensemble of multiple large vectors in Python. Instead of trying to concatenate the vectors and then putting the resulting huge vector through <a href=\"http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.percentile.html\" rel=\"nofollow\">numpy.percentile</a>, is there a more efficient way?</p>\\n\\n<p>My idea would be, first, counting the frequencies of different values (e.g. using <a href=\"http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.itemfreq.html\" rel=\"nofollow\">scipy.stats.itemfreq</a>), second, combining those item frequencies for the different vectors, and finally, calculating the percentiles from the counts. </p>\\n\\n<p>Unfortunately I haven\\'t been able to find functions either for combining the frequency tables (it is not very simple, as different tables may cover different items), or for calculating percentiles from an item frequency table. Do I need to implement these, or can I use existing Python functions? What would those functions be?</p>\\n'}, {'is_answered': True, 'view_count': 949, 'accepted_answer_id': 36936280, 'answer_count': 3, 'score': 3, 'last_activity_date': 1461931856, 'creation_date': 1461924444, 'question_id': 36935485, 'link': 'https://stackoverflow.com/questions/36935485/frequency-count-per-sub-array-or-slice-in-a-3d-numpy-array', 'title': 'Frequency count per sub array or slice in a 3D NumPy array', 'body': '<p>I am trying to get the frequency count (without the zeros) per sub-array in a numpy 3d-array. However, the scipy.stats.itemfreq tool returns the frequency count in a 2d array.</p>\\n\\n<p>What I get is:</p>\\n\\n<pre><code>array_3d= array([[[1, 0, 0],\\n    [1, 0, 0],\\n    [0, 2, 0]],\\n\\n   [[0, 0, 0],\\n    [0, 0, 3],\\n    [3, 3, 3]],\\n\\n   [[0, 0, 4],\\n    [0, 0, 4],\\n    [0, 0, 4]]])\\n\\n&gt;&gt;&gt; itemfreq(array_3d)[1:,]\\n# outputs\\narray([ 1,  2],\\n   [ 2,  1],\\n   [ 3,  4],\\n   [ 4,  3]], dtype=int64)\\n</code></pre>\\n\\n<p>While I would like the output:</p>\\n\\n<pre><code>array([[ 1,  2, 2, 1],\\n   [ 3,  4],\\n   [ 4,  3]], dtype=object)\\n</code></pre>\\n\\n<p>The idea is that the uneven number is always the unique value and the even number the frequency. </p>\\n\\n<p>Another output could be:</p>\\n\\n<pre><code>array([ 1,  2, 0],\\n   [ 2,  1, 0],\\n   [ 3,  4, 1],\\n   [ 4,  3, 2]], dtype=int64)\\n</code></pre>\\n\\n<p>Where the third column represents the subset number in the 3d array.</p>\\n\\n<p>I am also open to other outputs/solutions!</p>\\n\\n<p>Thanks in advance!</p>\\n'}], 'has_more': False, 'quota_max': 300, 'quota_remaining': 197}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n",
      " 50%|█████     | 1/2 [00:01<00:01,  1.04s/it]\n",
      "100%|██████████| 2/2 [00:02<00:00,  1.07s/it]\n",
      " 98%|█████████▊| 59/60 [01:58<00:02,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'items': [{'is_answered': True, 'view_count': 2153562, 'protected_date': 1531806256, 'accepted_answer_id': 944733, 'answer_count': 18, 'score': 1383, 'last_activity_date': 1647686548, 'creation_date': 1244035194, 'question_id': 944700, 'link': 'https://stackoverflow.com/questions/944700/how-can-i-check-for-nan-values', 'title': 'How can I check for NaN values?', 'body': \"<p><code>float('nan')</code> results in Nan (not a number). But how do I check for it? Should be very easy, but I cannot find it.</p>\\n\"}, {'is_answered': True, 'view_count': 71726, 'accepted_answer_id': 9033306, 'answer_count': 5, 'score': 776, 'last_activity_date': 1588772170, 'creation_date': 1327664990, 'question_id': 9032856, 'link': 'https://stackoverflow.com/questions/9032856/what-is-the-explanation-for-these-bizarre-javascript-behaviours-mentioned-in-the', 'title': 'What is the explanation for these bizarre JavaScript behaviours mentioned in the &#39;Wat&#39; talk for CodeMash 2012?', 'body': '<p>The <em><a href=\"https://www.destroyallsoftware.com/talks/wat\" rel=\"noreferrer\">\\'Wat\\' talk for CodeMash 2012</a></em> basically points out a few bizarre quirks with Ruby and JavaScript.</p>\\n\\n<p>I have made a JSFiddle of the results at <a href=\"http://jsfiddle.net/fe479/9/\" rel=\"noreferrer\">http://jsfiddle.net/fe479/9/</a>.</p>\\n\\n<p>The behaviours specific to JavaScript (as I don\\'t know Ruby) are listed below.</p>\\n\\n<p>I found in the JSFiddle that some of my results didn\\'t correspond with those in the video, and I am not sure why. I am, however, curious to know how JavaScript is handling working behind the scenes in each case.</p>\\n\\n<pre><code>Empty Array + Empty Array\\n[] + []\\nresult:\\n&lt;Empty String&gt;\\n</code></pre>\\n\\n<p>I am quite curious about the <code>+</code> operator when used with arrays in JavaScript.\\nThis matches the video\\'s result.</p>\\n\\n<pre><code>Empty Array + Object\\n[] + {}\\nresult:\\n[Object]\\n</code></pre>\\n\\n<p>This matches the video\\'s result. What\\'s going on here? Why is this an object. What does the <code>+</code> operator do?</p>\\n\\n<pre><code>Object + Empty Array\\n{} + []\\nresult:\\n[Object]\\n</code></pre>\\n\\n<p>This doesn\\'t match the video. The video suggests that the result is 0, whereas I get [Object].</p>\\n\\n<pre><code>Object + Object\\n{} + {}\\nresult:\\n[Object][Object]\\n</code></pre>\\n\\n<p>This doesn\\'t match the video either, and how does outputting a variable result in two objects? Maybe my JSFiddle is wrong.</p>\\n\\n<pre><code>Array(16).join(\"wat\" - 1)\\nresult:\\nNaNNaNNaNNaNNaNNaNNaNNaNNaNNaNNaNNaNNaNNaNNaNNaN\\n</code></pre>\\n\\n<p>Doing wat + 1 results in <code>wat1wat1wat1wat1</code>...</p>\\n\\n<p>I suspect this is just straightforward behaviour that trying to subtract a number from a string results in NaN.</p>\\n'}, {'is_answered': True, 'view_count': 1914409, 'protected_date': 1526542165, 'accepted_answer_id': 13786327, 'answer_count': 7, 'score': 734, 'last_activity_date': 1646802626, 'creation_date': 1355021438, 'question_id': 13784192, 'link': 'https://stackoverflow.com/questions/13784192/creating-an-empty-pandas-dataframe-then-filling-it', 'title': 'Creating an empty Pandas DataFrame, then filling it?', 'body': '<p>I\\'m starting from the pandas DataFrame docs here: <a href=\"http://pandas.pydata.org/pandas-docs/stable/dsintro.html\" rel=\"noreferrer\">http://pandas.pydata.org/pandas-docs/stable/dsintro.html</a></p>\\n\\n<p>I\\'d like to iteratively fill the DataFrame with values in a time series kind of calculation.\\nSo basically, I\\'d like to initialize the DataFrame with columns A, B and timestamp rows, all 0 or all NaN.</p>\\n\\n<p>I\\'d then add initial values and go over this data calculating the new row from the row before, say <code>row[A][t] = row[A][t-1]+1</code> or so.</p>\\n\\n<p>I\\'m currently using the code as below, but I feel it\\'s kind of ugly and there must be a  way to do this with a DataFrame directly, or just a better way in general.\\nNote: I\\'m using Python 2.7.</p>\\n\\n<pre><code>import datetime as dt\\nimport pandas as pd\\nimport scipy as s\\n\\nif __name__ == \\'__main__\\':\\n    base = dt.datetime.today().date()\\n    dates = [ base - dt.timedelta(days=x) for x in range(0,10) ]\\n    dates.sort()\\n\\n    valdict = {}\\n    symbols = [\\'A\\',\\'B\\', \\'C\\']\\n    for symb in symbols:\\n        valdict[symb] = pd.Series( s.zeros( len(dates)), dates )\\n\\n    for thedate in dates:\\n        if thedate &gt; dates[0]:\\n            for symb in valdict:\\n                valdict[symb][thedate] = 1+valdict[symb][thedate - dt.timedelta(days=1)]\\n\\n    print valdict\\n</code></pre>\\n'}, {'is_answered': True, 'view_count': 1357958, 'protected_date': 1539675375, 'accepted_answer_id': 13842286, 'answer_count': 23, 'score': 686, 'last_activity_date': 1633630590, 'creation_date': 1355323245, 'question_id': 13842088, 'link': 'https://stackoverflow.com/questions/13842088/set-value-for-particular-cell-in-pandas-dataframe-using-index', 'title': 'Set value for particular cell in pandas DataFrame using index', 'body': \"<p>I have created a Pandas DataFrame</p>\\n<pre><code>df = DataFrame(index=['A','B','C'], columns=['x','y'])\\n</code></pre>\\n<p>and have got this</p>\\n<pre>\\n    x    y\\nA  NaN  NaN\\nB  NaN  NaN\\nC  NaN  NaN\\n</pre>\\n<p>Now, I would like to assign a value to particular cell, for example to row <code>C</code> and column <code>x</code>.\\nI would expect to get this result:</p>\\n<pre>\\n    x    y\\nA  NaN  NaN\\nB  NaN  NaN\\nC  10  NaN\\n</pre>\\n<p>with this code:</p>\\n<pre><code>df.xs('C')['x'] = 10\\n</code></pre>\\n<p>However, the contents of <code>df</code> has not changed. The dataframe contains yet again only <code>NaN</code>s.</p>\\n<p>Any suggestions?</p>\\n\"}, {'is_answered': True, 'view_count': 1084095, 'accepted_answer_id': 29530601, 'answer_count': 26, 'score': 609, 'last_activity_date': 1642844040, 'creation_date': 1428556179, 'question_id': 29530232, 'link': 'https://stackoverflow.com/questions/29530232/how-to-check-if-any-value-is-nan-in-a-pandas-dataframe', 'title': 'How to check if any value is NaN in a Pandas DataFrame', 'body': '<p>In Python Pandas, what\\'s the best way to check whether a DataFrame has one (or more) NaN values?</p>\\n\\n<p>I know about the function <code>pd.isnan</code>, but this returns a DataFrame of booleans for each element. <a href=\"https://stackoverflow.com/questions/27754891/python-nan-value-in-pandas\">This post</a> right here doesn\\'t exactly answer my question either.</p>\\n'}, {'is_answered': True, 'view_count': 1066736, 'protected_date': 1530966871, 'accepted_answer_id': 13295801, 'answer_count': 17, 'score': 576, 'last_activity_date': 1648593481, 'creation_date': 1352400639, 'question_id': 13295735, 'link': 'https://stackoverflow.com/questions/13295735/how-to-replace-nan-values-by-zeroes-in-a-column-of-a-pandas-dataframe', 'title': 'How to replace NaN values by Zeroes in a column of a Pandas Dataframe?', 'body': '<p>I have a Pandas Dataframe as below:</p>\\n<pre><code>      itm Date                  Amount \\n67    420 2012-09-30 00:00:00   65211\\n68    421 2012-09-09 00:00:00   29424\\n69    421 2012-09-16 00:00:00   29877\\n70    421 2012-09-23 00:00:00   30990\\n71    421 2012-09-30 00:00:00   61303\\n72    485 2012-09-09 00:00:00   71781\\n73    485 2012-09-16 00:00:00     NaN\\n74    485 2012-09-23 00:00:00   11072\\n75    485 2012-09-30 00:00:00  113702\\n76    489 2012-09-09 00:00:00   64731\\n77    489 2012-09-16 00:00:00     NaN\\n</code></pre>\\n<p>When I try to apply a function to the Amount column, I get the following error:</p>\\n<pre><code>ValueError: cannot convert float NaN to integer\\n</code></pre>\\n<p>I have tried applying a function using .isnan from the Math Module\\nI have tried the pandas .replace attribute\\nI tried the .sparse data attribute from pandas 0.9\\nI have also tried if NaN == NaN statement in a function.\\nI have also looked at this article <a href=\"https://stackoverflow.com/questions/8161836/how-do-i-replace-na-values-with-zeros-in-r\">How do I replace NA values with zeros in an R dataframe?</a> whilst looking at some other articles.\\nAll the methods I have tried have not worked or do not recognise NaN.\\nAny Hints or solutions would be appreciated.</p>\\n'}, {'is_answered': True, 'view_count': 488767, 'protected_date': 1562192230, 'accepted_answer_id': 26838140, 'answer_count': 8, 'score': 329, 'last_activity_date': 1642242291, 'creation_date': 1415600966, 'question_id': 26837998, 'link': 'https://stackoverflow.com/questions/26837998/pandas-replace-nan-with-blank-empty-string', 'title': 'Pandas Replace NaN with blank/empty string', 'body': '<p>I have a Pandas Dataframe as shown below:</p>\\n\\n<pre><code>    1    2       3\\n 0  a  NaN    read\\n 1  b    l  unread\\n 2  c  NaN    read\\n</code></pre>\\n\\n<p>I want to remove the NaN values with an empty string so that it looks like so:</p>\\n\\n<pre><code>    1    2       3\\n 0  a   \"\"    read\\n 1  b    l  unread\\n 2  c   \"\"    read\\n</code></pre>\\n'}, {'is_answered': True, 'view_count': 62096, 'protected_date': 1507028623, 'accepted_answer_id': 1573715, 'answer_count': 11, 'score': 321, 'last_activity_date': 1528041430, 'creation_date': 1255511957, 'question_id': 1565164, 'link': 'https://stackoverflow.com/questions/1565164/what-is-the-rationale-for-all-comparisons-returning-false-for-ieee754-nan-values', 'title': 'What is the rationale for all comparisons returning false for IEEE754 NaN values?', 'body': '<p>Why do comparisons of NaN values behave differently from all other values?\\nThat is, all comparisons with the operators ==, &lt;=, >=, &lt;, > where one or both values is NaN returns false, contrary to the behaviour of all other values.</p>\\n\\n<p>I suppose this simplifies numerical computations in some way, but I couldn\\'t find an explicitly stated reason, not even in the <a href=\"http://www.cs.berkeley.edu/~wkahan/ieee754status/IEEE754.PDF\">Lecture Notes on the Status of IEEE 754</a> by Kahan which discusses other design decisions in detail.</p>\\n\\n<p>This deviant behavior is causing trouble when doing simple data processing. For example, when sorting a list of records w.r.t. some real-valued field in a C program I need to write extra code to handle NaN as the maximal element, otherwise the sort algorithm could become confused.</p>\\n\\n<p><strong>Edit:</strong>\\nThe answers so far all argue that it is meaningless to compare NaNs.</p>\\n\\n<p>I agree, but that doesn\\'t mean that the correct answer is false,\\nrather it would be a Not-a-Boolean (NaB), which fortunately doesn\\'t exist.</p>\\n\\n<p>So the choice of returning true or false for comparisons is in my view arbitrary,\\nand for general data processing it would be advantageous if it obeyed the usual laws\\n(reflexivity of ==, trichotomy of &lt;, ==, >),\\nlest data structures which rely on these laws become confused.</p>\\n\\n<p>So I\\'m asking for some concrete advantage of breaking these laws, not just philosophical reasoning.</p>\\n\\n<p><strong>Edit 2:</strong>\\nI think I understand now why making NaN maximal would be a bad idea, it would mess up the computation of upper limits.</p>\\n\\n<p>NaN != NaN might be desirable to avoid detecting convergence in a loop such as</p>\\n\\n<pre><code>while (x != oldX) {\\n    oldX = x;\\n    x = better_approximation(x);\\n}\\n</code></pre>\\n\\n<p>which however should better be written by comparing the absolute difference with a small limit.\\nSo IMHO this is a relatively weak argument for breaking reflexivity at NaN.</p>\\n'}, {'is_answered': True, 'view_count': 579108, 'protected_date': 1562320498, 'accepted_answer_id': 11620982, 'answer_count': 12, 'score': 314, 'last_activity_date': 1615833371, 'creation_date': 1343079414, 'question_id': 11620914, 'link': 'https://stackoverflow.com/questions/11620914/removing-nan-values-from-an-array', 'title': 'Removing nan values from an array', 'body': '<p>I want to figure out how to remove nan values from my array. My array looks something like this: </p>\\n\\n<pre><code>x = [1400, 1500, 1600, nan, nan, nan ,1700] #Not in this exact configuration\\n</code></pre>\\n\\n<p>How can I remove the <code>nan</code> values from <code>x</code>?</p>\\n'}, {'is_answered': True, 'view_count': 241620, 'accepted_answer_id': 7540412, 'answer_count': 11, 'score': 296, 'last_activity_date': 1647436031, 'creation_date': 1316882963, 'question_id': 7540397, 'link': 'https://stackoverflow.com/questions/7540397/convert-nan-to-0-in-javascript', 'title': 'Convert NaN to 0 in JavaScript', 'body': '<p>Is there a way to convert NaN values to 0 without an <em>if</em> statement? Example:</p>\\n<pre><code>if (isNaN(a)) a = 0;\\n</code></pre>\\n<p>It is very annoying to check my variables every time.</p>\\n'}, {'is_answered': True, 'view_count': 540736, 'protected_date': 1530138521, 'accepted_answer_id': 18691949, 'answer_count': 12, 'score': 261, 'last_activity_date': 1625650884, 'creation_date': 1378684445, 'question_id': 18689823, 'link': 'https://stackoverflow.com/questions/18689823/pandas-dataframe-replace-nan-values-with-average-of-columns', 'title': 'pandas DataFrame: replace nan values with average of columns', 'body': '<p>I\\'ve got a pandas DataFrame filled mostly with real numbers, but there is a few <code>nan</code> values in it as well.</p>\\n\\n<p>How can I replace the <code>nan</code>s with averages of columns where they are?</p>\\n\\n<p>This question is very similar to this one: <a href=\"https://stackoverflow.com/questions/18689235/numpy-array-replace-nan-values-with-average-of-columns\">numpy array: replace nan values with average of columns</a>  but, unfortunately, the solution given there doesn\\'t work for a pandas DataFrame.</p>\\n'}, {'is_answered': True, 'view_count': 579258, 'accepted_answer_id': 9873379, 'answer_count': 18, 'score': 244, 'last_activity_date': 1648325300, 'creation_date': 1332769134, 'question_id': 9873197, 'link': 'https://stackoverflow.com/questions/9873197/how-to-convert-date-to-timestamp', 'title': 'How to convert date to timestamp?', 'body': '<p>I want to convert date to timestamp, my input is <code>26-02-2012</code>. I used </p>\\n\\n<pre><code>new Date(myDate).getTime();\\n</code></pre>\\n\\n<p>It says NaN.. Can any one tell how to convert this?</p>\\n'}, {'is_answered': True, 'view_count': 239995, 'protected_date': 1603855035, 'answer_count': 7, 'score': 227, 'last_activity_date': 1629919076, 'creation_date': 1377437321, 'question_id': 18429491, 'link': 'https://stackoverflow.com/questions/18429491/pandas-groupby-columns-with-nan-missing-values', 'title': 'pandas GroupBy columns with NaN (missing) values', 'body': \"<p>I have a DataFrame with many missing values in columns which I wish to groupby:</p>\\n\\n<pre><code>import pandas as pd\\nimport numpy as np\\ndf = pd.DataFrame({'a': ['1', '2', '3'], 'b': ['4', np.NaN, '6']})\\n\\nIn [4]: df.groupby('b').groups\\nOut[4]: {'4': [0], '6': [2]}\\n</code></pre>\\n\\n<p>see that Pandas has dropped the rows with NaN target values. (I want to include these rows!)</p>\\n\\n<p><em>Since I need many such operations (many cols have missing values), and use more complicated functions than just medians (typically random forests), I want to avoid writing too complicated pieces of code.</em></p>\\n\\n<p>Any suggestions? Should I write a function for this or is there a simple solution?</p>\\n\"}, {'is_answered': True, 'view_count': 287636, 'accepted_answer_id': 36226137, 'answer_count': 14, 'score': 224, 'last_activity_date': 1644326298, 'creation_date': 1458931836, 'question_id': 36226083, 'link': 'https://stackoverflow.com/questions/36226083/how-to-find-which-columns-contain-any-nan-value-in-pandas-dataframe', 'title': 'How to find which columns contain any NaN value in Pandas dataframe', 'body': '<p>Given a pandas dataframe containing possible NaN values scattered here and there:</p>\\n\\n<p><strong>Question:</strong> How do I determine which columns contain NaN values? In particular, can I get a list of the column names containing NaNs?</p>\\n'}, {'is_answered': True, 'view_count': 248768, 'accepted_answer_id': 14163209, 'answer_count': 14, 'score': 219, 'last_activity_date': 1646729028, 'creation_date': 1357323966, 'question_id': 14162723, 'link': 'https://stackoverflow.com/questions/14162723/replacing-pandas-or-numpy-nan-with-a-none-to-use-with-mysqldb', 'title': 'Replacing Pandas or Numpy Nan with a None to use with MysqlDB', 'body': \"<p>I am trying to write a Pandas dataframe (or can use a numpy array) to a mysql database using MysqlDB . MysqlDB doesn't seem understand 'nan' and my database throws out an error saying nan is not in the field list. I need to find a way to convert the 'nan' into a NoneType.</p>\\n\\n<p>Any ideas? </p>\\n\"}, {'is_answered': True, 'view_count': 85312, 'answer_count': 10, 'score': 203, 'last_activity_date': 1625139131, 'creation_date': 1252951866, 'question_id': 1423081, 'link': 'https://stackoverflow.com/questions/1423081/json-left-out-infinity-and-nan-json-status-in-ecmascript', 'title': 'JSON left out Infinity and NaN; JSON status in ECMAScript?', 'body': '<p>Any idea why JSON left out NaN and +/- Infinity? It puts Javascript in the strange situation where objects that would otherwise be serializable, are not, if they contain NaN or +/- infinity values.</p>\\n<p>Looks like this has been cast in stone: see <a href=\"https://www.rfc-editor.org/rfc/rfc4627\" rel=\"nofollow noreferrer\">RFC4627</a> and <a href=\"http://www.ecma-international.org/publications/files/ECMA-ST/Ecma-262.pdf\" rel=\"nofollow noreferrer\">ECMA-262</a> (section 24.5.2, JSON.stringify, NOTE 4, page 683 of the ECMA-262 pdf at last edit):</p>\\n<blockquote>\\n<p>Finite numbers are stringified as if by calling <code>ToString(number)</code>. <strong>NaN</strong> and Infinity regardless of sign are represented as the String <code>null</code>.</p>\\n</blockquote>\\n'}, {'is_answered': True, 'view_count': 6751, 'protected_date': 1342118616, 'closed_date': 1398333503, 'accepted_answer_id': 11066117, 'answer_count': 2, 'score': 203, 'last_activity_date': 1342118499, 'creation_date': 1339871713, 'question_id': 11066050, 'link': 'https://stackoverflow.com/questions/11066050/why-are-my-balls-disappearing', 'closed_reason': 'Not suitable for this site', 'title': 'Why are my balls disappearing?', 'body': '<p>Pardon the funny title. I\\'ve created a little graphic demo of 200 balls bouncing and colliding, both against the walls and each other. You can see what I have currently here: <a href=\"http://www.exeneva.com/html5/multipleBallsBouncingAndColliding/\" rel=\"noreferrer\">http://www.exeneva.com/html5/multipleBallsBouncingAndColliding/</a></p>\\n\\n<p>The problem is that whenever they collide with each other, they disappear. I\\'m not sure why. Can someone take a look and help me out?</p>\\n\\n<p>UPDATE: Apparently the balls array has balls with coordinates of NaN. Below is the code where I push balls to the array. I\\'m not entirely sure how the coordinates are getting NaN.</p>\\n\\n<pre><code>// Variables\\nvar numBalls = 200;  // number of balls\\nvar maxSize = 15;\\nvar minSize = 5;\\nvar maxSpeed = maxSize + 5;\\nvar balls = new Array();\\nvar tempBall;\\nvar tempX;\\nvar tempY;\\nvar tempSpeed;\\nvar tempAngle;\\nvar tempRadius;\\nvar tempRadians;\\nvar tempVelocityX;\\nvar tempVelocityY;\\n\\n// Find spots to place each ball so none start on top of each other\\nfor (var i = 0; i &lt; numBalls; i += 1) {\\n  tempRadius = 5;\\n  var placeOK = false;\\n  while (!placeOK) {\\n    tempX = tempRadius * 3 + (Math.floor(Math.random() * theCanvas.width) - tempRadius * 3);\\n    tempY = tempRadius * 3 + (Math.floor(Math.random() * theCanvas.height) - tempRadius * 3);\\n    tempSpeed = 4;\\n    tempAngle = Math.floor(Math.random() * 360);\\n    tempRadians = tempAngle * Math.PI/180;\\n    tempVelocityX = Math.cos(tempRadians) * tempSpeed;\\n    tempVelocityY = Math.sin(tempRadians) * tempSpeed;\\n\\n    tempBall = {\\n      x: tempX, \\n      y: tempY, \\n      nextX: tempX, \\n      nextY: tempY, \\n      radius: tempRadius, \\n      speed: tempSpeed,\\n      angle: tempAngle,\\n      velocityX: tempVelocityX,\\n      velocityY: tempVelocityY,\\n      mass: tempRadius\\n    };\\n    placeOK = canStartHere(tempBall);\\n  }\\n  balls.push(tempBall);\\n}\\n</code></pre>\\n'}, {'is_answered': True, 'view_count': 88907, 'accepted_answer_id': 51997100, 'answer_count': 9, 'score': 201, 'last_activity_date': 1648603592, 'creation_date': 1342636202, 'question_id': 11548005, 'link': 'https://stackoverflow.com/questions/11548005/numpy-or-pandas-keeping-array-type-as-integer-while-having-a-nan-value', 'title': 'NumPy or Pandas: Keeping array type as integer while having a NaN value', 'body': \"<p>Is there a preferred way to keep the data type of a <code>numpy</code> array fixed as <code>int</code> (or <code>int64</code> or whatever), while still having an element inside listed as <code>numpy.NaN</code>?</p>\\n\\n<p>In particular, I am converting an in-house data structure to a Pandas DataFrame. In our structure, we have integer-type columns that still have NaN's (but the dtype of the column is int). It seems to recast everything as a float if we make this a DataFrame, but we'd really like to be <code>int</code>.</p>\\n\\n<p>Thoughts?</p>\\n\\n<p><strong>Things tried:</strong></p>\\n\\n<p>I tried using the <code>from_records()</code> function under pandas.DataFrame, with <code>coerce_float=False</code> and this did not help. I also tried using NumPy masked arrays, with NaN fill_value, which also did not work. All of these caused the column data type to become a float.</p>\\n\"}, {'is_answered': True, 'view_count': 176143, 'accepted_answer_id': 28312011, 'answer_count': 7, 'score': 199, 'last_activity_date': 1619610810, 'creation_date': 1423011437, 'question_id': 28311655, 'link': 'https://stackoverflow.com/questions/28311655/ignoring-nans-with-str-contains', 'title': 'Ignoring NaNs with str.contains', 'body': '<p>I want to find rows that contain a string, like so:</p>\\n\\n<pre><code>DF[DF.col.str.contains(\"foo\")]\\n</code></pre>\\n\\n<p>However, this fails because some elements are NaN:</p>\\n\\n<blockquote>\\n  <p>ValueError: cannot index with vector containing NA / NaN values</p>\\n</blockquote>\\n\\n<p>So I resort to the obfuscated</p>\\n\\n<pre><code>DF[DF.col.notnull()][DF.col.dropna().str.contains(\"foo\")]\\n</code></pre>\\n\\n<p>Is there a better way?</p>\\n'}, {'is_answered': True, 'view_count': 83069, 'accepted_answer_id': 2801629, 'answer_count': 21, 'score': 198, 'last_activity_date': 1600731830, 'creation_date': 1273483831, 'question_id': 2801601, 'link': 'https://stackoverflow.com/questions/2801601/why-does-typeof-nan-return-number', 'title': 'Why does typeof NaN return &#39;number&#39;?', 'body': \"<p>Just out of curiosity. </p>\\n\\n<p>It doesn't seem very logical that <code>typeof NaN</code> is number. Just like <code>NaN === NaN</code> or <code>NaN == NaN</code> returning false, by the way. Is this one of the peculiarities of javascript, or would there be a reason for this?</p>\\n\\n<p>Edit: thanks for your answers. It's not an easy thing to get ones head around though. Reading answers and the wiki I understood more, but still, a sentence like </p>\\n\\n<blockquote>\\n  <p>A comparison with a NaN always returns an unordered result even when comparing with itself. The comparison predicates are either signaling or non-signaling, the signaling versions signal an invalid exception for such comparisons. The equality and inequality predicates are non-signaling so x = x returning false can be used to test if x is a quiet NaN. </p>\\n</blockquote>\\n\\n<p>just keeps my head spinning. If someone can translate this in human (as opposed to, say, mathematician) readable language, I would be gratefull.</p>\\n\"}, {'is_answered': True, 'view_count': 252166, 'accepted_answer_id': 20027386, 'answer_count': 6, 'score': 195, 'last_activity_date': 1640012252, 'creation_date': 1384649769, 'question_id': 20025882, 'link': 'https://stackoverflow.com/questions/20025882/add-a-string-prefix-to-each-value-in-a-string-column-using-pandas', 'title': 'add a string prefix to each value in a string column using Pandas', 'body': \"<p>I would like to append a string to the start of each value in a said column of a pandas dataframe (elegantly).\\nI already figured out how to kind-of do this and I am currently using:</p>\\n\\n<pre><code>df.ix[(df['col'] != False), 'col'] = 'str'+df[(df['col'] != False), 'col']\\n</code></pre>\\n\\n<p>This seems one hell of an inelegant thing to do - do you know any other way (which maybe also adds the character to rows where that column is 0 or NaN)?</p>\\n\\n<p>In case this is yet unclear, I would like to turn:</p>\\n\\n<pre><code>    col \\n1     a\\n2     0\\n</code></pre>\\n\\n<p>into:</p>\\n\\n<pre><code>       col \\n1     stra\\n2     str0\\n</code></pre>\\n\"}, {'is_answered': True, 'view_count': 173798, 'protected_date': 1562192420, 'accepted_answer_id': 12307162, 'answer_count': 7, 'score': 179, 'last_activity_date': 1639736236, 'creation_date': 1346959945, 'question_id': 12307099, 'link': 'https://stackoverflow.com/questions/12307099/modifying-a-subset-of-rows-in-a-pandas-dataframe', 'title': 'Modifying a subset of rows in a pandas dataframe', 'body': \"<p>Assume I have a pandas DataFrame with two columns, A and B. I'd like to modify this DataFrame (or create a copy) so that B is always NaN whenever A is 0. How would I achieve that?</p>\\n\\n<p>I tried the following</p>\\n\\n<pre><code>df['A'==0]['B'] = np.nan\\n</code></pre>\\n\\n<p>and</p>\\n\\n<pre><code>df['A'==0]['B'].values.fill(np.nan)\\n</code></pre>\\n\\n<p>without success.</p>\\n\"}, {'is_answered': True, 'view_count': 54560, 'closed_date': 1400109850, 'accepted_answer_id': 10059796, 'answer_count': 6, 'score': 152, 'last_activity_date': 1577750053, 'creation_date': 1333651381, 'question_id': 10034149, 'link': 'https://stackoverflow.com/questions/10034149/why-is-nan-not-equal-to-nan', 'closed_reason': 'Duplicate', 'title': 'Why is NaN not equal to NaN?', 'body': '<p>The relevant IEEE standard defines a numeric constant NaN (not a number) and prescribes that NaN should compare as not equal to itself. Why is that?</p>\\n\\n<p>All the languages I\\'m familiar with implement this rule. But it often causes significant problems, for example unexpected behavior when NaN is stored in a container, when NaN is in the data that is being sorted, etc. Not to mention, the vast majority of programmers expect any object to be equal to itself (before they learn about NaN), so surprising them adds to the bugs and confusion.</p>\\n\\n<p>IEEE standards are well thought out, so I am sure there is a good reason why NaN comparing as equal to itself would be bad. I just can\\'t figure out what it is.</p>\\n\\n<p>Edit: please refer to <a href=\"https://stackoverflow.com/questions/1565164/what-is-the-rationale-for-all-comparisons-returning-false-for-ieee754-nan-values\">What is the rationale for all comparisons returning false for IEEE754 NaN values?</a> as the authoritative answer.</p>\\n'}, {'is_answered': True, 'view_count': 224521, 'protected_date': 1599212072, 'closed_date': 1593865349, 'accepted_answer_id': 24040239, 'answer_count': 4, 'score': 148, 'last_activity_date': 1594109304, 'creation_date': 1401889150, 'question_id': 24039023, 'link': 'https://stackoverflow.com/questions/24039023/add-column-with-constant-value-to-pandas-dataframe', 'closed_reason': 'Duplicate', 'title': 'Add column with constant value to pandas dataframe', 'body': \"<p>Given a DataFrame:</p>\\n\\n<pre><code>np.random.seed(0)\\ndf = pd.DataFrame(np.random.randn(3, 3), columns=list('ABC'), index=[1, 2, 3])\\ndf\\n\\n          A         B         C\\n1  1.764052  0.400157  0.978738\\n2  2.240893  1.867558 -0.977278\\n3  0.950088 -0.151357 -0.103219\\n</code></pre>\\n\\n<p>What is the simplest way to add a new column containing a constant value eg 0?</p>\\n\\n<pre><code>          A         B         C  new\\n1  1.764052  0.400157  0.978738    0\\n2  2.240893  1.867558 -0.977278    0\\n3  0.950088 -0.151357 -0.103219    0\\n</code></pre>\\n\\n<hr>\\n\\n<p>This is my solution, but I don't know why this puts NaN into 'new' column?</p>\\n\\n<pre><code>df['new'] = pd.Series([0 for x in range(len(df.index))])\\n\\n          A         B         C  new\\n1  1.764052  0.400157  0.978738  0.0\\n2  2.240893  1.867558 -0.977278  0.0\\n3  0.950088 -0.151357 -0.103219  NaN\\n</code></pre>\\n\"}, {'is_answered': True, 'view_count': 59195, 'protected_date': 1393010318, 'accepted_answer_id': 115696, 'answer_count': 9, 'score': 147, 'last_activity_date': 1644271457, 'creation_date': 1222097546, 'question_id': 115548, 'link': 'https://stackoverflow.com/questions/115548/why-is-isnannull-false-in-js', 'title': 'Why is isNaN(null) == false in JS?', 'body': '<p>This code in JS gives me a popup saying \"i think null is a number\", which I find slightly disturbing. What am I missing?</p>\\n\\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\\r\\n<div class=\"snippet-code\">\\r\\n<pre class=\"snippet-code-js lang-js prettyprint-override\"><code>if (isNaN(null)) {\\r\\n  alert(\"null is not a number\");\\r\\n} else {\\r\\n  alert(\"i think null is a number\");\\r\\n}</code></pre>\\r\\n</div>\\r\\n</div>\\r\\n</p>\\n\\n<p>I\\'m using Firefox 3. Is that a browser bug?</p>\\n\\n<p>Other tests:</p>\\n\\n<p><div class=\"snippet\" data-lang=\"js\" data-hide=\"false\" data-console=\"true\" data-babel=\"false\">\\r\\n<div class=\"snippet-code\">\\r\\n<pre class=\"snippet-code-js lang-js prettyprint-override\"><code>console.log(null == NaN);   // false\\r\\nconsole.log(isNaN(\"text\")); // true\\r\\nconsole.log(NaN == \"text\"); // false</code></pre>\\r\\n</div>\\r\\n</div>\\r\\n</p>\\n\\n<p>So, the problem seems not to be an exact comparison with NaN?</p>\\n\\n<p><i><b>Edit:</b> Now the question has been answered, I have cleaned up my post to have a better version for the archive. However, this renders some comments and even some answers a little incomprehensible. Don\\'t blame their authors. Among the things I changed was:</p>\\n\\n<ul>\\n<li>Removed a note saying that I had screwed up the headline in the first place by reverting its meaning</li>\\n<li>Earlier answers showed that I didn\\'t state clearly enough why I thought the behaviour was weird, so I added the examples that check a string and do a manual comparison.\\n</i></li>\\n</ul>\\n'}, {'is_answered': True, 'view_count': 142446, 'accepted_answer_id': 11005208, 'answer_count': 5, 'score': 144, 'last_activity_date': 1638289059, 'creation_date': 1338683917, 'question_id': 10867028, 'link': 'https://stackoverflow.com/questions/10867028/get-pandas-read-csv-to-read-empty-values-as-empty-string-instead-of-nan', 'title': 'Get pandas.read_csv to read empty values as empty string instead of nan', 'body': '<p>I\\'m using the pandas library to read in some CSV data.  In my data, certain columns contain strings.  The string <code>\"nan\"</code> is a possible value, as is an empty string.  I managed to get pandas to read \"nan\" as a string, but I can\\'t figure out how to get it not to read an empty value as NaN.  Here\\'s sample data and output</p>\\n\\n<pre><code>One,Two,Three\\na,1,one\\nb,2,two\\n,3,three\\nd,4,nan\\ne,5,five\\nnan,6,\\ng,7,seven\\n\\n&gt;&gt;&gt; pandas.read_csv(\\'test.csv\\', na_values={\\'One\\': [], \"Three\": []})\\n    One  Two  Three\\n0    a    1    one\\n1    b    2    two\\n2  NaN    3  three\\n3    d    4    nan\\n4    e    5   five\\n5  nan    6    NaN\\n6    g    7  seven\\n</code></pre>\\n\\n<p>It correctly reads \"nan\" as the string \"nan\\', but still reads the empty cells as NaN.  I tried passing in <code>str</code> in the <code>converters</code> argument to read_csv (with <code>converters={\\'One\\': str})</code>), but it still reads the empty cells as NaN.</p>\\n\\n<p>I realize I can fill the values after reading, with fillna, but is there really no way to tell pandas that an empty cell in a particular CSV column should be read as an empty string instead of NaN?</p>\\n'}, {'is_answered': True, 'view_count': 181149, 'accepted_answer_id': 26510251, 'answer_count': 9, 'score': 143, 'last_activity_date': 1642631119, 'creation_date': 1413883585, 'question_id': 26483254, 'link': 'https://stackoverflow.com/questions/26483254/python-pandas-insert-list-into-a-cell', 'title': 'Python pandas insert list into a cell', 'body': \"<p>I have a list 'abc' and a dataframe 'df':</p>\\n\\n<pre><code>abc = ['foo', 'bar']\\ndf =\\n    A  B\\n0  12  NaN\\n1  23  NaN\\n</code></pre>\\n\\n<p>I want to insert the list into cell 1B, so I want this result:</p>\\n\\n<pre><code>    A  B\\n0  12  NaN\\n1  23  ['foo', 'bar']\\n</code></pre>\\n\\n<p>Ho can I do that?</p>\\n\\n<p>1) If I use this:</p>\\n\\n<pre><code>df.ix[1,'B'] = abc\\n</code></pre>\\n\\n<p>I get the following error message:</p>\\n\\n<pre><code>ValueError: Must have equal len keys and value when setting with an iterable\\n</code></pre>\\n\\n<p>because it tries to insert the list (that has two elements) into a row / column but not into a cell.</p>\\n\\n<p>2) If I use this:</p>\\n\\n<pre><code>df.ix[1,'B'] = [abc]\\n</code></pre>\\n\\n<p>then it inserts a list that has only one element that is the 'abc' list ( <code>[['foo', 'bar']]</code> ).</p>\\n\\n<p>3) If I use this:</p>\\n\\n<pre><code>df.ix[1,'B'] = ', '.join(abc)\\n</code></pre>\\n\\n<p>then it inserts a string: ( <code>foo, bar</code> ) but not a list.</p>\\n\\n<p>4) If I use this:</p>\\n\\n<pre><code>df.ix[1,'B'] = [', '.join(abc)]\\n</code></pre>\\n\\n<p>then it inserts a list but it has only one element ( <code>['foo, bar']</code> ) but not two as I want ( <code>['foo', 'bar']</code> ).</p>\\n\\n<p>Thanks for help!</p>\\n\\n<hr>\\n\\n<h2>EDIT</h2>\\n\\n<p>My new dataframe and the old list:</p>\\n\\n<pre><code>abc = ['foo', 'bar']\\ndf2 =\\n    A    B         C\\n0  12  NaN      'bla'\\n1  23  NaN  'bla bla'\\n</code></pre>\\n\\n<p>Another dataframe:</p>\\n\\n<pre><code>df3 =\\n    A    B         C                    D\\n0  12  NaN      'bla'  ['item1', 'item2']\\n1  23  NaN  'bla bla'        [11, 12, 13]\\n</code></pre>\\n\\n<p>I want insert the 'abc' list into <code>df2.loc[1,'B']</code> and/or <code>df3.loc[1,'B']</code>.</p>\\n\\n<p>If the dataframe has columns only with integer values and/or NaN values and/or list values then inserting a list into a cell works perfectly. If the dataframe has columns only with string values and/or NaN values and/or list values then inserting a list into a cell works perfectly. But if the dataframe has columns with integer and string values and other columns then the error message appears if I use this: <code>df2.loc[1,'B'] = abc</code> or <code>df3.loc[1,'B'] = abc</code>.</p>\\n\\n<p>Another dataframe:</p>\\n\\n<pre><code>df4 =\\n          A     B\\n0      'bla'  NaN\\n1  'bla bla'  NaN\\n</code></pre>\\n\\n<p>These inserts work perfectly: <code>df.loc[1,'B'] = abc</code> or <code>df4.loc[1,'B'] = abc</code>.</p>\\n\"}, {'is_answered': True, 'view_count': 178225, 'accepted_answer_id': 6736970, 'answer_count': 8, 'score': 142, 'last_activity_date': 1629296827, 'creation_date': 1311009005, 'question_id': 6736590, 'link': 'https://stackoverflow.com/questions/6736590/fast-check-for-nan-in-numpy', 'title': 'Fast check for NaN in NumPy', 'body': '<p>I\\'m looking for the fastest way to check for the occurrence of NaN (<code>np.nan</code>) in a NumPy array <code>X</code>. <code>np.isnan(X)</code> is out of the question, since it builds a boolean array of shape <code>X.shape</code>, which is potentially gigantic.</p>\\n\\n<p>I tried <code>np.nan in X</code>, but that seems not to work because <code>np.nan != np.nan</code>. Is there a fast and memory-efficient way to do this at all?</p>\\n\\n<p>(To those who would ask \"how gigantic\": I can\\'t tell. This is input validation for library code.)</p>\\n'}, {'is_answered': True, 'view_count': 4965, 'accepted_answer_id': 17269376, 'answer_count': 1, 'score': 137, 'last_activity_date': 1503571368, 'creation_date': 1372051096, 'question_id': 17268468, 'link': 'https://stackoverflow.com/questions/17268468/why-is-nan-only-on-the-client-side-why-not-in-node-js', 'title': 'Why {} + {} is NaN only on the client side? Why not in Node.js?', 'body': '<p>While <code>[] + []</code> is an empty string, <code>[] + {}</code> is <code>\"[object Object]\"</code>, and <code>{} + []</code> is <code>0</code>. Why is <code>{} + {}</code> NaN?</p>\\n\\n<pre><code>&gt; {} + {}\\n  NaN\\n</code></pre>\\n\\n<p>My question isn\\'t why <code>({} + {}).toString()</code> is <code>\"[object Object][object Object]\"</code> while <code>NaN.toString()</code> is <code>\"NaN\"</code>, <a href=\"https://stackoverflow.com/a/9033306/1348195\">this part has an answer here already</a>.</p>\\n\\n<p>My question is why does this happen only on the client side? On the server side (<a href=\"http://en.wikipedia.org/wiki/Node.js\" rel=\"noreferrer\">Node.js</a>) <code>{} + {}</code> is <code>\"[object Object][object Object]\"</code>.</p>\\n\\n<pre><code>&gt; {} + {}\\n\\'[object Object][object Object]\\'\\n</code></pre>\\n\\n<hr>\\n\\n<p><strong>Summarizing</strong>:</p>\\n\\n<p>On the client side:</p>\\n\\n<pre><code> [] + []              // Returns \"\"\\n [] + {}              // Returns \"[object Object]\"\\n {} + []              // Returns 0\\n {} + {}              // Returns NaN\\n\\n NaN.toString()       // Returns \"NaN\"\\n ({} + {}).toString() // Returns \"[object Object][object Object]\"\\n var a = {} + {};     // \\'a\\' will be \"[object Object][object Object]\"\\n</code></pre>\\n\\n<p>In Node.js:</p>\\n\\n<pre><code> [] + []   // Returns \"\" (like on the client)\\n [] + {}   // Returns \"[object Object]\" (like on the client)\\n {} + []   // Returns \"[object Object]\" (not like on the client)\\n {} + {}   // Returns \"[object Object][object Object]\" (not like on the client)\\n</code></pre>\\n'}, {'is_answered': True, 'view_count': 305825, 'protected_date': 1562942734, 'answer_count': 14, 'score': 134, 'last_activity_date': 1643150362, 'creation_date': 1389242819, 'question_id': 21011777, 'link': 'https://stackoverflow.com/questions/21011777/how-can-i-remove-nan-from-list-python-numpy', 'title': 'How can I remove Nan from list Python/NumPy', 'body': \"<p>I have a list that countain values, one of the values I got is 'nan'</p>\\n\\n<pre><code>countries= [nan, 'USA', 'UK', 'France']\\n</code></pre>\\n\\n<p>I tried to remove it, but I everytime get an error </p>\\n\\n<pre><code>cleanedList = [x for x in countries if (math.isnan(x) == True)]\\nTypeError: a float is required\\n</code></pre>\\n\\n<p>When I tried this one : </p>\\n\\n<pre><code>cleanedList = cities[np.logical_not(np.isnan(countries))]\\ncleanedList = cities[~np.isnan(countries)]\\n\\nTypeError: ufunc 'isnan' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\\n</code></pre>\\n\"}], 'has_more': True, 'quota_max': 300, 'quota_remaining': 194}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/30 [00:00<?, ?it/s]\n",
      "  3%|▎         | 1/30 [00:01<00:30,  1.07s/it]\n",
      "  7%|▋         | 2/30 [00:02<00:29,  1.07s/it]\n",
      " 10%|█         | 3/30 [00:03<00:29,  1.09s/it]\n",
      " 13%|█▎        | 4/30 [00:04<00:28,  1.09s/it]\n",
      " 17%|█▋        | 5/30 [00:05<00:27,  1.09s/it]\n",
      " 20%|██        | 6/30 [00:06<00:26,  1.11s/it]\n",
      " 23%|██▎       | 7/30 [00:07<00:25,  1.11s/it]\n",
      " 27%|██▋       | 8/30 [00:09<00:24,  1.13s/it]\n",
      " 30%|███       | 9/30 [00:10<00:23,  1.13s/it]\n",
      " 33%|███▎      | 10/30 [00:11<00:22,  1.12s/it]\n",
      " 37%|███▋      | 11/30 [00:12<00:21,  1.11s/it]\n",
      " 40%|████      | 12/30 [00:13<00:20,  1.11s/it]\n",
      " 43%|████▎     | 13/30 [00:14<00:18,  1.11s/it]\n",
      " 47%|████▋     | 14/30 [00:15<00:17,  1.10s/it]\n",
      " 50%|█████     | 15/30 [00:16<00:16,  1.10s/it]\n",
      " 53%|█████▎    | 16/30 [00:17<00:15,  1.10s/it]\n",
      " 57%|█████▋    | 17/30 [00:18<00:14,  1.10s/it]\n",
      " 60%|██████    | 18/30 [00:19<00:13,  1.10s/it]\n",
      " 63%|██████▎   | 19/30 [00:20<00:12,  1.10s/it]\n",
      " 67%|██████▋   | 20/30 [00:21<00:10,  1.10s/it]\n",
      " 70%|███████   | 21/30 [00:22<00:09,  1.09s/it]\n",
      " 73%|███████▎  | 22/30 [00:24<00:08,  1.09s/it]\n",
      " 77%|███████▋  | 23/30 [00:25<00:07,  1.09s/it]\n",
      " 80%|████████  | 24/30 [00:26<00:06,  1.09s/it]\n",
      " 83%|████████▎ | 25/30 [00:27<00:05,  1.09s/it]\n",
      " 87%|████████▋ | 26/30 [00:28<00:04,  1.09s/it]\n",
      " 90%|█████████ | 27/30 [00:29<00:03,  1.09s/it]\n",
      " 93%|█████████▎| 28/30 [00:31<00:02,  1.12s/it]\n",
      " 97%|█████████▋| 29/30 [00:32<00:01,  1.12s/it]\n",
      "100%|██████████| 30/30 [00:33<00:00,  1.12s/it]\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12308/2369261792.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mscipy_replacement_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mmissing_api\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscipy_deprecated_api\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mscipy_replacement_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmissing_api\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetStackQuestionsv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_api\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\User\\Desktop\\IS706 Software Mining and Analysis\\Group Project\\Stack Overflow API\\StackOverflowAPI\\stack_replacement.py\u001b[0m in \u001b[0;36mgetStackQuestionsv2\u001b[1;34m(missing_api, top_only)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson_normalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilteredArr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"possible_replacement\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"answer_body\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpossible_replacement_api\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_api\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtop_only\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0mtop_candidate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"possible_replacement\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4355\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4356\u001b[0m         \"\"\"\n\u001b[1;32m-> 4357\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4358\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4359\u001b[0m     def _reduce(\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1044\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\pandas\\core\\apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1100\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1101\u001b[0m                     \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# type: ignore[arg-type]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1102\u001b[1;33m                     \u001b[0mconvert\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m                 )\n\u001b[0;32m   1104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\mlenv\\lib\\site-packages\\pandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\Desktop\\IS706 Software Mining and Analysis\\Group Project\\Stack Overflow API\\StackOverflowAPI\\stack_replacement.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson_normalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilteredArr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m         \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"possible_replacement\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"answer_body\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpossible_replacement_api\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_api\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtop_only\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m             \u001b[0mtop_candidate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"possible_replacement\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\Desktop\\IS706 Software Mining and Analysis\\Group Project\\Stack Overflow API\\StackOverflowAPI\\stack_replacement.py\u001b[0m in \u001b[0;36mpossible_replacement_api\u001b[1;34m(missing_api, answer_body)\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer_body\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"html.parser\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[0mcandidate_apis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"code\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m     \u001b[0mmissing_api_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtext_to_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_api\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m     \u001b[0mcosine_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_apis\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\Desktop\\IS706 Software Mining and Analysis\\Group Project\\Stack Overflow API\\StackOverflowAPI\\stack_replacement.py\u001b[0m in \u001b[0;36mtext_to_vector\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     15\u001b[0m     \"\"\"\n\u001b[0;32m     16\u001b[0m     \u001b[0mWORD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"\\w+\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWORD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "scipy_df = pd.read_csv(\"Labeling - scipy-final.csv\")\n",
    "scipy_deprecated_api = scipy_df[\"DEPRECATED_API\"].to_list()\n",
    "scipy_replacement_dict = {}\n",
    "for missing_api in tqdm(scipy_deprecated_api):\n",
    "    scipy_replacement_dict[missing_api] = getStackQuestionsv2(missing_api, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipy.misc.fromimage\n",
      "scipy.misc.imshow\n",
      "scipy.misc.toimage\n",
      "scipy.misc.comb\n",
      "scipy.misc.factorial\n",
      "scipy.stats.itemfreq\n",
      "scipy.stats.chisqprob\n",
      "scipy.linalg.expm2\n",
      "scipy.linalg.expm3\n"
     ]
    }
   ],
   "source": [
    "for missing_api, replacement_api in scipy_replacement_dict.items():\n",
    "    if replacement_api != \"\":\n",
    "        print(missing_api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "print(len(scipy_replacement_dict.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "028ff5fb01b2b2154ee933b11955c83d281cdaa18ea471c362014238a6fcaf6b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 ('mlenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
